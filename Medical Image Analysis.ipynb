{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 0. Import Libraries + Hyperparams","metadata":{"id":"11-1bHuumCMr"}},{"cell_type":"code","source":"!pip install segmentation_models_pytorch\n!pip install kornia","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H0hJQxk2ra_2","outputId":"2b21aadd-65cd-4a72-b27b-77d6340778c5","execution":{"iopub.status.busy":"2024-01-11T05:02:19.094441Z","iopub.execute_input":"2024-01-11T05:02:19.094826Z","iopub.status.idle":"2024-01-11T05:02:50.076156Z","shell.execute_reply.started":"2024-01-11T05:02:19.094797Z","shell.execute_reply":"2024-01-11T05:02:50.075045Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting segmentation_models_pytorch\n  Obtaining dependency information for segmentation_models_pytorch from https://files.pythonhosted.org/packages/cb/70/4aac1b240b399b108ce58029ae54bc14497e1bbc275dfab8fd3c84c1e35d/segmentation_models_pytorch-0.3.3-py3-none-any.whl.metadata\n  Downloading segmentation_models_pytorch-0.3.3-py3-none-any.whl.metadata (30 kB)\nRequirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.15.1)\nCollecting pretrainedmodels==0.7.4 (from segmentation_models_pytorch)\n  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting efficientnet-pytorch==0.7.1 (from segmentation_models_pytorch)\n  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting timm==0.9.2 (from segmentation_models_pytorch)\n  Obtaining dependency information for timm==0.9.2 from https://files.pythonhosted.org/packages/29/90/94f5deb8d76e24a89813aef95e8809ca8fd7414490428480eda19b133d4a/timm-0.9.2-py3-none-any.whl.metadata\n  Downloading timm-0.9.2-py3-none-any.whl.metadata (68 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.5/68.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (4.66.1)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (9.5.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.0.0)\nCollecting munch (from pretrainedmodels==0.7.4->segmentation_models_pytorch)\n  Obtaining dependency information for munch from https://files.pythonhosted.org/packages/56/b3/7c69b37f03260a061883bec0e7b05be7117c1b1c85f5212c72c8c2bc3c8c/munch-4.0.0-py2.py3-none-any.whl.metadata\n  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (6.0.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (0.20.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (0.4.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.24.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (2.31.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.1.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (2023.12.2)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (21.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2023.11.17)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.3.0)\nDownloading segmentation_models_pytorch-0.3.3-py3-none-any.whl (106 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\nBuilding wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16428 sha256=9701e992d8582e7988da22f4618cfaf1f1d0ac17b1a32cfd54ac2f379b4df9cc\n  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60943 sha256=8b249635ee0feeedd64ecfd70775dae42224fb2dfca0e72da0d4ea3c2ead3f6f\n  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\nSuccessfully built efficientnet-pytorch pretrainedmodels\nInstalling collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation_models_pytorch\n  Attempting uninstall: timm\n    Found existing installation: timm 0.9.12\n    Uninstalling timm-0.9.12:\n      Successfully uninstalled timm-0.9.12\nSuccessfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation_models_pytorch-0.3.3 timm-0.9.2\nRequirement already satisfied: kornia in /opt/conda/lib/python3.10/site-packages (0.7.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from kornia) (21.3)\nRequirement already satisfied: torch>=1.9.1 in /opt/conda/lib/python3.10/site-packages (from kornia) (2.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.1->kornia) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.1->kornia) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.1->kornia) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.1->kornia) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.1->kornia) (3.1.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->kornia) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.9.1->kornia) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.9.1->kornia) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install torchmetrics","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UfcvAV2ZCvcs","outputId":"fc09fa82-e191-4a9c-b6d4-da271e7e7a10","execution":{"iopub.status.busy":"2024-01-11T05:02:50.078101Z","iopub.execute_input":"2024-01-11T05:02:50.078435Z","iopub.status.idle":"2024-01-11T05:03:01.868385Z","shell.execute_reply.started":"2024-01-11T05:02:50.078399Z","shell.execute_reply":"2024-01-11T05:03:01.867415Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchmetrics in /opt/conda/lib/python3.10/site-packages (1.2.1)\nRequirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (1.24.3)\nRequirement already satisfied: packaging>17.1 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (21.3)\nRequirement already satisfied: torch>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (2.0.0)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (0.10.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (68.1.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>17.1->torchmetrics) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.1->torchmetrics) (3.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.1->torchmetrics) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.1->torchmetrics) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport os\nimport cv2\nimport logging\nimport sys\nimport time\nimport torchvision.transforms as T\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nimport segmentation_models_pytorch as smp\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\nfrom kornia.losses import focal_loss\nfrom torchmetrics import Accuracy, Precision, Recall, FBetaScore, Dice, JaccardIndex","metadata":{"id":"mwKc26wsmpc_","execution":{"iopub.status.busy":"2024-01-11T05:03:01.869899Z","iopub.execute_input":"2024-01-11T05:03:01.870202Z","iopub.status.idle":"2024-01-11T05:03:10.975295Z","shell.execute_reply.started":"2024-01-11T05:03:01.870175Z","shell.execute_reply":"2024-01-11T05:03:10.974339Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"id":"ZmXnfJELDGPC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_WORKERS = 0\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n#Solver\nCLASSES = {0: \"Benign\", 1: \"Malignant\", 2: \"Normal\"}\nINPUT_SIZE = (448,448)\nBATCH_SIZE = 8\nBASE_LR = 0.01\nMAX_EPOCHS = 3\nSAVE_INTERVAL = 10\nPATIENCE = 300\nN_FOLDS = 3\n\n\n#Model\nARCH = \"deeplabv3plus\" # chọn giữa ['unet', 'unetpp', , 'fpn', 'deeplabv3plus']\nENCODER_NAME = \"efficientnet-b4\" # chọn giữa các kiến trúc ['resnet50', 'resnext50_32x4d', 'tu-wide_resnet50_2', 'efficientnet-b4']\nIN_CHANNELS = 3\nSEG_NUM_CLASSES = 2\nCLA_NUM_CLASSES = 3\nOUTPUT_ACTIVATION = None #None for logits\n\n#Loss coefficient weight\nALPHA = 0.7\n\n#Path\nOUTPUT_DIR = r\"/kaggle/working/output\"\nDATASET_DIR = r\"/kaggle/input/medical-od-segmentation-homeowork/content/data/train\"\nCHECKPOINT = None\n\n#Eval\nWEIGHT = r\"\"","metadata":{"id":"xlorxtHLsbVk","execution":{"iopub.status.busy":"2024-01-11T05:03:10.977901Z","iopub.execute_input":"2024-01-11T05:03:10.978496Z","iopub.status.idle":"2024-01-11T05:03:11.010208Z","shell.execute_reply.started":"2024-01-11T05:03:10.978459Z","shell.execute_reply":"2024-01-11T05:03:11.008767Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# 1. Utils","metadata":{"id":"QMLdqRrvmuRo"}},{"cell_type":"code","source":"class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\nclass UnNormalize(object):\n    def __init__(self, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor):\n        \"\"\"\n        Args:\n            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n        Returns:\n            Tensor: Normalized image.\n        \"\"\"\n        for t, m, s in zip(tensor, self.mean, self.std):\n            t.mul_(s).add_(m)\n            # The normalize code -> t.sub_(m).div_(s)\n        return tensor","metadata":{"id":"nmYkAUrYvi1x","execution":{"iopub.status.busy":"2024-01-11T05:03:11.011106Z","iopub.execute_input":"2024-01-11T05:03:11.011399Z","iopub.status.idle":"2024-01-11T05:03:11.045405Z","shell.execute_reply.started":"2024-01-11T05:03:11.011374Z","shell.execute_reply":"2024-01-11T05:03:11.044445Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def calculate_overlap_metrics(pred, gt,eps=1e-5):\n    output = pred.view(-1,)\n    target = gt.view(-1,).float()\n\n    tp = torch.sum(output * target)  # TP\n    fp = torch.sum(output * (1 - target))  # FP\n    fn = torch.sum((1 - output) * target)  # FN\n    tn = torch.sum((1 - output) * (1 - target))  # TN\n\n    # pixel_acc = (tp + tn + eps) / (tp + tn + fp + fn + eps)\n    dice = (2 * tp + eps) / (2 * tp + fp + fn + eps)\n    iou = ( tp + eps) / ( tp + fp + fn + eps)\n    precision = (tp + eps) / (tp + fp + eps)\n    recall = (tp + eps) / (tp + fn + eps)\n#     specificity = (tn + eps) / (tn + fp + eps)\n\n    return iou, dice, precision, recall","metadata":{"id":"6QTQH0Q_vawg","execution":{"iopub.status.busy":"2024-01-11T05:03:11.047007Z","iopub.execute_input":"2024-01-11T05:03:11.047377Z","iopub.status.idle":"2024-01-11T05:03:11.060687Z","shell.execute_reply.started":"2024-01-11T05:03:11.047346Z","shell.execute_reply":"2024-01-11T05:03:11.059709Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\ndef setup_logger(logger_name, output_dir):\n    import os\n    logger = logging.getLogger(logger_name)\n    logger.setLevel(logging.DEBUG)\n    # create file handler which logs even debug messages\n    fh = logging.FileHandler(os.path.join(output_dir, 'log.log'))\n    fh.setLevel(logging.DEBUG)\n    # create console handler with a higher log level\n    ch = logging.StreamHandler()\n    ch.setLevel(logging.DEBUG)\n    # create formatter and add it to the handlers\n    formatter = logging.Formatter('%(asctime)s %(name)s %(levelname)s: %(message)s')\n    fh.setFormatter(formatter)\n    ch.setFormatter(formatter)\n    # add the handlers to logger\n    logger.addHandler(fh)\n    logger.addHandler(ch)\n    return logger\n\n\ndef logging_hyperparameters(logger):\n    logger.info(\"==========Hyperparameters==========\")\n    logger.info(f\"Device: {DEVICE}\")\n    logger.info(f\"Architecture: {ARCH}\")\n    logger.info(f\"Encoder: {ENCODER_NAME}\")\n    logger.info(f\"Encoder weight: imagenet\")\n    logger.info(f\"Input size: {INPUT_SIZE}\")\n    logger.info(f\"Batch size: {BATCH_SIZE}\")\n    logger.info(f\"Base learning rate: {BASE_LR}\")\n    logger.info(f\"Max epochs: {MAX_EPOCHS}\")\n    logger.info(f\"Weight decay: {1e-5}\")\n    logger.info(\"===================================\")\n\n\ndef init_path(task):\n    #Task == classification\n    if task == \"classification\":\n        weight_dir = os.path.join(OUTPUT_DIR, task, ENCODER_NAME)\n        os.makedirs(weight_dir, exist_ok=True)\n        log_dir = weight_dir\n        logger_name = f\"{task}_{ENCODER_NAME}\"\n    elif task == \"segmentation\":\n        weight_dir = os.path.join(OUTPUT_DIR, task, f\"{ENCODER_NAME}_{ARCH}\")\n        os.makedirs(weight_dir, exist_ok=True)\n        log_dir = weight_dir\n        logger_name = f\"{task}_{ENCODER_NAME}_{ARCH}\"\n    elif task == \"multitask\":\n        weight_dir = os.path.join(OUTPUT_DIR, f\"{ENCODER_NAME}_{ARCH}\")\n        os.makedirs(weight_dir, exist_ok=True)\n        log_dir = weight_dir\n        logger_name = f\"{task}_{ENCODER_NAME}_{ARCH}\"\n    return weight_dir, log_dir, logger_name","metadata":{"id":"KopjSjPSvnUd","execution":{"iopub.status.busy":"2024-01-11T05:03:11.062231Z","iopub.execute_input":"2024-01-11T05:03:11.062599Z","iopub.status.idle":"2024-01-11T05:03:11.077553Z","shell.execute_reply.started":"2024-01-11T05:03:11.062569Z","shell.execute_reply":"2024-01-11T05:03:11.076411Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# 2. Setup Data","metadata":{"id":"i7eDsnQFmxRO"}},{"cell_type":"markdown","source":"### 2.1. Download Dataset","metadata":{"id":"bARB4le9rIP9"}},{"cell_type":"code","source":"!mkdir output","metadata":{"id":"IvX1ZbXa0JkV","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cf003043-c354-4491-f949-6d852510d6a4","execution":{"iopub.status.busy":"2024-01-11T05:03:11.078710Z","iopub.execute_input":"2024-01-11T05:03:11.079061Z","iopub.status.idle":"2024-01-11T05:03:12.100098Z","shell.execute_reply.started":"2024-01-11T05:03:11.079030Z","shell.execute_reply":"2024-01-11T05:03:12.098922Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### 2.2. Setup dataloader","metadata":{"id":"tzUaztG7u3Mv"}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nimport numpy as np\n\n\ndef split_dataset(dataset_dir):\n    benign, malignant, normal = [], [], []\n    benign_images = [os.path.join(dataset_dir, 'benign', file) for file in os.listdir(os.path.join(dataset_dir, 'benign')) if file.endswith('.png')]\n    malignant_images = [os.path.join(dataset_dir, 'malignant', file) for file in os.listdir(os.path.join(dataset_dir, 'malignant')) if file.endswith('.png')]\n    normal_images = [os.path.join(dataset_dir, 'normal', file) for file in os.listdir(os.path.join(dataset_dir, 'normal')) if file.endswith('.png')]\n\n    for mask in benign_images:\n        if \"_mask\" in mask:\n            image = mask.replace('_mask.png', '.png')\n            benign.append((0, image, mask))\n    for mask in malignant_images:\n        if \"_mask\" in mask:\n            image = mask.replace('_mask.png', '.png')\n            malignant.append((1, image, mask))\n    for mask in normal_images:\n        if \"_mask\" in mask:\n            image = mask.replace('_mask.png', '.png')\n            normal.append((2, image, mask))\n\n    all_data = benign + malignant + normal\n    labels = [item[0] for item in all_data]\n\n    kf = StratifiedKFold(n_splits=3)\n\n    folds = []\n\n    # Splitting data into folds\n    for train_index, val_index in kf.split(np.zeros(len(labels)), labels):\n        train_set = [all_data[i] for i in train_index]\n        val_set = [all_data[i] for i in val_index]\n        folds.append((train_set, val_set))\n\n    return folds","metadata":{"id":"KQolPnCrBZCU","execution":{"iopub.status.busy":"2024-01-11T05:03:12.101781Z","iopub.execute_input":"2024-01-11T05:03:12.102158Z","iopub.status.idle":"2024-01-11T05:03:12.114211Z","shell.execute_reply.started":"2024-01-11T05:03:12.102128Z","shell.execute_reply":"2024-01-11T05:03:12.113323Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class BUSI(Dataset):\n    def __init__(self, dataset_dir, train_set, val_set, input_size=(512,512), transform=None, target_transform=None, is_train=True):\n        self.input_size = input_size\n        self.dataset_dir = dataset_dir\n        self.is_train = is_train\n        if not os.path.exists(self.dataset_dir):\n            raise ValueError('BUSI dataset not found at {}'.format(self.dataset_dir))\n\n        for _, _, files in os.walk(self.dataset_dir):\n            for file in files:\n                if \"_mask_1\" in file:\n                    raise Exception(\"This class requires BUSI dataset with combined mask. It can be done by running the BUSI() function in the process_data.py at utils folder\")\n\n        self.transform = transform\n        self.target_transform = target_transform\n        self.train_set = train_set\n        self.val_set = val_set\n        if self.is_train:\n            self.images = train_set\n        else:\n            self.images = val_set\n\n\n    def __len__(self):\n        if self.is_train:\n            return len(self.train_set)\n        else:\n            return len(self.val_set)\n\n    def __getitem__(self, idx):\n        label, image_path, mask_path = self.images[idx]\n        image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n\n\n        image = cv2.resize(image, self.input_size, interpolation=cv2.INTER_NEAREST)\n        mask = cv2.resize(mask, self.input_size, interpolation=cv2.INTER_NEAREST)\n\n\n        #Normalize\n        mask = mask/255\n        mask = torch.from_numpy(mask).long()\n        mask = torch.nn.functional.one_hot(mask, num_classes=2).permute(2,0,1).long()\n\n        if self.transform is not None:\n            image = self.transform(image)\n        if self.target_transform is not None:\n            mask = self.target_transform(mask)\n\n        return image, mask, label\n\n    @property\n    def info(self):\n        print(f\"Dataset: BUSI\")\n        print(f\"Train: {len(self.train_set)} images\")\n        print(\"-\"*20)\n        print(f\"Benign: {len([image for image in self.train_set if image[0] == 0])} images\")\n        print(f\"Malignant: {len([image for image in self.train_set if image[0] == 1])} images\")\n        print(f\"Normal: {len([image for image in self.train_set if image[0] == 2])} images\")\n        print(\"-\"*20)\n        print(f\"Val: {len(self.val_set)} images\")\n        print(\"-\"*20)\n        print(f\"Benign: {len([image for image in self.val_set if image[0] == 0])} images\")\n        print(f\"Malignant: {len([image for image in self.val_set if image[0] == 1])} images\")\n        print(f\"Normal: {len([image for image in self.val_set if image[0] == 2])} images\")\n        print(\"-\"*20)\n\n    def _get_images(self):\n        benign, malignant, normal = [], [], []\n        benign_images = [os.path.join(self.dataset_dir, 'benign', file) for file in os.listdir(os.path.join(self.dataset_dir, 'benign')) if file.endswith('.png')]\n        malignant_images = [os.path.join(self.dataset_dir, 'malignant', file) for file in os.listdir(os.path.join(self.dataset_dir, 'malignant')) if file.endswith('.png')]\n        normal_images = [os.path.join(self.dataset_dir, 'normal', file) for file in os.listdir(os.path.join(self.dataset_dir, 'normal')) if file.endswith('.png')]\n\n        for mask in benign_images:\n            if \"_mask\" in mask:\n                image = mask.replace('_mask.png', '.png')\n                benign.append((0, image, mask))\n        for mask in malignant_images:\n            if \"_mask\" in mask:\n                image = mask.replace('_mask.png', '.png')\n                malignant.append((1, image, mask))\n        for mask in normal_images:\n            if \"_mask\" in mask:\n                image = mask.replace('_mask.png', '.png')\n                normal.append((2, image, mask))\n\n        self.b_train_set, self.b_val_set = train_test_split(benign, test_size=0.2, random_state=42)\n        self.m_train_set, self.m_val_set = train_test_split(malignant, test_size=0.2, random_state=42)\n        self.n_train_set, self.n_val_set = train_test_split(normal, test_size=0.2, random_state=42)\n\n        train_set = self.b_train_set + self.m_train_set + self.n_train_set\n        val_set = self.b_val_set + self.m_val_set + self.n_val_set\n        # # without normal class\n        # train_set = b_train_set + m_train_set\n        # val_set = b_val_set + m_val_set\n        return train_set, val_set","metadata":{"id":"DUjnm8dkq3dK","execution":{"iopub.status.busy":"2024-01-11T05:03:12.118146Z","iopub.execute_input":"2024-01-11T05:03:12.118552Z","iopub.status.idle":"2024-01-11T05:03:12.140605Z","shell.execute_reply.started":"2024-01-11T05:03:12.118526Z","shell.execute_reply":"2024-01-11T05:03:12.139673Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"folds = split_dataset(DATASET_DIR)","metadata":{"id":"sRRFFZgZByW-","execution":{"iopub.status.busy":"2024-01-11T05:03:12.141706Z","iopub.execute_input":"2024-01-11T05:03:12.142054Z","iopub.status.idle":"2024-01-11T05:03:12.498204Z","shell.execute_reply.started":"2024-01-11T05:03:12.142023Z","shell.execute_reply":"2024-01-11T05:03:12.497237Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"len(folds)","metadata":{"id":"dACwRFTGCKKl","execution":{"iopub.status.busy":"2024-01-11T05:03:12.499468Z","iopub.execute_input":"2024-01-11T05:03:12.499774Z","iopub.status.idle":"2024-01-11T05:03:12.507064Z","shell.execute_reply.started":"2024-01-11T05:03:12.499747Z","shell.execute_reply":"2024-01-11T05:03:12.506134Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"3"},"metadata":{}}]},{"cell_type":"code","source":"transform = T.Compose([\n                    T.ToTensor(),\n                    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n              ])","metadata":{"id":"5ekjqaNusL1P","execution":{"iopub.status.busy":"2024-01-11T05:03:12.508115Z","iopub.execute_input":"2024-01-11T05:03:12.508444Z","iopub.status.idle":"2024-01-11T05:03:12.517342Z","shell.execute_reply.started":"2024-01-11T05:03:12.508421Z","shell.execute_reply":"2024-01-11T05:03:12.516446Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# 3. Single Model","metadata":{"id":"Xa895_NhtjQ2"}},{"cell_type":"markdown","source":"### 3.1 Setup model","metadata":{"id":"zJbfv6JRuB-r"}},{"cell_type":"code","source":"PRETRAINED_WEIGHT_URL = {\n    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n    'tu-wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n    'efficientnet-b4': 'https://download.pytorch.org/models/efficientnet_b4_rwightman-7eb33cd5.pth',\n}\n\ndef segmentation_model(aux_param=None):\n    assert ARCH in ['unet', 'unetpp', 'deeplabv3plus', 'fpn'], \"Invalid architecture, must be ['unet', 'unetpp', 'deeplabv3plus', 'fpn']\"\n    assert ENCODER_NAME in ['resnet50', 'resnext50_32x4d', 'tu-wide_resnet50_2', 'efficientnet-b4'], \"Invalid encoder name, must be ['resnet50', 'resnext50_32x4d', 'tu-wide_resnet50_2', 'efficientnet-b4']\"\n    #Params\n\n    params = dict(\n        encoder_name = ENCODER_NAME,\n        encoder_depth = 5,\n        encoder_weights = \"imagenet\",\n        in_channels = IN_CHANNELS,\n        classes = SEG_NUM_CLASSES,\n        activation = OUTPUT_ACTIVATION,\n        aux_params = aux_param\n    )\n    MODELS = {\n        'unet':smp.Unet(**params),\n        'unetpp': smp.UnetPlusPlus(**params),\n        'deeplabv3plus': smp.DeepLabV3Plus(**params),\n        'fpn': smp.FPN(**params),\n\n    }\n    return MODELS[ARCH]\n\ndef classification_model():\n    MODELS = {\n        'resnet50': torchvision.models.resnet50(weights='DEFAULT'),\n        'resnext50_32x4d': torchvision.models.resnext50_32x4d(weights='DEFAULT'),\n        'tu-wide_resnet50_2': torchvision.models.wide_resnet50_2(weights='DEFAULT'),\n        'efficientnet-b4': torchvision.models.efficientnet_b4(weights=None),\n    }\n    model = MODELS[ENCODER_NAME]\n\n    # Replace the last layer\n    if ENCODER_NAME == \"efficientnet-b4\":\n        state_dict = torch.hub.load_state_dict_from_url(PRETRAINED_WEIGHT_URL[ENCODER_NAME])\n        model.load_state_dict(state_dict)\n        model.classifier = torch.nn.Linear(1792, CLA_NUM_CLASSES)\n    else:\n        model.fc = torch.nn.Linear(2048, CLA_NUM_CLASSES)\n\n    return model\n\nclass TwoSingleModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.seg_model = segmentation_model()\n        self.cla_model = classification_model()\n\n    def forward(self, x):\n        seg_out = self.seg_model(x)\n        cla_out = self.cla_model(x)\n        return seg_out, cla_out\n","metadata":{"id":"WwbsL3ygt2yW","execution":{"iopub.status.busy":"2024-01-11T05:03:12.518867Z","iopub.execute_input":"2024-01-11T05:03:12.519225Z","iopub.status.idle":"2024-01-11T05:03:12.531422Z","shell.execute_reply.started":"2024-01-11T05:03:12.519193Z","shell.execute_reply":"2024-01-11T05:03:12.530484Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DFVq_wmdt3GZ","outputId":"e0a725b1-8114-4d5d-a667-0762a66e8e07","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2. Training Classification","metadata":{"id":"842kDmqEwIkr"}},{"cell_type":"code","source":"def train(folds):\n    \n    for fold in range(N_FOLDS):\n    \n            #TASK\n        TASK = \"classification\"\n\n        #Path\n        weight_dir, log_dir, logger_name = init_path(TASK)\n\n        #Model\n        model = classification_model().to(DEVICE)\n\n        #Loss & Optimizer\n        model = model.to(DEVICE)\n        optimizer = optim.Adam(model.parameters(), lr=BASE_LR, weight_decay=1e-5)\n\n\n        #Meters\n        train_loss_meter = AverageMeter()\n        val_loss_meter = AverageMeter()\n        acc_meter = AverageMeter()\n        precision_meter = AverageMeter()\n        recall_meter = AverageMeter()\n        f1_score_meter = AverageMeter()\n        \n        train_images, val_images = folds[fold]\n        \n        train_set = BUSI(DATASET_DIR, train_images, val_images, input_size=INPUT_SIZE,transform=transform, target_transform=None, is_train=True)\n        val_set = BUSI(DATASET_DIR, train_images, val_images, input_size=INPUT_SIZE,transform=transform, target_transform=None, is_train=False)\n\n        train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n        val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n\n        logger = setup_logger(logger_name, log_dir)\n        best_f1 = 0\n        stale = 0\n        start_epoch = 1\n\n        if CHECKPOINT is not None:\n            if os.path.exists(CHECKPOINT):\n                checkpoint = torch.load(CHECKPOINT)\n                model.load_state_dict(checkpoint['model_state_dict'])\n                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n                start_epoch = checkpoint['epoch']\n                best_f1 = checkpoint['best_f1']\n                logger.info(f\"Resume training from epoch {start_epoch}\")\n            else:\n                logger.info(f\"Checkpoint not found, start training from epoch 1\")\n\n        #Logging hyperparameters\n        logging_hyperparameters(logger)\n\n\n        for epoch in range(start_epoch, 1+MAX_EPOCHS):\n            #Start time\n            start_time = time.time()\n            #Train\n            model.train()\n            #Reset meters\n            train_loss_meter.reset()\n            precision_meter.reset()\n            recall_meter.reset()\n            f1_score_meter.reset()\n            acc_meter.reset()\n\n            logger.info(\"Start training\")\n            for batch_idx, (image, _, label) in enumerate(train_loader):\n                n = image.shape[0]\n                optimizer.zero_grad()\n                image = image.to(DEVICE)\n                label = label.to(DEVICE)\n\n                output = model(image) #Logits (batch_size,num_classes)\n                #Cal loss\n                train_loss = focal_loss(output, label, alpha=0.25, gamma=2, reduction='mean')\n                train_loss.backward()\n                optimizer.step()\n\n                train_loss_meter.update(train_loss.item(),n)\n\n                if batch_idx % 10 == 0:\n                    logger.info(f\"Epoch[{epoch}] - Fold[{fold}] - Iteration[{batch_idx}/{len(train_loader)}] Loss: {train_loss:.3f}\")\n            end_time = time.time()\n            logger.info(f\"Training Result: Epoch {epoch}/{MAX_EPOCHS} Fold {fold}/{N_FOLDS}, Loss: {train_loss_meter.avg:.3f}, Time epoch: {end_time-start_time:.3f}s\")\n\n            #Valid\n            model.eval()\n            with torch.no_grad():\n                for batch_idx, (image, _, label) in enumerate(val_loader):\n                    n = image.shape[0]\n                    image = image.to(DEVICE)\n                    label = label.to(DEVICE)\n\n                    output = model(image)\n                    val_loss = focal_loss(output, label, alpha=0.25, gamma=2,reduction='mean')\n\n                    #Calculate metrics\n                    #P, R and F1\n                    label = label.detach().cpu().numpy()\n                    output = output.argmax(1).detach().cpu().numpy()\n\n                    p_score = precision_score(label, output, average='macro', zero_division=0)\n                    r_score = recall_score(label, output, average='macro', zero_division=0)\n                    _f1_score = f1_score(label, output, average='macro')\n                    acc = accuracy_score(label, output)\n\n                    #Update meters\n                    val_loss_meter.update(val_loss.item(), n)\n                    acc_meter.update(acc.item(),n)\n                    precision_meter.update(p_score.item(), n)\n                    recall_meter.update(r_score.item(), n)\n                    f1_score_meter.update(_f1_score.item(), n)\n\n            logger.info(f\"Validation Result: Loss: {val_loss_meter.avg:.3f}, Accuracy: {acc_meter.avg:.3f} F1-Score: {f1_score_meter.avg:.3f}, Precision: {precision_meter.avg:.3f}, Recall: {recall_meter.avg:.3f}\")\n\n            #Save best model\n            to_save = {\n                    'epoch': epoch,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'best_f1': best_f1,\n                }\n            if f1_score_meter.avg > best_f1: # best base on IoU score\n                logger.info(f\"Best model found at epoch {epoch}, saving model\")\n                torch.save(to_save, os.path.join(weight_dir,f\"best_epoch{epoch}_fold{fold}_{INPUT_SIZE[0]}_BS={BATCH_SIZE}_f1={f1_score_meter.avg:.3f}.pth\")) # only save best to prevent output memory exceed error\n                best_f1 = f1_score_meter.avg\n                stale = 0\n            else:\n                stale += 1\n                if stale > 300:\n                    logger.info(f\"No improvement {300} consecutive epochs, early stopping\")\n                    break\n            if epoch % SAVE_INTERVAL == 0 or epoch == MAX_EPOCHS:\n                logger.info(f\"Save model at epoch {epoch}, saving model\")\n                torch.save(to_save, os.path.join(weight_dir,f\"epoch_{epoch}_{fold}.pth\"))\n","metadata":{"id":"0FAGnxCDwd1u","execution":{"iopub.status.busy":"2024-01-11T05:07:36.801958Z","iopub.execute_input":"2024-01-11T05:07:36.802474Z","iopub.status.idle":"2024-01-11T05:07:36.835535Z","shell.execute_reply.started":"2024-01-11T05:07:36.802435Z","shell.execute_reply":"2024-01-11T05:07:36.834503Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"train(folds)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"gOVH1a0CwrDq","outputId":"dc1b5074-55da-4f77-d316-db86437b7bd2","execution":{"iopub.status.busy":"2024-01-11T05:07:37.862773Z","iopub.execute_input":"2024-01-11T05:07:37.863139Z","iopub.status.idle":"2024-01-11T05:12:05.367696Z","shell.execute_reply.started":"2024-01-11T05:07:37.863111Z","shell.execute_reply":"2024-01-11T05:12:05.366622Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"2024-01-11 05:07:40,484 classification_efficientnet-b4 INFO: ==========Hyperparameters==========\n2024-01-11 05:07:40,484 classification_efficientnet-b4 INFO: ==========Hyperparameters==========\n2024-01-11 05:07:40,484 classification_efficientnet-b4 INFO: ==========Hyperparameters==========\n2024-01-11 05:07:40,486 classification_efficientnet-b4 INFO: Device: cuda\n2024-01-11 05:07:40,486 classification_efficientnet-b4 INFO: Device: cuda\n2024-01-11 05:07:40,486 classification_efficientnet-b4 INFO: Device: cuda\n2024-01-11 05:07:40,488 classification_efficientnet-b4 INFO: Architecture: deeplabv3plus\n2024-01-11 05:07:40,488 classification_efficientnet-b4 INFO: Architecture: deeplabv3plus\n2024-01-11 05:07:40,488 classification_efficientnet-b4 INFO: Architecture: deeplabv3plus\n2024-01-11 05:07:40,490 classification_efficientnet-b4 INFO: Encoder: efficientnet-b4\n2024-01-11 05:07:40,490 classification_efficientnet-b4 INFO: Encoder: efficientnet-b4\n2024-01-11 05:07:40,490 classification_efficientnet-b4 INFO: Encoder: efficientnet-b4\n2024-01-11 05:07:40,493 classification_efficientnet-b4 INFO: Encoder weight: imagenet\n2024-01-11 05:07:40,493 classification_efficientnet-b4 INFO: Encoder weight: imagenet\n2024-01-11 05:07:40,493 classification_efficientnet-b4 INFO: Encoder weight: imagenet\n2024-01-11 05:07:40,495 classification_efficientnet-b4 INFO: Input size: (448, 448)\n2024-01-11 05:07:40,495 classification_efficientnet-b4 INFO: Input size: (448, 448)\n2024-01-11 05:07:40,495 classification_efficientnet-b4 INFO: Input size: (448, 448)\n2024-01-11 05:07:40,498 classification_efficientnet-b4 INFO: Batch size: 8\n2024-01-11 05:07:40,498 classification_efficientnet-b4 INFO: Batch size: 8\n2024-01-11 05:07:40,498 classification_efficientnet-b4 INFO: Batch size: 8\n2024-01-11 05:07:40,501 classification_efficientnet-b4 INFO: Base learning rate: 0.01\n2024-01-11 05:07:40,501 classification_efficientnet-b4 INFO: Base learning rate: 0.01\n2024-01-11 05:07:40,501 classification_efficientnet-b4 INFO: Base learning rate: 0.01\n2024-01-11 05:07:40,504 classification_efficientnet-b4 INFO: Max epochs: 3\n2024-01-11 05:07:40,504 classification_efficientnet-b4 INFO: Max epochs: 3\n2024-01-11 05:07:40,504 classification_efficientnet-b4 INFO: Max epochs: 3\n2024-01-11 05:07:40,506 classification_efficientnet-b4 INFO: Weight decay: 1e-05\n2024-01-11 05:07:40,506 classification_efficientnet-b4 INFO: Weight decay: 1e-05\n2024-01-11 05:07:40,506 classification_efficientnet-b4 INFO: Weight decay: 1e-05\n2024-01-11 05:07:40,508 classification_efficientnet-b4 INFO: ===================================\n2024-01-11 05:07:40,508 classification_efficientnet-b4 INFO: ===================================\n2024-01-11 05:07:40,508 classification_efficientnet-b4 INFO: ===================================\n2024-01-11 05:07:40,515 classification_efficientnet-b4 INFO: Start training\n2024-01-11 05:07:40,515 classification_efficientnet-b4 INFO: Start training\n2024-01-11 05:07:40,515 classification_efficientnet-b4 INFO: Start training\n2024-01-11 05:07:42,189 classification_efficientnet-b4 INFO: Epoch[1] - Fold[0] - Iteration[0/52] Loss: 0.061\n2024-01-11 05:07:42,189 classification_efficientnet-b4 INFO: Epoch[1] - Fold[0] - Iteration[0/52] Loss: 0.061\n2024-01-11 05:07:42,189 classification_efficientnet-b4 INFO: Epoch[1] - Fold[0] - Iteration[0/52] Loss: 0.061\n2024-01-11 05:07:47,302 classification_efficientnet-b4 INFO: Epoch[1] - Fold[0] - Iteration[10/52] Loss: 0.075\n2024-01-11 05:07:47,302 classification_efficientnet-b4 INFO: Epoch[1] - Fold[0] - Iteration[10/52] Loss: 0.075\n2024-01-11 05:07:47,302 classification_efficientnet-b4 INFO: Epoch[1] - Fold[0] - Iteration[10/52] Loss: 0.075\n2024-01-11 05:07:52,472 classification_efficientnet-b4 INFO: Epoch[1] - Fold[0] - Iteration[20/52] Loss: 0.102\n2024-01-11 05:07:52,472 classification_efficientnet-b4 INFO: Epoch[1] - Fold[0] - Iteration[20/52] Loss: 0.102\n2024-01-11 05:07:52,472 classification_efficientnet-b4 INFO: Epoch[1] - Fold[0] - Iteration[20/52] Loss: 0.102\n2024-01-11 05:07:57,656 classification_efficientnet-b4 INFO: Epoch[1] - Fold[0] - Iteration[30/52] Loss: 0.097\n2024-01-11 05:07:57,656 classification_efficientnet-b4 INFO: Epoch[1] - Fold[0] - Iteration[30/52] Loss: 0.097\n2024-01-11 05:07:57,656 classification_efficientnet-b4 INFO: Epoch[1] - Fold[0] - Iteration[30/52] Loss: 0.097\n2024-01-11 05:08:02,880 classification_efficientnet-b4 INFO: Epoch[1] - Fold[0] - Iteration[40/52] Loss: 0.057\n2024-01-11 05:08:02,880 classification_efficientnet-b4 INFO: Epoch[1] - Fold[0] - Iteration[40/52] Loss: 0.057\n2024-01-11 05:08:02,880 classification_efficientnet-b4 INFO: Epoch[1] - Fold[0] - Iteration[40/52] Loss: 0.057\n2024-01-11 05:08:07,945 classification_efficientnet-b4 INFO: Epoch[1] - Fold[0] - Iteration[50/52] Loss: 0.060\n2024-01-11 05:08:07,945 classification_efficientnet-b4 INFO: Epoch[1] - Fold[0] - Iteration[50/52] Loss: 0.060\n2024-01-11 05:08:07,945 classification_efficientnet-b4 INFO: Epoch[1] - Fold[0] - Iteration[50/52] Loss: 0.060\n2024-01-11 05:08:08,378 classification_efficientnet-b4 INFO: Training Result: Epoch 1/3 Fold 0/3, Loss: 0.099, Time epoch: 27.868s\n2024-01-11 05:08:08,378 classification_efficientnet-b4 INFO: Training Result: Epoch 1/3 Fold 0/3, Loss: 0.099, Time epoch: 27.868s\n2024-01-11 05:08:08,378 classification_efficientnet-b4 INFO: Training Result: Epoch 1/3 Fold 0/3, Loss: 0.099, Time epoch: 27.868s\n2024-01-11 05:08:16,154 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.045, Accuracy: 0.577 F1-Score: 0.566, Precision: 0.608, Recall: 0.565\n2024-01-11 05:08:16,154 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.045, Accuracy: 0.577 F1-Score: 0.566, Precision: 0.608, Recall: 0.565\n2024-01-11 05:08:16,154 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.045, Accuracy: 0.577 F1-Score: 0.566, Precision: 0.608, Recall: 0.565\n2024-01-11 05:08:16,164 classification_efficientnet-b4 INFO: Best model found at epoch 1, saving model\n2024-01-11 05:08:16,164 classification_efficientnet-b4 INFO: Best model found at epoch 1, saving model\n2024-01-11 05:08:16,164 classification_efficientnet-b4 INFO: Best model found at epoch 1, saving model\n2024-01-11 05:08:16,590 classification_efficientnet-b4 INFO: Start training\n2024-01-11 05:08:16,590 classification_efficientnet-b4 INFO: Start training\n2024-01-11 05:08:16,590 classification_efficientnet-b4 INFO: Start training\n2024-01-11 05:08:17,021 classification_efficientnet-b4 INFO: Epoch[2] - Fold[0] - Iteration[0/52] Loss: 0.055\n2024-01-11 05:08:17,021 classification_efficientnet-b4 INFO: Epoch[2] - Fold[0] - Iteration[0/52] Loss: 0.055\n2024-01-11 05:08:17,021 classification_efficientnet-b4 INFO: Epoch[2] - Fold[0] - Iteration[0/52] Loss: 0.055\n2024-01-11 05:08:21,287 classification_efficientnet-b4 INFO: Epoch[2] - Fold[0] - Iteration[10/52] Loss: 0.033\n2024-01-11 05:08:21,287 classification_efficientnet-b4 INFO: Epoch[2] - Fold[0] - Iteration[10/52] Loss: 0.033\n2024-01-11 05:08:21,287 classification_efficientnet-b4 INFO: Epoch[2] - Fold[0] - Iteration[10/52] Loss: 0.033\n2024-01-11 05:08:25,498 classification_efficientnet-b4 INFO: Epoch[2] - Fold[0] - Iteration[20/52] Loss: 0.038\n2024-01-11 05:08:25,498 classification_efficientnet-b4 INFO: Epoch[2] - Fold[0] - Iteration[20/52] Loss: 0.038\n2024-01-11 05:08:25,498 classification_efficientnet-b4 INFO: Epoch[2] - Fold[0] - Iteration[20/52] Loss: 0.038\n2024-01-11 05:08:29,719 classification_efficientnet-b4 INFO: Epoch[2] - Fold[0] - Iteration[30/52] Loss: 0.068\n2024-01-11 05:08:29,719 classification_efficientnet-b4 INFO: Epoch[2] - Fold[0] - Iteration[30/52] Loss: 0.068\n2024-01-11 05:08:29,719 classification_efficientnet-b4 INFO: Epoch[2] - Fold[0] - Iteration[30/52] Loss: 0.068\n2024-01-11 05:08:33,992 classification_efficientnet-b4 INFO: Epoch[2] - Fold[0] - Iteration[40/52] Loss: 0.078\n2024-01-11 05:08:33,992 classification_efficientnet-b4 INFO: Epoch[2] - Fold[0] - Iteration[40/52] Loss: 0.078\n2024-01-11 05:08:33,992 classification_efficientnet-b4 INFO: Epoch[2] - Fold[0] - Iteration[40/52] Loss: 0.078\n2024-01-11 05:08:38,213 classification_efficientnet-b4 INFO: Epoch[2] - Fold[0] - Iteration[50/52] Loss: 0.024\n2024-01-11 05:08:38,213 classification_efficientnet-b4 INFO: Epoch[2] - Fold[0] - Iteration[50/52] Loss: 0.024\n2024-01-11 05:08:38,213 classification_efficientnet-b4 INFO: Epoch[2] - Fold[0] - Iteration[50/52] Loss: 0.024\n2024-01-11 05:08:38,583 classification_efficientnet-b4 INFO: Training Result: Epoch 2/3 Fold 0/3, Loss: 0.057, Time epoch: 21.997s\n2024-01-11 05:08:38,583 classification_efficientnet-b4 INFO: Training Result: Epoch 2/3 Fold 0/3, Loss: 0.057, Time epoch: 21.997s\n2024-01-11 05:08:38,583 classification_efficientnet-b4 INFO: Training Result: Epoch 2/3 Fold 0/3, Loss: 0.057, Time epoch: 21.997s\n2024-01-11 05:08:44,105 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.061, Accuracy: 0.562 F1-Score: 0.553, Precision: 0.550, Recall: 0.558\n2024-01-11 05:08:44,105 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.061, Accuracy: 0.562 F1-Score: 0.553, Precision: 0.550, Recall: 0.558\n2024-01-11 05:08:44,105 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.061, Accuracy: 0.562 F1-Score: 0.553, Precision: 0.550, Recall: 0.558\n2024-01-11 05:08:44,120 classification_efficientnet-b4 INFO: Start training\n2024-01-11 05:08:44,120 classification_efficientnet-b4 INFO: Start training\n2024-01-11 05:08:44,120 classification_efficientnet-b4 INFO: Start training\n2024-01-11 05:08:44,536 classification_efficientnet-b4 INFO: Epoch[3] - Fold[0] - Iteration[0/52] Loss: 0.033\n2024-01-11 05:08:44,536 classification_efficientnet-b4 INFO: Epoch[3] - Fold[0] - Iteration[0/52] Loss: 0.033\n2024-01-11 05:08:44,536 classification_efficientnet-b4 INFO: Epoch[3] - Fold[0] - Iteration[0/52] Loss: 0.033\n2024-01-11 05:08:48,746 classification_efficientnet-b4 INFO: Epoch[3] - Fold[0] - Iteration[10/52] Loss: 0.029\n2024-01-11 05:08:48,746 classification_efficientnet-b4 INFO: Epoch[3] - Fold[0] - Iteration[10/52] Loss: 0.029\n2024-01-11 05:08:48,746 classification_efficientnet-b4 INFO: Epoch[3] - Fold[0] - Iteration[10/52] Loss: 0.029\n2024-01-11 05:08:52,928 classification_efficientnet-b4 INFO: Epoch[3] - Fold[0] - Iteration[20/52] Loss: 0.068\n2024-01-11 05:08:52,928 classification_efficientnet-b4 INFO: Epoch[3] - Fold[0] - Iteration[20/52] Loss: 0.068\n2024-01-11 05:08:52,928 classification_efficientnet-b4 INFO: Epoch[3] - Fold[0] - Iteration[20/52] Loss: 0.068\n2024-01-11 05:08:57,178 classification_efficientnet-b4 INFO: Epoch[3] - Fold[0] - Iteration[30/52] Loss: 0.029\n2024-01-11 05:08:57,178 classification_efficientnet-b4 INFO: Epoch[3] - Fold[0] - Iteration[30/52] Loss: 0.029\n2024-01-11 05:08:57,178 classification_efficientnet-b4 INFO: Epoch[3] - Fold[0] - Iteration[30/52] Loss: 0.029\n2024-01-11 05:09:01,319 classification_efficientnet-b4 INFO: Epoch[3] - Fold[0] - Iteration[40/52] Loss: 0.026\n2024-01-11 05:09:01,319 classification_efficientnet-b4 INFO: Epoch[3] - Fold[0] - Iteration[40/52] Loss: 0.026\n2024-01-11 05:09:01,319 classification_efficientnet-b4 INFO: Epoch[3] - Fold[0] - Iteration[40/52] Loss: 0.026\n2024-01-11 05:09:05,584 classification_efficientnet-b4 INFO: Epoch[3] - Fold[0] - Iteration[50/52] Loss: 0.041\n2024-01-11 05:09:05,584 classification_efficientnet-b4 INFO: Epoch[3] - Fold[0] - Iteration[50/52] Loss: 0.041\n2024-01-11 05:09:05,584 classification_efficientnet-b4 INFO: Epoch[3] - Fold[0] - Iteration[50/52] Loss: 0.041\n2024-01-11 05:09:05,961 classification_efficientnet-b4 INFO: Training Result: Epoch 3/3 Fold 0/3, Loss: 0.053, Time epoch: 21.845s\n2024-01-11 05:09:05,961 classification_efficientnet-b4 INFO: Training Result: Epoch 3/3 Fold 0/3, Loss: 0.053, Time epoch: 21.845s\n2024-01-11 05:09:05,961 classification_efficientnet-b4 INFO: Training Result: Epoch 3/3 Fold 0/3, Loss: 0.053, Time epoch: 21.845s\n2024-01-11 05:09:11,508 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.062, Accuracy: 0.562 F1-Score: 0.553, Precision: 0.550, Recall: 0.558\n2024-01-11 05:09:11,508 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.062, Accuracy: 0.562 F1-Score: 0.553, Precision: 0.550, Recall: 0.558\n2024-01-11 05:09:11,508 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.062, Accuracy: 0.562 F1-Score: 0.553, Precision: 0.550, Recall: 0.558\n2024-01-11 05:09:11,518 classification_efficientnet-b4 INFO: Save model at epoch 3, saving model\n2024-01-11 05:09:11,518 classification_efficientnet-b4 INFO: Save model at epoch 3, saving model\n2024-01-11 05:09:11,518 classification_efficientnet-b4 INFO: Save model at epoch 3, saving model\n2024-01-11 05:09:14,384 classification_efficientnet-b4 INFO: ==========Hyperparameters==========\n2024-01-11 05:09:14,384 classification_efficientnet-b4 INFO: ==========Hyperparameters==========\n2024-01-11 05:09:14,384 classification_efficientnet-b4 INFO: ==========Hyperparameters==========\n2024-01-11 05:09:14,384 classification_efficientnet-b4 INFO: ==========Hyperparameters==========\n2024-01-11 05:09:14,387 classification_efficientnet-b4 INFO: Device: cuda\n2024-01-11 05:09:14,387 classification_efficientnet-b4 INFO: Device: cuda\n2024-01-11 05:09:14,387 classification_efficientnet-b4 INFO: Device: cuda\n2024-01-11 05:09:14,387 classification_efficientnet-b4 INFO: Device: cuda\n2024-01-11 05:09:14,390 classification_efficientnet-b4 INFO: Architecture: deeplabv3plus\n2024-01-11 05:09:14,390 classification_efficientnet-b4 INFO: Architecture: deeplabv3plus\n2024-01-11 05:09:14,390 classification_efficientnet-b4 INFO: Architecture: deeplabv3plus\n2024-01-11 05:09:14,390 classification_efficientnet-b4 INFO: Architecture: deeplabv3plus\n2024-01-11 05:09:14,393 classification_efficientnet-b4 INFO: Encoder: efficientnet-b4\n2024-01-11 05:09:14,393 classification_efficientnet-b4 INFO: Encoder: efficientnet-b4\n2024-01-11 05:09:14,393 classification_efficientnet-b4 INFO: Encoder: efficientnet-b4\n2024-01-11 05:09:14,393 classification_efficientnet-b4 INFO: Encoder: efficientnet-b4\n2024-01-11 05:09:14,396 classification_efficientnet-b4 INFO: Encoder weight: imagenet\n2024-01-11 05:09:14,396 classification_efficientnet-b4 INFO: Encoder weight: imagenet\n2024-01-11 05:09:14,396 classification_efficientnet-b4 INFO: Encoder weight: imagenet\n2024-01-11 05:09:14,396 classification_efficientnet-b4 INFO: Encoder weight: imagenet\n2024-01-11 05:09:14,399 classification_efficientnet-b4 INFO: Input size: (448, 448)\n2024-01-11 05:09:14,399 classification_efficientnet-b4 INFO: Input size: (448, 448)\n2024-01-11 05:09:14,399 classification_efficientnet-b4 INFO: Input size: (448, 448)\n2024-01-11 05:09:14,399 classification_efficientnet-b4 INFO: Input size: (448, 448)\n2024-01-11 05:09:14,402 classification_efficientnet-b4 INFO: Batch size: 8\n2024-01-11 05:09:14,402 classification_efficientnet-b4 INFO: Batch size: 8\n2024-01-11 05:09:14,402 classification_efficientnet-b4 INFO: Batch size: 8\n2024-01-11 05:09:14,402 classification_efficientnet-b4 INFO: Batch size: 8\n2024-01-11 05:09:14,405 classification_efficientnet-b4 INFO: Base learning rate: 0.01\n2024-01-11 05:09:14,405 classification_efficientnet-b4 INFO: Base learning rate: 0.01\n2024-01-11 05:09:14,405 classification_efficientnet-b4 INFO: Base learning rate: 0.01\n2024-01-11 05:09:14,405 classification_efficientnet-b4 INFO: Base learning rate: 0.01\n2024-01-11 05:09:14,408 classification_efficientnet-b4 INFO: Max epochs: 3\n2024-01-11 05:09:14,408 classification_efficientnet-b4 INFO: Max epochs: 3\n2024-01-11 05:09:14,408 classification_efficientnet-b4 INFO: Max epochs: 3\n2024-01-11 05:09:14,408 classification_efficientnet-b4 INFO: Max epochs: 3\n2024-01-11 05:09:14,410 classification_efficientnet-b4 INFO: Weight decay: 1e-05\n2024-01-11 05:09:14,410 classification_efficientnet-b4 INFO: Weight decay: 1e-05\n2024-01-11 05:09:14,410 classification_efficientnet-b4 INFO: Weight decay: 1e-05\n2024-01-11 05:09:14,410 classification_efficientnet-b4 INFO: Weight decay: 1e-05\n2024-01-11 05:09:14,414 classification_efficientnet-b4 INFO: ===================================\n2024-01-11 05:09:14,414 classification_efficientnet-b4 INFO: ===================================\n2024-01-11 05:09:14,414 classification_efficientnet-b4 INFO: ===================================\n2024-01-11 05:09:14,414 classification_efficientnet-b4 INFO: ===================================\n2024-01-11 05:09:14,423 classification_efficientnet-b4 INFO: Start training\n2024-01-11 05:09:14,423 classification_efficientnet-b4 INFO: Start training\n2024-01-11 05:09:14,423 classification_efficientnet-b4 INFO: Start training\n2024-01-11 05:09:14,423 classification_efficientnet-b4 INFO: Start training\n2024-01-11 05:09:14,863 classification_efficientnet-b4 INFO: Epoch[1] - Fold[1] - Iteration[0/52] Loss: 0.073\n2024-01-11 05:09:14,863 classification_efficientnet-b4 INFO: Epoch[1] - Fold[1] - Iteration[0/52] Loss: 0.073\n2024-01-11 05:09:14,863 classification_efficientnet-b4 INFO: Epoch[1] - Fold[1] - Iteration[0/52] Loss: 0.073\n2024-01-11 05:09:14,863 classification_efficientnet-b4 INFO: Epoch[1] - Fold[1] - Iteration[0/52] Loss: 0.073\n2024-01-11 05:09:19,030 classification_efficientnet-b4 INFO: Epoch[1] - Fold[1] - Iteration[10/52] Loss: 0.084\n2024-01-11 05:09:19,030 classification_efficientnet-b4 INFO: Epoch[1] - Fold[1] - Iteration[10/52] Loss: 0.084\n2024-01-11 05:09:19,030 classification_efficientnet-b4 INFO: Epoch[1] - Fold[1] - Iteration[10/52] Loss: 0.084\n2024-01-11 05:09:19,030 classification_efficientnet-b4 INFO: Epoch[1] - Fold[1] - Iteration[10/52] Loss: 0.084\n2024-01-11 05:09:23,270 classification_efficientnet-b4 INFO: Epoch[1] - Fold[1] - Iteration[20/52] Loss: 0.061\n2024-01-11 05:09:23,270 classification_efficientnet-b4 INFO: Epoch[1] - Fold[1] - Iteration[20/52] Loss: 0.061\n2024-01-11 05:09:23,270 classification_efficientnet-b4 INFO: Epoch[1] - Fold[1] - Iteration[20/52] Loss: 0.061\n2024-01-11 05:09:23,270 classification_efficientnet-b4 INFO: Epoch[1] - Fold[1] - Iteration[20/52] Loss: 0.061\n2024-01-11 05:09:27,526 classification_efficientnet-b4 INFO: Epoch[1] - Fold[1] - Iteration[30/52] Loss: 0.039\n2024-01-11 05:09:27,526 classification_efficientnet-b4 INFO: Epoch[1] - Fold[1] - Iteration[30/52] Loss: 0.039\n2024-01-11 05:09:27,526 classification_efficientnet-b4 INFO: Epoch[1] - Fold[1] - Iteration[30/52] Loss: 0.039\n2024-01-11 05:09:27,526 classification_efficientnet-b4 INFO: Epoch[1] - Fold[1] - Iteration[30/52] Loss: 0.039\n2024-01-11 05:09:31,759 classification_efficientnet-b4 INFO: Epoch[1] - Fold[1] - Iteration[40/52] Loss: 0.029\n2024-01-11 05:09:31,759 classification_efficientnet-b4 INFO: Epoch[1] - Fold[1] - Iteration[40/52] Loss: 0.029\n2024-01-11 05:09:31,759 classification_efficientnet-b4 INFO: Epoch[1] - Fold[1] - Iteration[40/52] Loss: 0.029\n2024-01-11 05:09:31,759 classification_efficientnet-b4 INFO: Epoch[1] - Fold[1] - Iteration[40/52] Loss: 0.029\n2024-01-11 05:09:35,998 classification_efficientnet-b4 INFO: Epoch[1] - Fold[1] - Iteration[50/52] Loss: 0.047\n2024-01-11 05:09:35,998 classification_efficientnet-b4 INFO: Epoch[1] - Fold[1] - Iteration[50/52] Loss: 0.047\n2024-01-11 05:09:35,998 classification_efficientnet-b4 INFO: Epoch[1] - Fold[1] - Iteration[50/52] Loss: 0.047\n2024-01-11 05:09:35,998 classification_efficientnet-b4 INFO: Epoch[1] - Fold[1] - Iteration[50/52] Loss: 0.047\n2024-01-11 05:09:36,386 classification_efficientnet-b4 INFO: Training Result: Epoch 1/3 Fold 1/3, Loss: 0.071, Time epoch: 21.968s\n2024-01-11 05:09:36,386 classification_efficientnet-b4 INFO: Training Result: Epoch 1/3 Fold 1/3, Loss: 0.071, Time epoch: 21.968s\n2024-01-11 05:09:36,386 classification_efficientnet-b4 INFO: Training Result: Epoch 1/3 Fold 1/3, Loss: 0.071, Time epoch: 21.968s\n2024-01-11 05:09:36,386 classification_efficientnet-b4 INFO: Training Result: Epoch 1/3 Fold 1/3, Loss: 0.071, Time epoch: 21.968s\n2024-01-11 05:09:41,982 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.060, Accuracy: 0.514 F1-Score: 0.266, Precision: 0.387, Recall: 0.216\n2024-01-11 05:09:41,982 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.060, Accuracy: 0.514 F1-Score: 0.266, Precision: 0.387, Recall: 0.216\n2024-01-11 05:09:41,982 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.060, Accuracy: 0.514 F1-Score: 0.266, Precision: 0.387, Recall: 0.216\n2024-01-11 05:09:41,982 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.060, Accuracy: 0.514 F1-Score: 0.266, Precision: 0.387, Recall: 0.216\n2024-01-11 05:09:41,996 classification_efficientnet-b4 INFO: Best model found at epoch 1, saving model\n2024-01-11 05:09:41,996 classification_efficientnet-b4 INFO: Best model found at epoch 1, saving model\n2024-01-11 05:09:41,996 classification_efficientnet-b4 INFO: Best model found at epoch 1, saving model\n2024-01-11 05:09:41,996 classification_efficientnet-b4 INFO: Best model found at epoch 1, saving model\n2024-01-11 05:09:42,452 classification_efficientnet-b4 INFO: Start training\n2024-01-11 05:09:42,452 classification_efficientnet-b4 INFO: Start training\n2024-01-11 05:09:42,452 classification_efficientnet-b4 INFO: Start training\n2024-01-11 05:09:42,452 classification_efficientnet-b4 INFO: Start training\n2024-01-11 05:09:42,914 classification_efficientnet-b4 INFO: Epoch[2] - Fold[1] - Iteration[0/52] Loss: 0.112\n2024-01-11 05:09:42,914 classification_efficientnet-b4 INFO: Epoch[2] - Fold[1] - Iteration[0/52] Loss: 0.112\n2024-01-11 05:09:42,914 classification_efficientnet-b4 INFO: Epoch[2] - Fold[1] - Iteration[0/52] Loss: 0.112\n2024-01-11 05:09:42,914 classification_efficientnet-b4 INFO: Epoch[2] - Fold[1] - Iteration[0/52] Loss: 0.112\n2024-01-11 05:09:47,100 classification_efficientnet-b4 INFO: Epoch[2] - Fold[1] - Iteration[10/52] Loss: 0.139\n2024-01-11 05:09:47,100 classification_efficientnet-b4 INFO: Epoch[2] - Fold[1] - Iteration[10/52] Loss: 0.139\n2024-01-11 05:09:47,100 classification_efficientnet-b4 INFO: Epoch[2] - Fold[1] - Iteration[10/52] Loss: 0.139\n2024-01-11 05:09:47,100 classification_efficientnet-b4 INFO: Epoch[2] - Fold[1] - Iteration[10/52] Loss: 0.139\n2024-01-11 05:09:51,297 classification_efficientnet-b4 INFO: Epoch[2] - Fold[1] - Iteration[20/52] Loss: 0.085\n2024-01-11 05:09:51,297 classification_efficientnet-b4 INFO: Epoch[2] - Fold[1] - Iteration[20/52] Loss: 0.085\n2024-01-11 05:09:51,297 classification_efficientnet-b4 INFO: Epoch[2] - Fold[1] - Iteration[20/52] Loss: 0.085\n2024-01-11 05:09:51,297 classification_efficientnet-b4 INFO: Epoch[2] - Fold[1] - Iteration[20/52] Loss: 0.085\n2024-01-11 05:09:55,616 classification_efficientnet-b4 INFO: Epoch[2] - Fold[1] - Iteration[30/52] Loss: 0.100\n2024-01-11 05:09:55,616 classification_efficientnet-b4 INFO: Epoch[2] - Fold[1] - Iteration[30/52] Loss: 0.100\n2024-01-11 05:09:55,616 classification_efficientnet-b4 INFO: Epoch[2] - Fold[1] - Iteration[30/52] Loss: 0.100\n2024-01-11 05:09:55,616 classification_efficientnet-b4 INFO: Epoch[2] - Fold[1] - Iteration[30/52] Loss: 0.100\n2024-01-11 05:09:59,860 classification_efficientnet-b4 INFO: Epoch[2] - Fold[1] - Iteration[40/52] Loss: 0.078\n2024-01-11 05:09:59,860 classification_efficientnet-b4 INFO: Epoch[2] - Fold[1] - Iteration[40/52] Loss: 0.078\n2024-01-11 05:09:59,860 classification_efficientnet-b4 INFO: Epoch[2] - Fold[1] - Iteration[40/52] Loss: 0.078\n2024-01-11 05:09:59,860 classification_efficientnet-b4 INFO: Epoch[2] - Fold[1] - Iteration[40/52] Loss: 0.078\n2024-01-11 05:10:04,046 classification_efficientnet-b4 INFO: Epoch[2] - Fold[1] - Iteration[50/52] Loss: 0.060\n2024-01-11 05:10:04,046 classification_efficientnet-b4 INFO: Epoch[2] - Fold[1] - Iteration[50/52] Loss: 0.060\n2024-01-11 05:10:04,046 classification_efficientnet-b4 INFO: Epoch[2] - Fold[1] - Iteration[50/52] Loss: 0.060\n2024-01-11 05:10:04,046 classification_efficientnet-b4 INFO: Epoch[2] - Fold[1] - Iteration[50/52] Loss: 0.060\n2024-01-11 05:10:04,419 classification_efficientnet-b4 INFO: Training Result: Epoch 2/3 Fold 1/3, Loss: 0.071, Time epoch: 21.971s\n2024-01-11 05:10:04,419 classification_efficientnet-b4 INFO: Training Result: Epoch 2/3 Fold 1/3, Loss: 0.071, Time epoch: 21.971s\n2024-01-11 05:10:04,419 classification_efficientnet-b4 INFO: Training Result: Epoch 2/3 Fold 1/3, Loss: 0.071, Time epoch: 21.971s\n2024-01-11 05:10:04,419 classification_efficientnet-b4 INFO: Training Result: Epoch 2/3 Fold 1/3, Loss: 0.071, Time epoch: 21.971s\n2024-01-11 05:10:09,893 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.081, Accuracy: 0.490 F1-Score: 0.337, Precision: 0.433, Recall: 0.312\n2024-01-11 05:10:09,893 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.081, Accuracy: 0.490 F1-Score: 0.337, Precision: 0.433, Recall: 0.312\n2024-01-11 05:10:09,893 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.081, Accuracy: 0.490 F1-Score: 0.337, Precision: 0.433, Recall: 0.312\n2024-01-11 05:10:09,893 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.081, Accuracy: 0.490 F1-Score: 0.337, Precision: 0.433, Recall: 0.312\n2024-01-11 05:10:09,904 classification_efficientnet-b4 INFO: Best model found at epoch 2, saving model\n2024-01-11 05:10:09,904 classification_efficientnet-b4 INFO: Best model found at epoch 2, saving model\n2024-01-11 05:10:09,904 classification_efficientnet-b4 INFO: Best model found at epoch 2, saving model\n2024-01-11 05:10:09,904 classification_efficientnet-b4 INFO: Best model found at epoch 2, saving model\n2024-01-11 05:10:10,338 classification_efficientnet-b4 INFO: Start training\n2024-01-11 05:10:10,338 classification_efficientnet-b4 INFO: Start training\n2024-01-11 05:10:10,338 classification_efficientnet-b4 INFO: Start training\n2024-01-11 05:10:10,338 classification_efficientnet-b4 INFO: Start training\n2024-01-11 05:10:10,758 classification_efficientnet-b4 INFO: Epoch[3] - Fold[1] - Iteration[0/52] Loss: 0.032\n2024-01-11 05:10:10,758 classification_efficientnet-b4 INFO: Epoch[3] - Fold[1] - Iteration[0/52] Loss: 0.032\n2024-01-11 05:10:10,758 classification_efficientnet-b4 INFO: Epoch[3] - Fold[1] - Iteration[0/52] Loss: 0.032\n2024-01-11 05:10:10,758 classification_efficientnet-b4 INFO: Epoch[3] - Fold[1] - Iteration[0/52] Loss: 0.032\n2024-01-11 05:10:15,001 classification_efficientnet-b4 INFO: Epoch[3] - Fold[1] - Iteration[10/52] Loss: 0.058\n2024-01-11 05:10:15,001 classification_efficientnet-b4 INFO: Epoch[3] - Fold[1] - Iteration[10/52] Loss: 0.058\n2024-01-11 05:10:15,001 classification_efficientnet-b4 INFO: Epoch[3] - Fold[1] - Iteration[10/52] Loss: 0.058\n2024-01-11 05:10:15,001 classification_efficientnet-b4 INFO: Epoch[3] - Fold[1] - Iteration[10/52] Loss: 0.058\n2024-01-11 05:10:19,279 classification_efficientnet-b4 INFO: Epoch[3] - Fold[1] - Iteration[20/52] Loss: 0.053\n2024-01-11 05:10:19,279 classification_efficientnet-b4 INFO: Epoch[3] - Fold[1] - Iteration[20/52] Loss: 0.053\n2024-01-11 05:10:19,279 classification_efficientnet-b4 INFO: Epoch[3] - Fold[1] - Iteration[20/52] Loss: 0.053\n2024-01-11 05:10:19,279 classification_efficientnet-b4 INFO: Epoch[3] - Fold[1] - Iteration[20/52] Loss: 0.053\n2024-01-11 05:10:23,519 classification_efficientnet-b4 INFO: Epoch[3] - Fold[1] - Iteration[30/52] Loss: 0.075\n2024-01-11 05:10:23,519 classification_efficientnet-b4 INFO: Epoch[3] - Fold[1] - Iteration[30/52] Loss: 0.075\n2024-01-11 05:10:23,519 classification_efficientnet-b4 INFO: Epoch[3] - Fold[1] - Iteration[30/52] Loss: 0.075\n2024-01-11 05:10:23,519 classification_efficientnet-b4 INFO: Epoch[3] - Fold[1] - Iteration[30/52] Loss: 0.075\n2024-01-11 05:10:27,755 classification_efficientnet-b4 INFO: Epoch[3] - Fold[1] - Iteration[40/52] Loss: 0.064\n2024-01-11 05:10:27,755 classification_efficientnet-b4 INFO: Epoch[3] - Fold[1] - Iteration[40/52] Loss: 0.064\n2024-01-11 05:10:27,755 classification_efficientnet-b4 INFO: Epoch[3] - Fold[1] - Iteration[40/52] Loss: 0.064\n2024-01-11 05:10:27,755 classification_efficientnet-b4 INFO: Epoch[3] - Fold[1] - Iteration[40/52] Loss: 0.064\n2024-01-11 05:10:32,058 classification_efficientnet-b4 INFO: Epoch[3] - Fold[1] - Iteration[50/52] Loss: 0.055\n2024-01-11 05:10:32,058 classification_efficientnet-b4 INFO: Epoch[3] - Fold[1] - Iteration[50/52] Loss: 0.055\n2024-01-11 05:10:32,058 classification_efficientnet-b4 INFO: Epoch[3] - Fold[1] - Iteration[50/52] Loss: 0.055\n2024-01-11 05:10:32,058 classification_efficientnet-b4 INFO: Epoch[3] - Fold[1] - Iteration[50/52] Loss: 0.055\n2024-01-11 05:10:32,446 classification_efficientnet-b4 INFO: Training Result: Epoch 3/3 Fold 1/3, Loss: 0.060, Time epoch: 22.112s\n2024-01-11 05:10:32,446 classification_efficientnet-b4 INFO: Training Result: Epoch 3/3 Fold 1/3, Loss: 0.060, Time epoch: 22.112s\n2024-01-11 05:10:32,446 classification_efficientnet-b4 INFO: Training Result: Epoch 3/3 Fold 1/3, Loss: 0.060, Time epoch: 22.112s\n2024-01-11 05:10:32,446 classification_efficientnet-b4 INFO: Training Result: Epoch 3/3 Fold 1/3, Loss: 0.060, Time epoch: 22.112s\n2024-01-11 05:10:38,210 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.070, Accuracy: 0.558 F1-Score: 0.551, Precision: 0.548, Recall: 0.558\n2024-01-11 05:10:38,210 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.070, Accuracy: 0.558 F1-Score: 0.551, Precision: 0.548, Recall: 0.558\n2024-01-11 05:10:38,210 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.070, Accuracy: 0.558 F1-Score: 0.551, Precision: 0.548, Recall: 0.558\n2024-01-11 05:10:38,210 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.070, Accuracy: 0.558 F1-Score: 0.551, Precision: 0.548, Recall: 0.558\n2024-01-11 05:10:38,221 classification_efficientnet-b4 INFO: Best model found at epoch 3, saving model\n2024-01-11 05:10:38,221 classification_efficientnet-b4 INFO: Best model found at epoch 3, saving model\n2024-01-11 05:10:38,221 classification_efficientnet-b4 INFO: Best model found at epoch 3, saving model\n2024-01-11 05:10:38,221 classification_efficientnet-b4 INFO: Best model found at epoch 3, saving model\n2024-01-11 05:10:38,844 classification_efficientnet-b4 INFO: Save model at epoch 3, saving model\n2024-01-11 05:10:38,844 classification_efficientnet-b4 INFO: Save model at epoch 3, saving model\n2024-01-11 05:10:38,844 classification_efficientnet-b4 INFO: Save model at epoch 3, saving model\n2024-01-11 05:10:38,844 classification_efficientnet-b4 INFO: Save model at epoch 3, saving model\n2024-01-11 05:10:41,698 classification_efficientnet-b4 INFO: ==========Hyperparameters==========\n2024-01-11 05:10:41,698 classification_efficientnet-b4 INFO: ==========Hyperparameters==========\n2024-01-11 05:10:41,698 classification_efficientnet-b4 INFO: ==========Hyperparameters==========\n2024-01-11 05:10:41,698 classification_efficientnet-b4 INFO: ==========Hyperparameters==========\n2024-01-11 05:10:41,698 classification_efficientnet-b4 INFO: ==========Hyperparameters==========\n2024-01-11 05:10:41,702 classification_efficientnet-b4 INFO: Device: cuda\n2024-01-11 05:10:41,702 classification_efficientnet-b4 INFO: Device: cuda\n2024-01-11 05:10:41,702 classification_efficientnet-b4 INFO: Device: cuda\n2024-01-11 05:10:41,702 classification_efficientnet-b4 INFO: Device: cuda\n2024-01-11 05:10:41,702 classification_efficientnet-b4 INFO: Device: cuda\n2024-01-11 05:10:41,706 classification_efficientnet-b4 INFO: Architecture: deeplabv3plus\n2024-01-11 05:10:41,706 classification_efficientnet-b4 INFO: Architecture: deeplabv3plus\n2024-01-11 05:10:41,706 classification_efficientnet-b4 INFO: Architecture: deeplabv3plus\n2024-01-11 05:10:41,706 classification_efficientnet-b4 INFO: Architecture: deeplabv3plus\n2024-01-11 05:10:41,706 classification_efficientnet-b4 INFO: Architecture: deeplabv3plus\n2024-01-11 05:10:41,711 classification_efficientnet-b4 INFO: Encoder: efficientnet-b4\n2024-01-11 05:10:41,711 classification_efficientnet-b4 INFO: Encoder: efficientnet-b4\n2024-01-11 05:10:41,711 classification_efficientnet-b4 INFO: Encoder: efficientnet-b4\n2024-01-11 05:10:41,711 classification_efficientnet-b4 INFO: Encoder: efficientnet-b4\n2024-01-11 05:10:41,711 classification_efficientnet-b4 INFO: Encoder: efficientnet-b4\n2024-01-11 05:10:41,716 classification_efficientnet-b4 INFO: Encoder weight: imagenet\n2024-01-11 05:10:41,716 classification_efficientnet-b4 INFO: Encoder weight: imagenet\n2024-01-11 05:10:41,716 classification_efficientnet-b4 INFO: Encoder weight: imagenet\n2024-01-11 05:10:41,716 classification_efficientnet-b4 INFO: Encoder weight: imagenet\n2024-01-11 05:10:41,716 classification_efficientnet-b4 INFO: Encoder weight: imagenet\n2024-01-11 05:10:41,721 classification_efficientnet-b4 INFO: Input size: (448, 448)\n2024-01-11 05:10:41,721 classification_efficientnet-b4 INFO: Input size: (448, 448)\n2024-01-11 05:10:41,721 classification_efficientnet-b4 INFO: Input size: (448, 448)\n2024-01-11 05:10:41,721 classification_efficientnet-b4 INFO: Input size: (448, 448)\n2024-01-11 05:10:41,721 classification_efficientnet-b4 INFO: Input size: (448, 448)\n2024-01-11 05:10:41,725 classification_efficientnet-b4 INFO: Batch size: 8\n2024-01-11 05:10:41,725 classification_efficientnet-b4 INFO: Batch size: 8\n2024-01-11 05:10:41,725 classification_efficientnet-b4 INFO: Batch size: 8\n2024-01-11 05:10:41,725 classification_efficientnet-b4 INFO: Batch size: 8\n2024-01-11 05:10:41,725 classification_efficientnet-b4 INFO: Batch size: 8\n2024-01-11 05:10:41,730 classification_efficientnet-b4 INFO: Base learning rate: 0.01\n2024-01-11 05:10:41,730 classification_efficientnet-b4 INFO: Base learning rate: 0.01\n2024-01-11 05:10:41,730 classification_efficientnet-b4 INFO: Base learning rate: 0.01\n2024-01-11 05:10:41,730 classification_efficientnet-b4 INFO: Base learning rate: 0.01\n2024-01-11 05:10:41,730 classification_efficientnet-b4 INFO: Base learning rate: 0.01\n2024-01-11 05:10:41,734 classification_efficientnet-b4 INFO: Max epochs: 3\n2024-01-11 05:10:41,734 classification_efficientnet-b4 INFO: Max epochs: 3\n2024-01-11 05:10:41,734 classification_efficientnet-b4 INFO: Max epochs: 3\n2024-01-11 05:10:41,734 classification_efficientnet-b4 INFO: Max epochs: 3\n2024-01-11 05:10:41,734 classification_efficientnet-b4 INFO: Max epochs: 3\n2024-01-11 05:10:41,738 classification_efficientnet-b4 INFO: Weight decay: 1e-05\n2024-01-11 05:10:41,738 classification_efficientnet-b4 INFO: Weight decay: 1e-05\n2024-01-11 05:10:41,738 classification_efficientnet-b4 INFO: Weight decay: 1e-05\n2024-01-11 05:10:41,738 classification_efficientnet-b4 INFO: Weight decay: 1e-05\n2024-01-11 05:10:41,738 classification_efficientnet-b4 INFO: Weight decay: 1e-05\n2024-01-11 05:10:41,741 classification_efficientnet-b4 INFO: ===================================\n2024-01-11 05:10:41,741 classification_efficientnet-b4 INFO: ===================================\n2024-01-11 05:10:41,741 classification_efficientnet-b4 INFO: ===================================\n2024-01-11 05:10:41,741 classification_efficientnet-b4 INFO: ===================================\n2024-01-11 05:10:41,741 classification_efficientnet-b4 INFO: ===================================\n2024-01-11 05:10:41,750 classification_efficientnet-b4 INFO: Start training\n2024-01-11 05:10:41,750 classification_efficientnet-b4 INFO: Start training\n2024-01-11 05:10:41,750 classification_efficientnet-b4 INFO: Start training\n2024-01-11 05:10:41,750 classification_efficientnet-b4 INFO: Start training\n2024-01-11 05:10:41,750 classification_efficientnet-b4 INFO: Start training\n2024-01-11 05:10:42,176 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[0/52] Loss: 0.092\n2024-01-11 05:10:42,176 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[0/52] Loss: 0.092\n2024-01-11 05:10:42,176 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[0/52] Loss: 0.092\n2024-01-11 05:10:42,176 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[0/52] Loss: 0.092\n2024-01-11 05:10:42,176 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[0/52] Loss: 0.092\n2024-01-11 05:10:46,462 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[10/52] Loss: 0.225\n2024-01-11 05:10:46,462 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[10/52] Loss: 0.225\n2024-01-11 05:10:46,462 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[10/52] Loss: 0.225\n2024-01-11 05:10:46,462 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[10/52] Loss: 0.225\n2024-01-11 05:10:46,462 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[10/52] Loss: 0.225\n2024-01-11 05:10:50,658 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[20/52] Loss: 0.016\n2024-01-11 05:10:50,658 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[20/52] Loss: 0.016\n2024-01-11 05:10:50,658 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[20/52] Loss: 0.016\n2024-01-11 05:10:50,658 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[20/52] Loss: 0.016\n2024-01-11 05:10:50,658 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[20/52] Loss: 0.016\n2024-01-11 05:10:54,893 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[30/52] Loss: 0.072\n2024-01-11 05:10:54,893 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[30/52] Loss: 0.072\n2024-01-11 05:10:54,893 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[30/52] Loss: 0.072\n2024-01-11 05:10:54,893 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[30/52] Loss: 0.072\n2024-01-11 05:10:54,893 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[30/52] Loss: 0.072\n2024-01-11 05:10:59,120 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[40/52] Loss: 0.085\n2024-01-11 05:10:59,120 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[40/52] Loss: 0.085\n2024-01-11 05:10:59,120 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[40/52] Loss: 0.085\n2024-01-11 05:10:59,120 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[40/52] Loss: 0.085\n2024-01-11 05:10:59,120 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[40/52] Loss: 0.085\n2024-01-11 05:11:03,376 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[50/52] Loss: 0.060\n2024-01-11 05:11:03,376 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[50/52] Loss: 0.060\n2024-01-11 05:11:03,376 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[50/52] Loss: 0.060\n2024-01-11 05:11:03,376 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[50/52] Loss: 0.060\n2024-01-11 05:11:03,376 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[50/52] Loss: 0.060\n2024-01-11 05:11:03,801 classification_efficientnet-b4 INFO: Training Result: Epoch 1/3 Fold 2/3, Loss: 0.123, Time epoch: 22.056s\n2024-01-11 05:11:03,801 classification_efficientnet-b4 INFO: Training Result: Epoch 1/3 Fold 2/3, Loss: 0.123, Time epoch: 22.056s\n2024-01-11 05:11:03,801 classification_efficientnet-b4 INFO: Training Result: Epoch 1/3 Fold 2/3, Loss: 0.123, Time epoch: 22.056s\n2024-01-11 05:11:03,801 classification_efficientnet-b4 INFO: Training Result: Epoch 1/3 Fold 2/3, Loss: 0.123, Time epoch: 22.056s\n2024-01-11 05:11:03,801 classification_efficientnet-b4 INFO: Training Result: Epoch 1/3 Fold 2/3, Loss: 0.123, Time epoch: 22.056s\n2024-01-11 05:11:09,260 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.053, Accuracy: 0.565 F1-Score: 0.558, Precision: 0.570, Recall: 0.563\n2024-01-11 05:11:09,260 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.053, Accuracy: 0.565 F1-Score: 0.558, Precision: 0.570, Recall: 0.563\n2024-01-11 05:11:09,260 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.053, Accuracy: 0.565 F1-Score: 0.558, Precision: 0.570, Recall: 0.563\n2024-01-11 05:11:09,260 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.053, Accuracy: 0.565 F1-Score: 0.558, Precision: 0.570, Recall: 0.563\n2024-01-11 05:11:09,260 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.053, Accuracy: 0.565 F1-Score: 0.558, Precision: 0.570, Recall: 0.563\n2024-01-11 05:11:09,275 classification_efficientnet-b4 INFO: Best model found at epoch 1, saving model\n2024-01-11 05:11:09,275 classification_efficientnet-b4 INFO: Best model found at epoch 1, saving model\n2024-01-11 05:11:09,275 classification_efficientnet-b4 INFO: Best model found at epoch 1, saving model\n2024-01-11 05:11:09,275 classification_efficientnet-b4 INFO: Best model found at epoch 1, saving model\n2024-01-11 05:11:09,275 classification_efficientnet-b4 INFO: Best model found at epoch 1, saving model\n2024-01-11 05:11:09,695 classification_efficientnet-b4 INFO: Start training\n2024-01-11 05:11:09,695 classification_efficientnet-b4 INFO: Start training\n2024-01-11 05:11:09,695 classification_efficientnet-b4 INFO: Start training\n2024-01-11 05:11:09,695 classification_efficientnet-b4 INFO: Start training\n2024-01-11 05:11:09,695 classification_efficientnet-b4 INFO: Start training\n2024-01-11 05:11:10,097 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[0/52] Loss: 0.048\n2024-01-11 05:11:10,097 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[0/52] Loss: 0.048\n2024-01-11 05:11:10,097 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[0/52] Loss: 0.048\n2024-01-11 05:11:10,097 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[0/52] Loss: 0.048\n2024-01-11 05:11:10,097 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[0/52] Loss: 0.048\n2024-01-11 05:11:14,312 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[10/52] Loss: 0.065\n2024-01-11 05:11:14,312 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[10/52] Loss: 0.065\n2024-01-11 05:11:14,312 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[10/52] Loss: 0.065\n2024-01-11 05:11:14,312 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[10/52] Loss: 0.065\n2024-01-11 05:11:14,312 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[10/52] Loss: 0.065\n2024-01-11 05:11:18,558 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[20/52] Loss: 0.056\n2024-01-11 05:11:18,558 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[20/52] Loss: 0.056\n2024-01-11 05:11:18,558 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[20/52] Loss: 0.056\n2024-01-11 05:11:18,558 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[20/52] Loss: 0.056\n2024-01-11 05:11:18,558 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[20/52] Loss: 0.056\n2024-01-11 05:11:22,706 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[30/52] Loss: 0.034\n2024-01-11 05:11:22,706 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[30/52] Loss: 0.034\n2024-01-11 05:11:22,706 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[30/52] Loss: 0.034\n2024-01-11 05:11:22,706 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[30/52] Loss: 0.034\n2024-01-11 05:11:22,706 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[30/52] Loss: 0.034\n2024-01-11 05:11:26,934 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[40/52] Loss: 0.028\n2024-01-11 05:11:26,934 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[40/52] Loss: 0.028\n2024-01-11 05:11:26,934 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[40/52] Loss: 0.028\n2024-01-11 05:11:26,934 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[40/52] Loss: 0.028\n2024-01-11 05:11:26,934 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[40/52] Loss: 0.028\n2024-01-11 05:11:31,339 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[50/52] Loss: 0.036\n2024-01-11 05:11:31,339 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[50/52] Loss: 0.036\n2024-01-11 05:11:31,339 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[50/52] Loss: 0.036\n2024-01-11 05:11:31,339 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[50/52] Loss: 0.036\n2024-01-11 05:11:31,339 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[50/52] Loss: 0.036\n2024-01-11 05:11:31,782 classification_efficientnet-b4 INFO: Training Result: Epoch 2/3 Fold 2/3, Loss: 0.049, Time epoch: 22.091s\n2024-01-11 05:11:31,782 classification_efficientnet-b4 INFO: Training Result: Epoch 2/3 Fold 2/3, Loss: 0.049, Time epoch: 22.091s\n2024-01-11 05:11:31,782 classification_efficientnet-b4 INFO: Training Result: Epoch 2/3 Fold 2/3, Loss: 0.049, Time epoch: 22.091s\n2024-01-11 05:11:31,782 classification_efficientnet-b4 INFO: Training Result: Epoch 2/3 Fold 2/3, Loss: 0.049, Time epoch: 22.091s\n2024-01-11 05:11:31,782 classification_efficientnet-b4 INFO: Training Result: Epoch 2/3 Fold 2/3, Loss: 0.049, Time epoch: 22.091s\n2024-01-11 05:11:37,338 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.050, Accuracy: 0.599 F1-Score: 0.530, Precision: 0.606, Recall: 0.519\n2024-01-11 05:11:37,338 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.050, Accuracy: 0.599 F1-Score: 0.530, Precision: 0.606, Recall: 0.519\n2024-01-11 05:11:37,338 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.050, Accuracy: 0.599 F1-Score: 0.530, Precision: 0.606, Recall: 0.519\n2024-01-11 05:11:37,338 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.050, Accuracy: 0.599 F1-Score: 0.530, Precision: 0.606, Recall: 0.519\n2024-01-11 05:11:37,338 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.050, Accuracy: 0.599 F1-Score: 0.530, Precision: 0.606, Recall: 0.519\n2024-01-11 05:11:37,354 classification_efficientnet-b4 INFO: Start training\n2024-01-11 05:11:37,354 classification_efficientnet-b4 INFO: Start training\n2024-01-11 05:11:37,354 classification_efficientnet-b4 INFO: Start training\n2024-01-11 05:11:37,354 classification_efficientnet-b4 INFO: Start training\n2024-01-11 05:11:37,354 classification_efficientnet-b4 INFO: Start training\n2024-01-11 05:11:37,768 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[0/52] Loss: 0.029\n2024-01-11 05:11:37,768 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[0/52] Loss: 0.029\n2024-01-11 05:11:37,768 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[0/52] Loss: 0.029\n2024-01-11 05:11:37,768 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[0/52] Loss: 0.029\n2024-01-11 05:11:37,768 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[0/52] Loss: 0.029\n2024-01-11 05:11:41,961 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[10/52] Loss: 0.041\n2024-01-11 05:11:41,961 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[10/52] Loss: 0.041\n2024-01-11 05:11:41,961 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[10/52] Loss: 0.041\n2024-01-11 05:11:41,961 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[10/52] Loss: 0.041\n2024-01-11 05:11:41,961 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[10/52] Loss: 0.041\n2024-01-11 05:11:46,160 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[20/52] Loss: 0.045\n2024-01-11 05:11:46,160 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[20/52] Loss: 0.045\n2024-01-11 05:11:46,160 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[20/52] Loss: 0.045\n2024-01-11 05:11:46,160 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[20/52] Loss: 0.045\n2024-01-11 05:11:46,160 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[20/52] Loss: 0.045\n2024-01-11 05:11:50,334 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[30/52] Loss: 0.032\n2024-01-11 05:11:50,334 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[30/52] Loss: 0.032\n2024-01-11 05:11:50,334 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[30/52] Loss: 0.032\n2024-01-11 05:11:50,334 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[30/52] Loss: 0.032\n2024-01-11 05:11:50,334 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[30/52] Loss: 0.032\n2024-01-11 05:11:54,485 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[40/52] Loss: 0.038\n2024-01-11 05:11:54,485 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[40/52] Loss: 0.038\n2024-01-11 05:11:54,485 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[40/52] Loss: 0.038\n2024-01-11 05:11:54,485 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[40/52] Loss: 0.038\n2024-01-11 05:11:54,485 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[40/52] Loss: 0.038\n2024-01-11 05:11:58,761 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[50/52] Loss: 0.045\n2024-01-11 05:11:58,761 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[50/52] Loss: 0.045\n2024-01-11 05:11:58,761 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[50/52] Loss: 0.045\n2024-01-11 05:11:58,761 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[50/52] Loss: 0.045\n2024-01-11 05:11:58,761 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[50/52] Loss: 0.045\n2024-01-11 05:11:59,199 classification_efficientnet-b4 INFO: Training Result: Epoch 3/3 Fold 2/3, Loss: 0.041, Time epoch: 21.849s\n2024-01-11 05:11:59,199 classification_efficientnet-b4 INFO: Training Result: Epoch 3/3 Fold 2/3, Loss: 0.041, Time epoch: 21.849s\n2024-01-11 05:11:59,199 classification_efficientnet-b4 INFO: Training Result: Epoch 3/3 Fold 2/3, Loss: 0.041, Time epoch: 21.849s\n2024-01-11 05:11:59,199 classification_efficientnet-b4 INFO: Training Result: Epoch 3/3 Fold 2/3, Loss: 0.041, Time epoch: 21.849s\n2024-01-11 05:11:59,199 classification_efficientnet-b4 INFO: Training Result: Epoch 3/3 Fold 2/3, Loss: 0.041, Time epoch: 21.849s\n2024-01-11 05:12:04,932 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.056, Accuracy: 0.483 F1-Score: 0.305, Precision: 0.380, Recall: 0.267\n2024-01-11 05:12:04,932 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.056, Accuracy: 0.483 F1-Score: 0.305, Precision: 0.380, Recall: 0.267\n2024-01-11 05:12:04,932 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.056, Accuracy: 0.483 F1-Score: 0.305, Precision: 0.380, Recall: 0.267\n2024-01-11 05:12:04,932 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.056, Accuracy: 0.483 F1-Score: 0.305, Precision: 0.380, Recall: 0.267\n2024-01-11 05:12:04,932 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.056, Accuracy: 0.483 F1-Score: 0.305, Precision: 0.380, Recall: 0.267\n2024-01-11 05:12:04,944 classification_efficientnet-b4 INFO: Save model at epoch 3, saving model\n2024-01-11 05:12:04,944 classification_efficientnet-b4 INFO: Save model at epoch 3, saving model\n2024-01-11 05:12:04,944 classification_efficientnet-b4 INFO: Save model at epoch 3, saving model\n2024-01-11 05:12:04,944 classification_efficientnet-b4 INFO: Save model at epoch 3, saving model\n2024-01-11 05:12:04,944 classification_efficientnet-b4 INFO: Save model at epoch 3, saving model\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 3.4. Train Segmentation","metadata":{"id":"0T2vcliPw5Bj"}},{"cell_type":"code","source":"","metadata":{"id":"wcIG0NgaELJi","execution":{"iopub.status.busy":"2024-01-11T04:53:18.871034Z","iopub.execute_input":"2024-01-11T04:53:18.871437Z","iopub.status.idle":"2024-01-11T04:53:18.875784Z","shell.execute_reply.started":"2024-01-11T04:53:18.871381Z","shell.execute_reply":"2024-01-11T04:53:18.874890Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"\ndef train(folds):\n    for fold in range(N_FOLDS):\n        #TASK\n        TASK = \"segmentation\"\n\n        #Path\n        weight_dir, log_dir, logger_name = init_path(TASK)\n\n\n        #Model\n        model = segmentation_model().to(DEVICE)\n\n        #Loss & Optimizer\n        model = model.to(DEVICE)\n        dice_loss = smp.losses.DiceLoss(mode='binary', from_logits=True)\n        optimizer = optim.Adam(model.parameters(), lr=BASE_LR, weight_decay=1e-5)\n\n\n        #Meters\n        overall_meter = AverageMeter()\n        iou_meter = AverageMeter()\n        dice_meter = AverageMeter()\n        train_loss_meter = AverageMeter()\n        val_loss_meter = AverageMeter()\n        precision_meter = AverageMeter()\n        recall_meter = AverageMeter()\n        f1_score_meter = AverageMeter()\n        \n        train_images, val_images = folds[fold]\n        train_set = BUSI(DATASET_DIR, train_images, val_images, input_size=INPUT_SIZE,transform=transform, target_transform=None, is_train=True)\n        val_set = BUSI(DATASET_DIR, train_images, val_images, input_size=INPUT_SIZE,transform=transform, target_transform=None, is_train=False)\n\n        train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n        val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n\n        logger = setup_logger(logger_name, log_dir)\n        stale = 0\n        best_overall = 0\n        start_epoch = 1\n\n        if CHECKPOINT is not None:\n            if os.path.exists(CHECKPOINT):\n                checkpoint = torch.load(CHECKPOINT)\n                model.load_state_dict(checkpoint['model_state_dict'])\n                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n                start_epoch = checkpoint['epoch']\n                best_overall = checkpoint['best_overall']\n                logger.info(f\"Resume training from epoch {start_epoch}\")\n            else:\n                logger.info(f\"Checkpoint not found, start training from epoch 1\")\n        #Logging hyperparameters\n        logging_hyperparameters(logger)\n\n        for epoch in range(start_epoch, 1+MAX_EPOCHS):\n            start_time = time.time()\n            #Train\n            model.train()\n            #Reset meters\n            overall_meter.reset()\n            train_loss_meter.reset()\n            val_loss_meter.reset()\n\n            iou_meter.reset()\n            dice_meter.reset()\n            precision_meter.reset()\n            recall_meter.reset()\n            f1_score_meter.reset()\n\n            logger.info(\"Start training\")\n            for batch_idx, (image, mask, _) in enumerate(train_loader):\n                n = image.shape[0]\n                optimizer.zero_grad()\n                image = image.to(DEVICE)\n                mask = mask.to(DEVICE)\n\n                output = model(image) #Logits\n                #Cal loss\n                train_loss = dice_loss(output, mask)\n                train_loss.backward()\n                optimizer.step()\n\n                train_loss_meter.update(train_loss.item(),n)\n\n                if batch_idx % 10 == 0:\n                    logger.info(f\"Epoch[{epoch}] - Fold[{fold}] - Iteration[{batch_idx}/{len(train_loader)}] Loss: {train_loss:.3f}\")\n            end_time = time.time()\n            logger.info(f\"Training Result: Epoch {epoch}/{MAX_EPOCHS} - Fold {fold}/{N_FOLDS}, Loss: {train_loss_meter.avg:.3f}, Time epoch: {end_time-start_time:.3f}s\")\n\n            #Valid\n            model.eval()\n            with torch.no_grad():\n                for batch_idx, (image, mask, _) in enumerate(val_loader):\n                    n = image.shape[0]\n                    image = image.to(DEVICE)\n                    mask = mask.to(DEVICE)\n\n                    output = model(image)\n                    val_loss = dice_loss(output, mask)\n\n                    # #Calculate metrics\n                    mask = F.sigmoid(mask).round().long()\n                    tp, fp, fn, tn = smp.metrics.get_stats(output, mask, mode='binary', threshold=0.5)\n\n\n                    iou_score = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"macro\")\n\n                    dice_score = torch.mean((2*tp.sum(0)/(2*tp.sum(0) + fp.sum(0) + fn.sum(0) + 1e-5)))\n                    precision_score = smp.metrics.precision(tp, fp, fn, tn, reduction=\"macro\")\n                    recall_score = smp.metrics.recall(tp, fp, fn, tn, reduction=\"macro\")\n                    f1_score = smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"macro\")\n\n\n                    #Update meters\n                    val_loss_meter.update(val_loss.item(), n)\n\n                    iou_meter.update(iou_score.item(), n)\n                    dice_meter.update(dice_score.item(), n)\n                    precision_meter.update(precision_score.item(), n)\n                    recall_meter.update(recall_score.item(), n)\n                    f1_score_meter.update(f1_score.item(), n)\n\n                    #Overall score\n                    overall_score = (iou_score + dice_score + f1_score)/3\n                    overall_meter.update(overall_score.item(), n)\n\n            logger.info(f\"Validation Result: Dice Loss: {val_loss_meter.avg:.3f}, IoU: {iou_meter.avg:.3f}, Dice Score: {dice_meter.avg:.3f}, F1-Score: {f1_score_meter.avg:.3f}, Average Score: {overall_meter.avg:.3f}\")\n\n            #Save best model\n            to_save = {\n                    'epoch': epoch,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'best_overall': best_overall,\n                }\n            if overall_meter.avg > best_overall: # best base on IoU score\n                logger.info(f\"Best model found at epoch {epoch}, saving model\")\n\n                torch.save(to_save, os.path.join(weight_dir,f\"best_epoch{epoch}_fold{fold}_{INPUT_SIZE[0]}_BS={BATCH_SIZE}_average={overall_meter.avg:.3f}.pth\"))\n                best_overall = overall_meter.avg\n                stale = 0\n            else:\n                stale += 1\n                if stale > 300:\n                    logger.info(f\"No improvement {300} consecutive epochs, early stopping\")\n                    break\n            if epoch % SAVE_INTERVAL == 0 or epoch == MAX_EPOCHS:\n                logger.info(f\"Save model at epoch {epoch}, saving model\")\n                torch.save(to_save, os.path.join(weight_dir,f\"epoch{epoch}_fold{fold}.pth\"))\n","metadata":{"id":"mFcBifzMw8wB","execution":{"iopub.status.busy":"2024-01-11T04:53:18.877026Z","iopub.execute_input":"2024-01-11T04:53:18.877334Z","iopub.status.idle":"2024-01-11T04:53:18.902277Z","shell.execute_reply.started":"2024-01-11T04:53:18.877310Z","shell.execute_reply":"2024-01-11T04:53:18.901338Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"train(folds)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I76QY9xXw_ev","outputId":"4cc94ace-4f36-49ad-c37f-ae5c382feb4a","execution":{"iopub.status.busy":"2024-01-11T04:53:18.903320Z","iopub.execute_input":"2024-01-11T04:53:18.903619Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"2024-01-11 04:53:18,918 segmentation_efficientnet-b4_deeplabv3plus INFO: ==========Hyperparameters==========\n2024-01-11 04:53:18,919 segmentation_efficientnet-b4_deeplabv3plus INFO: Device: cuda\n2024-01-11 04:53:18,920 segmentation_efficientnet-b4_deeplabv3plus INFO: Architecture: deeplabv3plus\n2024-01-11 04:53:18,921 segmentation_efficientnet-b4_deeplabv3plus INFO: Encoder: efficientnet-b4\n2024-01-11 04:53:18,921 segmentation_efficientnet-b4_deeplabv3plus INFO: Encoder weight: imagenet\n2024-01-11 04:53:18,922 segmentation_efficientnet-b4_deeplabv3plus INFO: Input size: (448, 448)\n2024-01-11 04:53:18,923 segmentation_efficientnet-b4_deeplabv3plus INFO: Batch size: 8\n2024-01-11 04:53:18,925 segmentation_efficientnet-b4_deeplabv3plus INFO: Base learning rate: 0.01\n2024-01-11 04:53:18,925 segmentation_efficientnet-b4_deeplabv3plus INFO: Max epochs: 10\n2024-01-11 04:53:18,926 segmentation_efficientnet-b4_deeplabv3plus INFO: Weight decay: 1e-05\n2024-01-11 04:53:18,927 segmentation_efficientnet-b4_deeplabv3plus INFO: ===================================\n2024-01-11 04:53:18,932 segmentation_efficientnet-b4_deeplabv3plus INFO: Start training\n2024-01-11 04:53:19,705 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[1] - Fold[0] - Iteration[0/52] Loss: 0.542\n2024-01-11 04:53:26,403 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[1] - Fold[0] - Iteration[10/52] Loss: 0.109\n2024-01-11 04:53:33,128 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[1] - Fold[0] - Iteration[20/52] Loss: 0.040\n2024-01-11 04:53:39,813 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[1] - Fold[0] - Iteration[30/52] Loss: 0.071\n2024-01-11 04:53:46,557 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[1] - Fold[0] - Iteration[40/52] Loss: 0.063\n2024-01-11 04:53:53,252 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[1] - Fold[0] - Iteration[50/52] Loss: 0.056\n2024-01-11 04:53:53,841 segmentation_efficientnet-b4_deeplabv3plus INFO: Training Result: Epoch 1/10 - Fold 0/3, Loss: 0.097, Time epoch: 34.913s\n2024-01-11 04:54:01,402 segmentation_efficientnet-b4_deeplabv3plus INFO: Validation Result: Dice Loss: 0.083, IoU: 0.535, Dice Score: 0.478, F1-Score: 0.555, Average Score: 0.523\n2024-01-11 04:54:01,409 segmentation_efficientnet-b4_deeplabv3plus INFO: Best model found at epoch 1, saving model\n2024-01-11 04:54:01,890 segmentation_efficientnet-b4_deeplabv3plus INFO: Start training\n2024-01-11 04:54:02,585 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[2] - Fold[0] - Iteration[0/52] Loss: 0.096\n2024-01-11 04:54:09,364 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[2] - Fold[0] - Iteration[10/52] Loss: 0.072\n2024-01-11 04:54:16,056 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[2] - Fold[0] - Iteration[20/52] Loss: 0.087\n2024-01-11 04:54:22,788 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[2] - Fold[0] - Iteration[30/52] Loss: 0.072\n2024-01-11 04:54:29,424 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[2] - Fold[0] - Iteration[40/52] Loss: 0.159\n2024-01-11 04:54:36,136 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[2] - Fold[0] - Iteration[50/52] Loss: 0.056\n2024-01-11 04:54:36,728 segmentation_efficientnet-b4_deeplabv3plus INFO: Training Result: Epoch 2/10 - Fold 0/3, Loss: 0.075, Time epoch: 34.841s\n2024-01-11 04:54:44,249 segmentation_efficientnet-b4_deeplabv3plus INFO: Validation Result: Dice Loss: 0.083, IoU: 0.535, Dice Score: 0.478, F1-Score: 0.555, Average Score: 0.523\n2024-01-11 04:54:44,261 segmentation_efficientnet-b4_deeplabv3plus INFO: Start training\n2024-01-11 04:54:44,918 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[3] - Fold[0] - Iteration[0/52] Loss: 0.109\n2024-01-11 04:54:51,635 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[3] - Fold[0] - Iteration[10/52] Loss: 0.048\n2024-01-11 04:54:58,347 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[3] - Fold[0] - Iteration[20/52] Loss: 0.071\n2024-01-11 04:55:05,068 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[3] - Fold[0] - Iteration[30/52] Loss: 0.092\n2024-01-11 04:55:11,777 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[3] - Fold[0] - Iteration[40/52] Loss: 0.047\n2024-01-11 04:55:18,435 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[3] - Fold[0] - Iteration[50/52] Loss: 0.084\n2024-01-11 04:55:19,050 segmentation_efficientnet-b4_deeplabv3plus INFO: Training Result: Epoch 3/10 - Fold 0/3, Loss: 0.075, Time epoch: 34.793s\n2024-01-11 04:55:26,701 segmentation_efficientnet-b4_deeplabv3plus INFO: Validation Result: Dice Loss: 0.083, IoU: 0.535, Dice Score: 0.478, F1-Score: 0.555, Average Score: 0.523\n2024-01-11 04:55:26,712 segmentation_efficientnet-b4_deeplabv3plus INFO: Start training\n2024-01-11 04:55:27,378 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[4] - Fold[0] - Iteration[0/52] Loss: 0.114\n2024-01-11 04:55:34,068 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[4] - Fold[0] - Iteration[10/52] Loss: 0.066\n2024-01-11 04:55:40,731 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[4] - Fold[0] - Iteration[20/52] Loss: 0.082\n2024-01-11 04:55:47,427 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[4] - Fold[0] - Iteration[30/52] Loss: 0.080\n2024-01-11 04:55:54,163 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[4] - Fold[0] - Iteration[40/52] Loss: 0.106\n2024-01-11 04:56:00,841 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[4] - Fold[0] - Iteration[50/52] Loss: 0.117\n2024-01-11 04:56:01,424 segmentation_efficientnet-b4_deeplabv3plus INFO: Training Result: Epoch 4/10 - Fold 0/3, Loss: 0.075, Time epoch: 34.714s\n2024-01-11 04:56:08,961 segmentation_efficientnet-b4_deeplabv3plus INFO: Validation Result: Dice Loss: 0.083, IoU: 0.535, Dice Score: 0.478, F1-Score: 0.555, Average Score: 0.523\n2024-01-11 04:56:08,974 segmentation_efficientnet-b4_deeplabv3plus INFO: Start training\n2024-01-11 04:56:09,640 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[5] - Fold[0] - Iteration[0/52] Loss: 0.055\n2024-01-11 04:56:16,346 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[5] - Fold[0] - Iteration[10/52] Loss: 0.054\n2024-01-11 04:56:23,002 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[5] - Fold[0] - Iteration[20/52] Loss: 0.112\n2024-01-11 04:56:29,740 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[5] - Fold[0] - Iteration[30/52] Loss: 0.084\n2024-01-11 04:56:36,451 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[5] - Fold[0] - Iteration[40/52] Loss: 0.137\n2024-01-11 04:56:43,128 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[5] - Fold[0] - Iteration[50/52] Loss: 0.058\n2024-01-11 04:56:43,736 segmentation_efficientnet-b4_deeplabv3plus INFO: Training Result: Epoch 5/10 - Fold 0/3, Loss: 0.075, Time epoch: 34.766s\n2024-01-11 04:56:51,308 segmentation_efficientnet-b4_deeplabv3plus INFO: Validation Result: Dice Loss: 0.083, IoU: 0.535, Dice Score: 0.478, F1-Score: 0.555, Average Score: 0.523\n2024-01-11 04:56:51,319 segmentation_efficientnet-b4_deeplabv3plus INFO: Start training\n2024-01-11 04:56:51,989 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[6] - Fold[0] - Iteration[0/52] Loss: 0.063\n2024-01-11 04:56:58,732 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[6] - Fold[0] - Iteration[10/52] Loss: 0.040\n2024-01-11 04:57:05,376 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[6] - Fold[0] - Iteration[20/52] Loss: 0.092\n2024-01-11 04:57:12,076 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[6] - Fold[0] - Iteration[30/52] Loss: 0.079\n2024-01-11 04:57:18,751 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[6] - Fold[0] - Iteration[40/52] Loss: 0.119\n2024-01-11 04:57:25,427 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[6] - Fold[0] - Iteration[50/52] Loss: 0.041\n2024-01-11 04:57:26,030 segmentation_efficientnet-b4_deeplabv3plus INFO: Training Result: Epoch 6/10 - Fold 0/3, Loss: 0.075, Time epoch: 34.714s\n2024-01-11 04:57:33,741 segmentation_efficientnet-b4_deeplabv3plus INFO: Validation Result: Dice Loss: 0.083, IoU: 0.535, Dice Score: 0.478, F1-Score: 0.555, Average Score: 0.523\n2024-01-11 04:57:33,752 segmentation_efficientnet-b4_deeplabv3plus INFO: Start training\n2024-01-11 04:57:34,408 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[7] - Fold[0] - Iteration[0/52] Loss: 0.046\n2024-01-11 04:57:41,080 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[7] - Fold[0] - Iteration[10/52] Loss: 0.103\n2024-01-11 04:57:47,762 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[7] - Fold[0] - Iteration[20/52] Loss: 0.083\n2024-01-11 04:57:54,452 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[7] - Fold[0] - Iteration[30/52] Loss: 0.119\n2024-01-11 04:58:01,223 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[7] - Fold[0] - Iteration[40/52] Loss: 0.027\n2024-01-11 04:58:07,867 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[7] - Fold[0] - Iteration[50/52] Loss: 0.071\n2024-01-11 04:58:08,446 segmentation_efficientnet-b4_deeplabv3plus INFO: Training Result: Epoch 7/10 - Fold 0/3, Loss: 0.075, Time epoch: 34.697s\n2024-01-11 04:58:16,011 segmentation_efficientnet-b4_deeplabv3plus INFO: Validation Result: Dice Loss: 0.083, IoU: 0.535, Dice Score: 0.478, F1-Score: 0.555, Average Score: 0.523\n2024-01-11 04:58:16,022 segmentation_efficientnet-b4_deeplabv3plus INFO: Start training\n2024-01-11 04:58:16,693 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[8] - Fold[0] - Iteration[0/52] Loss: 0.075\n2024-01-11 04:58:23,390 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[8] - Fold[0] - Iteration[10/52] Loss: 0.108\n2024-01-11 04:58:30,079 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[8] - Fold[0] - Iteration[20/52] Loss: 0.066\n2024-01-11 04:58:36,808 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[8] - Fold[0] - Iteration[30/52] Loss: 0.084\n2024-01-11 04:58:43,601 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[8] - Fold[0] - Iteration[40/52] Loss: 0.027\n2024-01-11 04:58:50,207 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[8] - Fold[0] - Iteration[50/52] Loss: 0.122\n2024-01-11 04:58:50,786 segmentation_efficientnet-b4_deeplabv3plus INFO: Training Result: Epoch 8/10 - Fold 0/3, Loss: 0.075, Time epoch: 34.767s\n2024-01-11 04:58:58,855 segmentation_efficientnet-b4_deeplabv3plus INFO: Validation Result: Dice Loss: 0.083, IoU: 0.535, Dice Score: 0.478, F1-Score: 0.555, Average Score: 0.523\n2024-01-11 04:58:58,867 segmentation_efficientnet-b4_deeplabv3plus INFO: Start training\n2024-01-11 04:58:59,531 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[9] - Fold[0] - Iteration[0/52] Loss: 0.112\n2024-01-11 04:59:06,376 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[9] - Fold[0] - Iteration[10/52] Loss: 0.069\n2024-01-11 04:59:13,104 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[9] - Fold[0] - Iteration[20/52] Loss: 0.059\n2024-01-11 04:59:19,912 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[9] - Fold[0] - Iteration[30/52] Loss: 0.102\n2024-01-11 04:59:26,624 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[9] - Fold[0] - Iteration[40/52] Loss: 0.042\n2024-01-11 04:59:33,328 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[9] - Fold[0] - Iteration[50/52] Loss: 0.030\n2024-01-11 04:59:33,924 segmentation_efficientnet-b4_deeplabv3plus INFO: Training Result: Epoch 9/10 - Fold 0/3, Loss: 0.075, Time epoch: 35.060s\n2024-01-11 04:59:41,460 segmentation_efficientnet-b4_deeplabv3plus INFO: Validation Result: Dice Loss: 0.083, IoU: 0.535, Dice Score: 0.478, F1-Score: 0.555, Average Score: 0.523\n2024-01-11 04:59:41,471 segmentation_efficientnet-b4_deeplabv3plus INFO: Start training\n2024-01-11 04:59:42,160 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[10] - Fold[0] - Iteration[0/52] Loss: 0.078\n2024-01-11 04:59:48,864 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[10] - Fold[0] - Iteration[10/52] Loss: 0.044\n2024-01-11 04:59:55,542 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[10] - Fold[0] - Iteration[20/52] Loss: 0.052\n2024-01-11 05:00:02,257 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[10] - Fold[0] - Iteration[30/52] Loss: 0.038\n2024-01-11 05:00:09,021 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[10] - Fold[0] - Iteration[40/52] Loss: 0.129\n2024-01-11 05:00:15,706 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[10] - Fold[0] - Iteration[50/52] Loss: 0.099\n2024-01-11 05:00:16,293 segmentation_efficientnet-b4_deeplabv3plus INFO: Training Result: Epoch 10/10 - Fold 0/3, Loss: 0.075, Time epoch: 34.825s\n2024-01-11 05:00:23,836 segmentation_efficientnet-b4_deeplabv3plus INFO: Validation Result: Dice Loss: 0.083, IoU: 0.535, Dice Score: 0.478, F1-Score: 0.555, Average Score: 0.523\n2024-01-11 05:00:23,844 segmentation_efficientnet-b4_deeplabv3plus INFO: Save model at epoch 10, saving model\n2024-01-11 05:00:24,308 segmentation_efficientnet-b4_deeplabv3plus INFO: ==========Hyperparameters==========\n2024-01-11 05:00:24,308 segmentation_efficientnet-b4_deeplabv3plus INFO: ==========Hyperparameters==========\n2024-01-11 05:00:24,310 segmentation_efficientnet-b4_deeplabv3plus INFO: Device: cuda\n2024-01-11 05:00:24,310 segmentation_efficientnet-b4_deeplabv3plus INFO: Device: cuda\n2024-01-11 05:00:24,312 segmentation_efficientnet-b4_deeplabv3plus INFO: Architecture: deeplabv3plus\n2024-01-11 05:00:24,312 segmentation_efficientnet-b4_deeplabv3plus INFO: Architecture: deeplabv3plus\n2024-01-11 05:00:24,315 segmentation_efficientnet-b4_deeplabv3plus INFO: Encoder: efficientnet-b4\n2024-01-11 05:00:24,315 segmentation_efficientnet-b4_deeplabv3plus INFO: Encoder: efficientnet-b4\n2024-01-11 05:00:24,316 segmentation_efficientnet-b4_deeplabv3plus INFO: Encoder weight: imagenet\n2024-01-11 05:00:24,316 segmentation_efficientnet-b4_deeplabv3plus INFO: Encoder weight: imagenet\n2024-01-11 05:00:24,318 segmentation_efficientnet-b4_deeplabv3plus INFO: Input size: (448, 448)\n2024-01-11 05:00:24,318 segmentation_efficientnet-b4_deeplabv3plus INFO: Input size: (448, 448)\n2024-01-11 05:00:24,320 segmentation_efficientnet-b4_deeplabv3plus INFO: Batch size: 8\n2024-01-11 05:00:24,320 segmentation_efficientnet-b4_deeplabv3plus INFO: Batch size: 8\n2024-01-11 05:00:24,322 segmentation_efficientnet-b4_deeplabv3plus INFO: Base learning rate: 0.01\n2024-01-11 05:00:24,322 segmentation_efficientnet-b4_deeplabv3plus INFO: Base learning rate: 0.01\n2024-01-11 05:00:24,324 segmentation_efficientnet-b4_deeplabv3plus INFO: Max epochs: 10\n2024-01-11 05:00:24,324 segmentation_efficientnet-b4_deeplabv3plus INFO: Max epochs: 10\n2024-01-11 05:00:24,326 segmentation_efficientnet-b4_deeplabv3plus INFO: Weight decay: 1e-05\n2024-01-11 05:00:24,326 segmentation_efficientnet-b4_deeplabv3plus INFO: Weight decay: 1e-05\n2024-01-11 05:00:24,328 segmentation_efficientnet-b4_deeplabv3plus INFO: ===================================\n2024-01-11 05:00:24,328 segmentation_efficientnet-b4_deeplabv3plus INFO: ===================================\n2024-01-11 05:00:24,332 segmentation_efficientnet-b4_deeplabv3plus INFO: Start training\n2024-01-11 05:00:24,332 segmentation_efficientnet-b4_deeplabv3plus INFO: Start training\n2024-01-11 05:00:25,011 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[1] - Fold[1] - Iteration[0/52] Loss: 0.055\n2024-01-11 05:00:25,011 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[1] - Fold[1] - Iteration[0/52] Loss: 0.055\n2024-01-11 05:00:31,737 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[1] - Fold[1] - Iteration[10/52] Loss: 0.039\n2024-01-11 05:00:31,737 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[1] - Fold[1] - Iteration[10/52] Loss: 0.039\n2024-01-11 05:00:38,473 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[1] - Fold[1] - Iteration[20/52] Loss: 0.091\n2024-01-11 05:00:38,473 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[1] - Fold[1] - Iteration[20/52] Loss: 0.091\n2024-01-11 05:00:45,172 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[1] - Fold[1] - Iteration[30/52] Loss: 0.069\n2024-01-11 05:00:45,172 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[1] - Fold[1] - Iteration[30/52] Loss: 0.069\n2024-01-11 05:00:51,830 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[1] - Fold[1] - Iteration[40/52] Loss: 0.094\n2024-01-11 05:00:51,830 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[1] - Fold[1] - Iteration[40/52] Loss: 0.094\n2024-01-11 05:00:58,457 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[1] - Fold[1] - Iteration[50/52] Loss: 0.147\n2024-01-11 05:00:58,457 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[1] - Fold[1] - Iteration[50/52] Loss: 0.147\n2024-01-11 05:00:59,053 segmentation_efficientnet-b4_deeplabv3plus INFO: Training Result: Epoch 1/10 - Fold 1/3, Loss: 0.082, Time epoch: 34.724s\n2024-01-11 05:00:59,053 segmentation_efficientnet-b4_deeplabv3plus INFO: Training Result: Epoch 1/10 - Fold 1/3, Loss: 0.082, Time epoch: 34.724s\n2024-01-11 05:01:06,703 segmentation_efficientnet-b4_deeplabv3plus INFO: Validation Result: Dice Loss: 0.069, IoU: 0.542, Dice Score: 0.482, F1-Score: 0.559, Average Score: 0.528\n2024-01-11 05:01:06,703 segmentation_efficientnet-b4_deeplabv3plus INFO: Validation Result: Dice Loss: 0.069, IoU: 0.542, Dice Score: 0.482, F1-Score: 0.559, Average Score: 0.528\n2024-01-11 05:01:06,711 segmentation_efficientnet-b4_deeplabv3plus INFO: Best model found at epoch 1, saving model\n2024-01-11 05:01:06,711 segmentation_efficientnet-b4_deeplabv3plus INFO: Best model found at epoch 1, saving model\n2024-01-11 05:01:07,374 segmentation_efficientnet-b4_deeplabv3plus INFO: Start training\n2024-01-11 05:01:07,374 segmentation_efficientnet-b4_deeplabv3plus INFO: Start training\n2024-01-11 05:01:08,028 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[2] - Fold[1] - Iteration[0/52] Loss: 0.204\n2024-01-11 05:01:08,028 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[2] - Fold[1] - Iteration[0/52] Loss: 0.204\n2024-01-11 05:01:14,746 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[2] - Fold[1] - Iteration[10/52] Loss: 0.133\n2024-01-11 05:01:14,746 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[2] - Fold[1] - Iteration[10/52] Loss: 0.133\n2024-01-11 05:01:21,460 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[2] - Fold[1] - Iteration[20/52] Loss: 0.106\n2024-01-11 05:01:21,460 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[2] - Fold[1] - Iteration[20/52] Loss: 0.106\n2024-01-11 05:01:28,184 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[2] - Fold[1] - Iteration[30/52] Loss: 0.105\n2024-01-11 05:01:28,184 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[2] - Fold[1] - Iteration[30/52] Loss: 0.105\n2024-01-11 05:01:34,938 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[2] - Fold[1] - Iteration[40/52] Loss: 0.111\n2024-01-11 05:01:34,938 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[2] - Fold[1] - Iteration[40/52] Loss: 0.111\n2024-01-11 05:01:41,797 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[2] - Fold[1] - Iteration[50/52] Loss: 0.084\n2024-01-11 05:01:41,797 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[2] - Fold[1] - Iteration[50/52] Loss: 0.084\n2024-01-11 05:01:42,382 segmentation_efficientnet-b4_deeplabv3plus INFO: Training Result: Epoch 2/10 - Fold 1/3, Loss: 0.082, Time epoch: 35.012s\n2024-01-11 05:01:42,382 segmentation_efficientnet-b4_deeplabv3plus INFO: Training Result: Epoch 2/10 - Fold 1/3, Loss: 0.082, Time epoch: 35.012s\n2024-01-11 05:01:50,048 segmentation_efficientnet-b4_deeplabv3plus INFO: Validation Result: Dice Loss: 0.069, IoU: 0.542, Dice Score: 0.482, F1-Score: 0.559, Average Score: 0.528\n2024-01-11 05:01:50,048 segmentation_efficientnet-b4_deeplabv3plus INFO: Validation Result: Dice Loss: 0.069, IoU: 0.542, Dice Score: 0.482, F1-Score: 0.559, Average Score: 0.528\n2024-01-11 05:01:50,061 segmentation_efficientnet-b4_deeplabv3plus INFO: Start training\n2024-01-11 05:01:50,061 segmentation_efficientnet-b4_deeplabv3plus INFO: Start training\n2024-01-11 05:01:50,745 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[3] - Fold[1] - Iteration[0/52] Loss: 0.046\n2024-01-11 05:01:50,745 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[3] - Fold[1] - Iteration[0/52] Loss: 0.046\n2024-01-11 05:01:57,371 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[3] - Fold[1] - Iteration[10/52] Loss: 0.068\n2024-01-11 05:01:57,371 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[3] - Fold[1] - Iteration[10/52] Loss: 0.068\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"id":"C6Kb62cAIwow"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Multitask Model","metadata":{"id":"QAD1fxEdxB6X"}},{"cell_type":"code","source":"RESNET50_ENCODER_WEIGHTS_URL = \"https://download.pytorch.org/models/resnet50-19c8e357.pth\"\n\ndef multitask_model():\n    aux_param=dict(\n                    pooling='avg',             # one of 'avg', 'max'\n                    dropout=0.5,               # dropout ratio, default is None\n                    # activation='sigmoid',      # activation function, default is None\n                    classes=CLA_NUM_CLASSES,      # define number of output labels\n                )\n    model = segmentation_model(aux_param=aux_param)\n    return model","metadata":{"id":"ZRut7ATHEaT2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(folds):\n    for fold in range(N_FOLDS):\n        #Task\n        TASK = \"multitask\"\n\n        #Path\n        weight_dir, log_dir, logger_name = init_path(TASK)\n\n\n        #Model\n        model = multitask_model().to(DEVICE)\n\n        #Loss & Optimizer\n        dice_loss = smp.losses.DiceLoss(mode='binary', from_logits=True)\n        # CE_loss = torch.nn.CrossEntropyLoss()\n\n        optimizer = optim.Adam(model.parameters(), lr=BASE_LR, weight_decay=1e-5)\n\n\n\n        #Common meter\n        overall_meter = AverageMeter()\n        train_loss_meter = AverageMeter()\n        val_loss_meter = AverageMeter()\n\n        #Meters segmentation\n        seg_train_loss_meter = AverageMeter()\n        seg_val_loss_meter = AverageMeter()\n        seg_iou_meter = AverageMeter()\n        seg_dice_meter = AverageMeter()\n        seg_precision_meter = AverageMeter()\n        seg_recall_meter = AverageMeter()\n        seg_f1_score_meter = AverageMeter()\n\n        #Meters classification\n        cla_train_loss_meter = AverageMeter()\n        cla_val_loss_meter = AverageMeter()\n        cla_acc_meter = AverageMeter()\n        cla_precision_meter = AverageMeter()\n        cla_recall_meter = AverageMeter()\n        cla_f1_score_meter = AverageMeter()\n\n        train_images, val_images = folds[fold]\n        train_set = BUSI(DATASET_DIR, train_images, val_images, input_size=INPUT_SIZE,transform=transform, target_transform=None, is_train=True)\n        val_set = BUSI(DATASET_DIR, train_images, val_images, input_size=INPUT_SIZE,transform=transform, target_transform=None, is_train=False)\n\n        train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n        val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n\n                #Setup logging\n        logger = setup_logger(logger_name, log_dir)\n\n        start_epoch=1\n        best_overall = 0\n        stale = 0\n\n        if CHECKPOINT is not None:\n            if os.path.exists(CHECKPOINT):\n                checkpoint = torch.load(CHECKPOINT)\n                model.load_state_dict(checkpoint['model_state_dict'])\n                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n                start_epoch = checkpoint['epoch']\n                best_overall = checkpoint['best_overall']\n                print(f\"Resume training from epoch {start_epoch}\")\n            else:\n                print(f\"Checkpoint not found, start training from epoch 1\")\n\n        #Logging hyperparameters\n        logging_hyperparameters(logger)\n\n        for epoch in range(start_epoch, 1+MAX_EPOCHS):\n            start_time = time.time()\n            #Train\n            model.train()\n\n            #Reset meters\n            #Common meter\n            train_loss_meter.reset()\n            val_loss_meter.reset()\n            overall_meter.reset()\n\n            #Meters segmentation\n            seg_iou_meter.reset()\n            seg_dice_meter.reset()\n            seg_precision_meter.reset()\n            seg_recall_meter.reset()\n            seg_f1_score_meter.reset()\n\n            #Meters classification\n            cla_acc_meter.reset()\n            cla_precision_meter.reset()\n            cla_recall_meter.reset()\n            cla_f1_score_meter.reset()\n\n            logger.info(\"Start training\")\n            for batch_idx, (image, mask, label) in enumerate(train_loader):\n                n = image.shape[0]\n                optimizer.zero_grad()\n                image = image.to(DEVICE)\n                mask = mask.to(DEVICE)\n                label = label.to(DEVICE)\n\n                #Forward\n                output_mask, output_classification = model(image)\n\n                #Cal loss\n                loss_segmentation = dice_loss(output_mask, mask)\n                loss_classification = focal_loss(output_classification, label, alpha=0.25, gamma=2,reduction='mean')\n                train_loss = ALPHA*loss_segmentation + (1 - ALPHA)*loss_classification\n\n                train_loss.backward()\n                optimizer.step()\n                train_loss_meter.update(train_loss.item(), n)\n                seg_train_loss_meter.update(loss_segmentation.item(), n)\n                cla_train_loss_meter.update(loss_classification.item(), n)\n                if batch_idx % 10 == 0:\n                    logger.info(f\"Epoch[{epoch}] Iteration[{batch_idx}/{len(train_loader)}] Loss: {train_loss:.3f}\")\n\n            end_time = time.time()\n            logger.info(f\"Training Result: Epoch {epoch}/{MAX_EPOCHS}, Loss: {train_loss_meter.avg:.3f}  Segmentation loss: {seg_train_loss_meter.avg:.3f} Classification loss: {cla_train_loss_meter.avg:.3f} Time epoch: {end_time-start_time:.3f}s\")\n\n            #Valid\n            model.eval()\n            with torch.no_grad():\n                for batch_idx, (image, mask, label) in enumerate(val_loader):\n                    n = image.shape[0]\n                    image = image.to(DEVICE)\n                    mask = mask.to(DEVICE)\n                    label = label.to(DEVICE)\n\n                    #Forward\n                    output_mask, output_classification = model(image)\n\n                    #Cal loss\n                    loss_segmentation = dice_loss(output_mask, mask)\n                    loss_classification = focal_loss(output_classification, label, alpha=0.25, gamma=2,reduction='mean')\n                    val_loss = ALPHA*loss_segmentation + (1 - ALPHA)*loss_classification\n\n\n                    #Calculate metrics\n                    #Segmentation: iou, dice, p, r, f1\n\n                    mask = F.sigmoid(mask).round().long()\n                    tp, fp, fn, tn = smp.metrics.get_stats(output_mask, mask, mode='binary', threshold=0.5)\n\n\n                    seg_iou_score = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"macro\")\n\n                    seg_dice_score = torch.mean((2*tp.sum(0)/(2*tp.sum(0) + fp.sum(0) + fn.sum(0) + 1e-5)))\n                    seg_precision_score = smp.metrics.precision(tp, fp, fn, tn, reduction=\"macro\")\n                    seg_recall_score = smp.metrics.recall(tp, fp, fn, tn, reduction=\"macro\")\n                    seg_f1_score = smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"macro\")\n\n\n                    #Classification: acc, p, r, f1\n                    label = label.detach().cpu().numpy()\n                    output_classification = output_classification.argmax(1).detach().cpu().numpy()\n\n                    cla_acc = accuracy_score(label, output_classification)\n                    cla_precision_score = precision_score(label, output_classification, average='macro', zero_division=0)\n                    cla_recall_score = recall_score(label, output_classification, average='macro', zero_division=0)\n                    cla_f1_score = f1_score(label, output_classification, average='macro')\n\n\n                    #Update meters\n                    val_loss_meter.update(val_loss.item(), n)\n\n                    #Segmentation\n                    seg_val_loss_meter.update(loss_segmentation.item(), n)\n                    seg_iou_meter.update(seg_iou_score.item(), n)\n                    seg_dice_meter.update(seg_dice_score.item(), n)\n                    seg_precision_meter.update(seg_precision_score.item(), n)\n                    seg_recall_meter.update(seg_recall_score.item(), n)\n                    seg_f1_score_meter.update(seg_f1_score.item(), n)\n\n                    #Classification\n                    cla_val_loss_meter.update(loss_classification.item(), n)\n                    cla_acc_meter.update(cla_acc.item(),n)\n                    cla_precision_meter.update(cla_precision_score.item(), n)\n                    cla_recall_meter.update(cla_recall_score.item(), n)\n                    cla_f1_score_meter.update(cla_f1_score.item(), n)\n\n                    #Common\n                    overall_score = ((seg_iou_score + seg_dice_score + seg_f1_score)/3 + cla_f1_score)/2\n                    overall_meter.update(overall_score.item(), n)\n\n            logger.info(f\"Validation Result: Loss: {val_loss_meter.avg:.3f}, Segmentation loss: {seg_val_loss_meter.avg:.3f} Classification loss: {cla_val_loss_meter.avg:.3f} Overal Score: {overall_meter.avg:.3f}\")\n            logger.info(f\"Classification: Accuracy: {cla_acc_meter.avg:.3f}, F1-Score: {cla_f1_score_meter.avg:.3f}, Precision: {cla_precision_meter.avg:.3f}, Recall: {cla_recall_meter.avg:.3f}\")\n            logger.info(f\"Segmentation: IoU: {seg_iou_meter.avg:.3f} Dice: {seg_dice_meter.avg:.3f}, F1-score: {seg_f1_score_meter.avg:.3f}, Precision: {seg_precision_meter.avg:.3f}, Recall: {seg_recall_meter.avg:.3f}\")\n            #Save best model\n            to_save = {\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'best_overall': best_overall\n            }\n            if overall_meter.avg > best_overall: # best base on IoU score\n                logger.info(f\"Best model found at epoch {epoch}, saving model\")\n                torch.save(to_save, os.path.join(weight_dir,f\"best_{epoch}_{INPUT_SIZE[0]}_BS={BATCH_SIZE}_overal={overall_meter.avg:.3f}.pth\"))\n                best_overall = overall_meter.avg\n                stale = 0\n            else:\n                stale += 1\n                if stale > 300:\n                    logger.info(f\"No improvement {300} consecutive epochs, early stopping\")\n                    break\n            if epoch % SAVE_INTERVAL == 0 or epoch == MAX_EPOCHS:\n                logger.info(f\"Save model at epoch {epoch}, saving model\")\n\n                torch.save(to_save, os.path.join(weight_dir,f\"epoch_{epoch}.pth\"))\n","metadata":{"id":"C-WsmKv8FVQD","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qv7b0JwnFYRl","outputId":"0fd82a38-3ed5-4708-8f78-11c8d65bd26a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"YSQO3Nr8ObDH"},"execution_count":null,"outputs":[]}]}