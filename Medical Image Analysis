{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8703699,"sourceType":"datasetVersion","datasetId":5220395}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 0. Import Libraries + Hyperparams","metadata":{"id":"11-1bHuumCMr"}},{"cell_type":"code","source":"!pip install segmentation_models_pytorch\n!pip install kornia","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H0hJQxk2ra_2","outputId":"2b21aadd-65cd-4a72-b27b-77d6340778c5","execution":{"iopub.status.busy":"2024-06-16T10:26:14.933792Z","iopub.execute_input":"2024-06-16T10:26:14.934033Z","iopub.status.idle":"2024-06-16T10:26:47.094854Z","shell.execute_reply.started":"2024-06-16T10:26:14.934009Z","shell.execute_reply":"2024-06-16T10:26:47.093755Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting segmentation_models_pytorch\n  Downloading segmentation_models_pytorch-0.3.3-py3-none-any.whl.metadata (30 kB)\nRequirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.16.2)\nCollecting pretrainedmodels==0.7.4 (from segmentation_models_pytorch)\n  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting efficientnet-pytorch==0.7.1 (from segmentation_models_pytorch)\n  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting timm==0.9.2 (from segmentation_models_pytorch)\n  Downloading timm-0.9.2-py3-none-any.whl.metadata (68 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.5/68.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (4.66.4)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (9.5.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.2)\nCollecting munch (from pretrainedmodels==0.7.4->segmentation_models_pytorch)\n  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (6.0.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (0.23.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (0.4.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (2.32.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2024.3.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (21.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2024.2.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.3.0)\nDownloading segmentation_models_pytorch-0.3.3-py3-none-any.whl (106 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\nBuilding wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16428 sha256=246f9af988bc26660312d50a35c8729af91db8a20fc17c6cda78438fb5400d30\n  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60945 sha256=52d2f2ae2a27159d5328c1f4d68c8504eb08dfb0eabd34b9b16fdbab13b0055a\n  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\nSuccessfully built efficientnet-pytorch pretrainedmodels\nInstalling collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation_models_pytorch\n  Attempting uninstall: timm\n    Found existing installation: timm 1.0.3\n    Uninstalling timm-1.0.3:\n      Successfully uninstalled timm-1.0.3\nSuccessfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation_models_pytorch-0.3.3 timm-0.9.2\nRequirement already satisfied: kornia in /opt/conda/lib/python3.10/site-packages (0.7.2)\nRequirement already satisfied: kornia-rs>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from kornia) (0.1.3)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from kornia) (21.3)\nRequirement already satisfied: torch>=1.9.1 in /opt/conda/lib/python3.10/site-packages (from kornia) (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.1->kornia) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.1->kornia) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.1->kornia) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.1->kornia) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.1->kornia) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.1->kornia) (2024.3.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->kornia) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.9.1->kornia) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.9.1->kornia) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install torchmetrics","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UfcvAV2ZCvcs","outputId":"fc09fa82-e191-4a9c-b6d4-da271e7e7a10","execution":{"iopub.status.busy":"2024-06-16T10:26:47.097193Z","iopub.execute_input":"2024-06-16T10:26:47.097601Z","iopub.status.idle":"2024-06-16T10:26:59.124419Z","shell.execute_reply.started":"2024-06-16T10:26:47.097567Z","shell.execute_reply":"2024-06-16T10:26:59.123532Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchmetrics in /opt/conda/lib/python3.10/site-packages (1.4.0.post0)\nRequirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (1.26.4)\nRequirement already satisfied: packaging>17.1 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (21.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (2.1.2)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (0.11.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (69.0.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>17.1->torchmetrics) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (2024.3.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport os\nimport cv2\nimport logging\nimport sys\nimport time\nimport torchvision.transforms as T\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nimport segmentation_models_pytorch as smp\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\nfrom kornia.losses import focal_loss\nfrom torchmetrics import Accuracy, Precision, Recall, FBetaScore, Dice, JaccardIndex","metadata":{"id":"mwKc26wsmpc_","execution":{"iopub.status.busy":"2024-06-16T10:26:59.125721Z","iopub.execute_input":"2024-06-16T10:26:59.126008Z","iopub.status.idle":"2024-06-16T10:27:09.441185Z","shell.execute_reply.started":"2024-06-16T10:26:59.125982Z","shell.execute_reply":"2024-06-16T10:27:09.440381Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"ZmXnfJELDGPC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_WORKERS = 0\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n#Solver\nCLASSES = {0: \"Benign\", 1: \"Malignant\", 2: \"Normal\"}\nINPUT_SIZE = (448,448)\nBATCH_SIZE = 8\nBASE_LR = 0.01\nMAX_EPOCHS = 10\nSAVE_INTERVAL = 10\nPATIENCE = 300\nN_FOLDS = 3\n\n\n#Model\nARCH = \"deeplabv3plus\" # chọn giữa ['unet', 'unetpp', , 'fpn', 'deeplabv3plus']\nENCODER_NAME = \"efficientnet-b4\" # chọn giữa các kiến trúc ['resnet50', 'resnext50_32x4d', 'tu-wide_resnet50_2', 'efficientnet-b4']\nIN_CHANNELS = 3\nSEG_NUM_CLASSES = 2\nCLA_NUM_CLASSES = 3\nOUTPUT_ACTIVATION = None #None for logits\n\n#Loss coefficient weight\nALPHA = 0.7\n\n#Path\nOUTPUT_DIR = r\"/kaggle/working/output\"\nDATASET_DIR = r\"/kaggle/input/medical-image/data/train/content/data/train\"\nCHECKPOINT = None\n\n#Eval\nWEIGHT = r\"\"","metadata":{"id":"xlorxtHLsbVk","execution":{"iopub.status.busy":"2024-06-16T10:30:33.177624Z","iopub.execute_input":"2024-06-16T10:30:33.177978Z","iopub.status.idle":"2024-06-16T10:30:33.185051Z","shell.execute_reply.started":"2024-06-16T10:30:33.177954Z","shell.execute_reply":"2024-06-16T10:30:33.184192Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# 1. Utils","metadata":{"id":"QMLdqRrvmuRo"}},{"cell_type":"code","source":"class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\nclass UnNormalize(object):\n    def __init__(self, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor):\n        \"\"\"\n        Args:\n            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n        Returns:\n            Tensor: Normalized image.\n        \"\"\"\n        for t, m, s in zip(tensor, self.mean, self.std):\n            t.mul_(s).add_(m)\n            # The normalize code -> t.sub_(m).div_(s)\n        return tensor","metadata":{"id":"nmYkAUrYvi1x","execution":{"iopub.status.busy":"2024-06-16T10:30:34.091336Z","iopub.execute_input":"2024-06-16T10:30:34.091686Z","iopub.status.idle":"2024-06-16T10:30:34.100570Z","shell.execute_reply.started":"2024-06-16T10:30:34.091660Z","shell.execute_reply":"2024-06-16T10:30:34.099271Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def calculate_overlap_metrics(pred, gt,eps=1e-5):\n    output = pred.view(-1,)\n    target = gt.view(-1,).float()\n\n    tp = torch.sum(output * target)  # TP\n    fp = torch.sum(output * (1 - target))  # FP\n    fn = torch.sum((1 - output) * target)  # FN\n    tn = torch.sum((1 - output) * (1 - target))  # TN\n\n    # pixel_acc = (tp + tn + eps) / (tp + tn + fp + fn + eps)\n    dice = (2 * tp + eps) / (2 * tp + fp + fn + eps)\n    iou = ( tp + eps) / ( tp + fp + fn + eps)\n    precision = (tp + eps) / (tp + fp + eps)\n    recall = (tp + eps) / (tp + fn + eps)\n#     specificity = (tn + eps) / (tn + fp + eps)\n\n    return iou, dice, precision, recall","metadata":{"id":"6QTQH0Q_vawg","execution":{"iopub.status.busy":"2024-06-16T10:30:34.283351Z","iopub.execute_input":"2024-06-16T10:30:34.284042Z","iopub.status.idle":"2024-06-16T10:30:34.290737Z","shell.execute_reply.started":"2024-06-16T10:30:34.284014Z","shell.execute_reply":"2024-06-16T10:30:34.289857Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"\ndef setup_logger(logger_name, output_dir):\n    import os\n    logger = logging.getLogger(logger_name)\n    logger.setLevel(logging.DEBUG)\n    # create file handler which logs even debug messages\n    fh = logging.FileHandler(os.path.join(output_dir, 'log.log'))\n    fh.setLevel(logging.DEBUG)\n    # create console handler with a higher log level\n    ch = logging.StreamHandler()\n    ch.setLevel(logging.DEBUG)\n    # create formatter and add it to the handlers\n    formatter = logging.Formatter('%(asctime)s %(name)s %(levelname)s: %(message)s')\n    fh.setFormatter(formatter)\n    ch.setFormatter(formatter)\n    # add the handlers to logger\n    logger.addHandler(fh)\n    logger.addHandler(ch)\n    return logger\n\n\ndef logging_hyperparameters(logger):\n    logger.info(\"==========Hyperparameters==========\")\n    logger.info(f\"Device: {DEVICE}\")\n    logger.info(f\"Architecture: {ARCH}\")\n    logger.info(f\"Encoder: {ENCODER_NAME}\")\n    logger.info(f\"Encoder weight: imagenet\")\n    logger.info(f\"Input size: {INPUT_SIZE}\")\n    logger.info(f\"Batch size: {BATCH_SIZE}\")\n    logger.info(f\"Base learning rate: {BASE_LR}\")\n    logger.info(f\"Max epochs: {MAX_EPOCHS}\")\n    logger.info(f\"Weight decay: {1e-5}\")\n    logger.info(\"===================================\")\n\n\ndef init_path(task):\n    #Task == classification\n    if task == \"classification\":\n        weight_dir = os.path.join(OUTPUT_DIR, task, ENCODER_NAME)\n        os.makedirs(weight_dir, exist_ok=True)\n        log_dir = weight_dir\n        logger_name = f\"{task}_{ENCODER_NAME}\"\n    elif task == \"segmentation\":\n        weight_dir = os.path.join(OUTPUT_DIR, task, f\"{ENCODER_NAME}_{ARCH}\")\n        os.makedirs(weight_dir, exist_ok=True)\n        log_dir = weight_dir\n        logger_name = f\"{task}_{ENCODER_NAME}_{ARCH}\"\n    elif task == \"multitask\":\n        weight_dir = os.path.join(OUTPUT_DIR, f\"{ENCODER_NAME}_{ARCH}\")\n        os.makedirs(weight_dir, exist_ok=True)\n        log_dir = weight_dir\n        logger_name = f\"{task}_{ENCODER_NAME}_{ARCH}\"\n    return weight_dir, log_dir, logger_name","metadata":{"id":"KopjSjPSvnUd","execution":{"iopub.status.busy":"2024-06-16T10:30:34.494444Z","iopub.execute_input":"2024-06-16T10:30:34.494743Z","iopub.status.idle":"2024-06-16T10:30:34.506130Z","shell.execute_reply.started":"2024-06-16T10:30:34.494718Z","shell.execute_reply":"2024-06-16T10:30:34.505249Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# 2. Setup Data","metadata":{"id":"i7eDsnQFmxRO"}},{"cell_type":"markdown","source":"### 2.1. Download Dataset","metadata":{"id":"bARB4le9rIP9"}},{"cell_type":"code","source":"!mkdir output","metadata":{"id":"IvX1ZbXa0JkV","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cf003043-c354-4491-f949-6d852510d6a4","execution":{"iopub.status.busy":"2024-06-16T10:30:35.099642Z","iopub.execute_input":"2024-06-16T10:30:35.099946Z","iopub.status.idle":"2024-06-16T10:30:36.075998Z","shell.execute_reply.started":"2024-06-16T10:30:35.099922Z","shell.execute_reply":"2024-06-16T10:30:36.074866Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"mkdir: cannot create directory 'output': File exists\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 2.2. Setup dataloader","metadata":{"id":"tzUaztG7u3Mv"}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nimport numpy as np\n\n\ndef split_dataset(dataset_dir):\n    benign, malignant, normal = [], [], []\n    benign_images = [os.path.join(dataset_dir, 'benign', file) for file in os.listdir(os.path.join(dataset_dir, 'benign')) if file.endswith('.png')]\n    malignant_images = [os.path.join(dataset_dir, 'malignant', file) for file in os.listdir(os.path.join(dataset_dir, 'malignant')) if file.endswith('.png')]\n    normal_images = [os.path.join(dataset_dir, 'normal', file) for file in os.listdir(os.path.join(dataset_dir, 'normal')) if file.endswith('.png')]\n\n    for mask in benign_images:\n        if \"_mask\" in mask:\n            image = mask.replace('_mask.png', '.png')\n            benign.append((0, image, mask))\n    for mask in malignant_images:\n        if \"_mask\" in mask:\n            image = mask.replace('_mask.png', '.png')\n            malignant.append((1, image, mask))\n    for mask in normal_images:\n        if \"_mask\" in mask:\n            image = mask.replace('_mask.png', '.png')\n            normal.append((2, image, mask))\n\n    all_data = benign + malignant + normal\n    labels = [item[0] for item in all_data]\n\n    kf = StratifiedKFold(n_splits=3)\n\n    folds = []\n\n    # Splitting data into folds\n    for train_index, val_index in kf.split(np.zeros(len(labels)), labels):\n        train_set = [all_data[i] for i in train_index]\n        val_set = [all_data[i] for i in val_index]\n        folds.append((train_set, val_set))\n\n    return folds","metadata":{"id":"KQolPnCrBZCU","execution":{"iopub.status.busy":"2024-06-16T10:30:36.078137Z","iopub.execute_input":"2024-06-16T10:30:36.078448Z","iopub.status.idle":"2024-06-16T10:30:36.090330Z","shell.execute_reply.started":"2024-06-16T10:30:36.078420Z","shell.execute_reply":"2024-06-16T10:30:36.089423Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"class BUSI(Dataset):\n    def __init__(self, dataset_dir, train_set, val_set, input_size=(512,512), transform=None, target_transform=None, is_train=True):\n        self.input_size = input_size\n        self.dataset_dir = dataset_dir\n        self.is_train = is_train\n        if not os.path.exists(self.dataset_dir):\n            raise ValueError('BUSI dataset not found at {}'.format(self.dataset_dir))\n\n        for _, _, files in os.walk(self.dataset_dir):\n            for file in files:\n                if \"_mask_1\" in file:\n                    raise Exception(\"This class requires BUSI dataset with combined mask. It can be done by running the BUSI() function in the process_data.py at utils folder\")\n\n        self.transform = transform\n        self.target_transform = target_transform\n        self.train_set = train_set\n        self.val_set = val_set\n        if self.is_train:\n            self.images = train_set\n        else:\n            self.images = val_set\n\n\n    def __len__(self):\n        if self.is_train:\n            return len(self.train_set)\n        else:\n            return len(self.val_set)\n\n    def __getitem__(self, idx):\n        label, image_path, mask_path = self.images[idx]\n        image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n\n\n        image = cv2.resize(image, self.input_size, interpolation=cv2.INTER_NEAREST)\n        mask = cv2.resize(mask, self.input_size, interpolation=cv2.INTER_NEAREST)\n\n\n        #Normalize\n        mask = mask/255\n        mask = torch.from_numpy(mask).long()\n        mask = torch.nn.functional.one_hot(mask, num_classes=2).permute(2,0,1).long()\n\n        if self.transform is not None:\n            image = self.transform(image)\n        if self.target_transform is not None:\n            mask = self.target_transform(mask)\n\n        return image, mask, label\n\n    @property\n    def info(self):\n        print(f\"Dataset: BUSI\")\n        print(f\"Train: {len(self.train_set)} images\")\n        print(\"-\"*20)\n        print(f\"Benign: {len([image for image in self.train_set if image[0] == 0])} images\")\n        print(f\"Malignant: {len([image for image in self.train_set if image[0] == 1])} images\")\n        print(f\"Normal: {len([image for image in self.train_set if image[0] == 2])} images\")\n        print(\"-\"*20)\n        print(f\"Val: {len(self.val_set)} images\")\n        print(\"-\"*20)\n        print(f\"Benign: {len([image for image in self.val_set if image[0] == 0])} images\")\n        print(f\"Malignant: {len([image for image in self.val_set if image[0] == 1])} images\")\n        print(f\"Normal: {len([image for image in self.val_set if image[0] == 2])} images\")\n        print(\"-\"*20)\n\n    def _get_images(self):\n        benign, malignant, normal = [], [], []\n        benign_images = [os.path.join(self.dataset_dir, 'benign', file) for file in os.listdir(os.path.join(self.dataset_dir, 'benign')) if file.endswith('.png')]\n        malignant_images = [os.path.join(self.dataset_dir, 'malignant', file) for file in os.listdir(os.path.join(self.dataset_dir, 'malignant')) if file.endswith('.png')]\n        normal_images = [os.path.join(self.dataset_dir, 'normal', file) for file in os.listdir(os.path.join(self.dataset_dir, 'normal')) if file.endswith('.png')]\n\n        for mask in benign_images:\n            if \"_mask\" in mask:\n                image = mask.replace('_mask.png', '.png')\n                benign.append((0, image, mask))\n        for mask in malignant_images:\n            if \"_mask\" in mask:\n                image = mask.replace('_mask.png', '.png')\n                malignant.append((1, image, mask))\n        for mask in normal_images:\n            if \"_mask\" in mask:\n                image = mask.replace('_mask.png', '.png')\n                normal.append((2, image, mask))\n\n        self.b_train_set, self.b_val_set = train_test_split(benign, test_size=0.2, random_state=42)\n        self.m_train_set, self.m_val_set = train_test_split(malignant, test_size=0.2, random_state=42)\n        self.n_train_set, self.n_val_set = train_test_split(normal, test_size=0.2, random_state=42)\n\n        train_set = self.b_train_set + self.m_train_set + self.n_train_set\n        val_set = self.b_val_set + self.m_val_set + self.n_val_set\n        # # without normal class\n        # train_set = b_train_set + m_train_set\n        # val_set = b_val_set + m_val_set\n        return train_set, val_set","metadata":{"id":"DUjnm8dkq3dK","execution":{"iopub.status.busy":"2024-06-16T10:30:36.091811Z","iopub.execute_input":"2024-06-16T10:30:36.092182Z","iopub.status.idle":"2024-06-16T10:30:36.114607Z","shell.execute_reply.started":"2024-06-16T10:30:36.092152Z","shell.execute_reply":"2024-06-16T10:30:36.113765Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"folds = split_dataset(DATASET_DIR)","metadata":{"id":"sRRFFZgZByW-","execution":{"iopub.status.busy":"2024-06-16T10:30:36.116147Z","iopub.execute_input":"2024-06-16T10:30:36.116897Z","iopub.status.idle":"2024-06-16T10:30:36.281234Z","shell.execute_reply.started":"2024-06-16T10:30:36.116873Z","shell.execute_reply":"2024-06-16T10:30:36.280483Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"len(folds)","metadata":{"id":"dACwRFTGCKKl","execution":{"iopub.status.busy":"2024-06-16T10:30:36.282548Z","iopub.execute_input":"2024-06-16T10:30:36.282810Z","iopub.status.idle":"2024-06-16T10:30:36.290224Z","shell.execute_reply.started":"2024-06-16T10:30:36.282787Z","shell.execute_reply":"2024-06-16T10:30:36.289204Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"3"},"metadata":{}}]},{"cell_type":"code","source":"transform = T.Compose([\n                    T.ToTensor(),\n                    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n              ])","metadata":{"id":"5ekjqaNusL1P","execution":{"iopub.status.busy":"2024-06-16T10:30:36.371698Z","iopub.execute_input":"2024-06-16T10:30:36.371955Z","iopub.status.idle":"2024-06-16T10:30:36.376405Z","shell.execute_reply.started":"2024-06-16T10:30:36.371932Z","shell.execute_reply":"2024-06-16T10:30:36.375495Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# 3. Single Model","metadata":{"id":"Xa895_NhtjQ2"}},{"cell_type":"markdown","source":"### 3.1 Setup model","metadata":{"id":"zJbfv6JRuB-r"}},{"cell_type":"code","source":"PRETRAINED_WEIGHT_URL = {\n    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n    'tu-wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n    'efficientnet-b4': 'https://download.pytorch.org/models/efficientnet_b4_rwightman-7eb33cd5.pth',\n}\n\ndef segmentation_model(aux_param=None):\n    assert ARCH in ['unet', 'unetpp', 'deeplabv3plus', 'fpn'], \"Invalid architecture, must be ['unet', 'unetpp', 'deeplabv3plus', 'fpn']\"\n    assert ENCODER_NAME in ['resnet50', 'resnext50_32x4d', 'tu-wide_resnet50_2', 'efficientnet-b4'], \"Invalid encoder name, must be ['resnet50', 'resnext50_32x4d', 'tu-wide_resnet50_2', 'efficientnet-b4']\"\n    #Params\n\n    params = dict(\n        encoder_name = ENCODER_NAME,\n        encoder_depth = 5,\n        encoder_weights = \"imagenet\",\n        in_channels = IN_CHANNELS,\n        classes = SEG_NUM_CLASSES,\n        activation = OUTPUT_ACTIVATION,\n        aux_params = aux_param\n    )\n    MODELS = {\n        'unet':smp.Unet(**params),\n        'unetpp': smp.UnetPlusPlus(**params),\n        'deeplabv3plus': smp.DeepLabV3Plus(**params),\n        'fpn': smp.FPN(**params),\n\n    }\n    return MODELS[ARCH]\n\ndef classification_model():\n    MODELS = {\n        'resnet50': torchvision.models.resnet50(weights='DEFAULT'),\n        'resnext50_32x4d': torchvision.models.resnext50_32x4d(weights='DEFAULT'),\n        'tu-wide_resnet50_2': torchvision.models.wide_resnet50_2(weights='DEFAULT'),\n        'efficientnet-b4': torchvision.models.efficientnet_b4(weights=None),\n    }\n    model = MODELS[ENCODER_NAME]\n\n    # Replace the last layer\n    if ENCODER_NAME == \"efficientnet-b4\":\n        state_dict = torch.hub.load_state_dict_from_url(PRETRAINED_WEIGHT_URL[ENCODER_NAME])\n        model.load_state_dict(state_dict)\n        model.classifier = torch.nn.Linear(1792, CLA_NUM_CLASSES)\n    else:\n        model.fc = torch.nn.Linear(2048, CLA_NUM_CLASSES)\n\n    return model\n\nclass TwoSingleModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.seg_model = segmentation_model()\n        self.cla_model = classification_model()\n\n    def forward(self, x):\n        seg_out = self.seg_model(x)\n        cla_out = self.cla_model(x)\n        return seg_out, cla_out\n","metadata":{"id":"WwbsL3ygt2yW","execution":{"iopub.status.busy":"2024-06-16T10:30:37.342310Z","iopub.execute_input":"2024-06-16T10:30:37.342668Z","iopub.status.idle":"2024-06-16T10:30:37.354094Z","shell.execute_reply.started":"2024-06-16T10:30:37.342641Z","shell.execute_reply":"2024-06-16T10:30:37.353108Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DFVq_wmdt3GZ","outputId":"e0a725b1-8114-4d5d-a667-0762a66e8e07","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2. Training Classification","metadata":{"id":"842kDmqEwIkr"}},{"cell_type":"code","source":"def train(folds):\n    \n    for fold in range(N_FOLDS):\n    \n            #TASK\n        TASK = \"classification\"\n\n        #Path\n        weight_dir, log_dir, logger_name = init_path(TASK)\n\n        #Model\n        model = classification_model().to(DEVICE)\n\n        #Loss & Optimizer\n        model = model.to(DEVICE)\n        optimizer = optim.Adam(model.parameters(), lr=BASE_LR, weight_decay=1e-5)\n\n\n        #Meters\n        train_loss_meter = AverageMeter()\n        val_loss_meter = AverageMeter()\n        acc_meter = AverageMeter()\n        precision_meter = AverageMeter()\n        recall_meter = AverageMeter()\n        f1_score_meter = AverageMeter()\n        \n        train_images, val_images = folds[fold]\n        \n        train_set = BUSI(DATASET_DIR, train_images, val_images, input_size=INPUT_SIZE,transform=transform, target_transform=None, is_train=True)\n        val_set = BUSI(DATASET_DIR, train_images, val_images, input_size=INPUT_SIZE,transform=transform, target_transform=None, is_train=False)\n\n        train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n        val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n\n        logger = setup_logger(logger_name, log_dir)\n        best_f1 = 0\n        stale = 0\n        start_epoch = 1\n\n        if CHECKPOINT is not None:\n            if os.path.exists(CHECKPOINT):\n                checkpoint = torch.load(CHECKPOINT)\n                model.load_state_dict(checkpoint['model_state_dict'])\n                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n                start_epoch = checkpoint['epoch']\n                best_f1 = checkpoint['best_f1']\n                logger.info(f\"Resume training from epoch {start_epoch}\")\n            else:\n                logger.info(f\"Checkpoint not found, start training from epoch 1\")\n\n        #Logging hyperparameters\n        logging_hyperparameters(logger)\n\n\n        for epoch in range(start_epoch, 1+MAX_EPOCHS):\n            #Start time\n            start_time = time.time()\n            #Train\n            model.train()\n            #Reset meters\n            train_loss_meter.reset()\n            precision_meter.reset()\n            recall_meter.reset()\n            f1_score_meter.reset()\n            acc_meter.reset()\n\n            logger.info(\"Start training\")\n            for batch_idx, (image, _, label) in enumerate(train_loader):\n                n = image.shape[0]\n                optimizer.zero_grad()\n                image = image.to(DEVICE)\n                label = label.to(DEVICE)\n\n                output = model(image) #Logits (batch_size,num_classes)\n                #Cal loss\n                train_loss = focal_loss(output, label, alpha=0.25, gamma=2, reduction='mean')\n                train_loss.backward()\n                optimizer.step()\n\n                train_loss_meter.update(train_loss.item(),n)\n\n                if batch_idx % 10 == 0:\n                    logger.info(f\"Epoch[{epoch}] - Fold[{fold}] - Iteration[{batch_idx}/{len(train_loader)}] Loss: {train_loss:.3f}\")\n            end_time = time.time()\n            logger.info(f\"Training Result: Epoch {epoch}/{MAX_EPOCHS} Fold {fold}/{N_FOLDS}, Loss: {train_loss_meter.avg:.3f}, Time epoch: {end_time-start_time:.3f}s\")\n\n            #Valid\n            model.eval()\n            with torch.no_grad():\n                for batch_idx, (image, _, label) in enumerate(val_loader):\n                    n = image.shape[0]\n                    image = image.to(DEVICE)\n                    label = label.to(DEVICE)\n\n                    output = model(image)\n                    val_loss = focal_loss(output, label, alpha=0.25, gamma=2,reduction='mean')\n\n                    #Calculate metrics\n                    #P, R and F1\n                    label = label.detach().cpu().numpy()\n                    output = output.argmax(1).detach().cpu().numpy()\n\n                    p_score = precision_score(label, output, average='macro', zero_division=0)\n                    r_score = recall_score(label, output, average='macro', zero_division=0)\n                    _f1_score = f1_score(label, output, average='macro')\n                    acc = accuracy_score(label, output)\n\n                    #Update meters\n                    val_loss_meter.update(val_loss.item(), n)\n                    acc_meter.update(acc.item(),n)\n                    precision_meter.update(p_score.item(), n)\n                    recall_meter.update(r_score.item(), n)\n                    f1_score_meter.update(_f1_score.item(), n)\n\n            logger.info(f\"Validation Result: Loss: {val_loss_meter.avg:.3f}, Accuracy: {acc_meter.avg:.3f} F1-Score: {f1_score_meter.avg:.3f}, Precision: {precision_meter.avg:.3f}, Recall: {recall_meter.avg:.3f}\")\n\n            #Save best model\n            to_save = {\n                    'epoch': epoch,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'best_f1': best_f1,\n                }\n            if f1_score_meter.avg > best_f1: # best base on IoU score\n                logger.info(f\"Best model found at epoch {epoch}, saving model\")\n                torch.save(to_save, os.path.join(weight_dir,f\"best_epoch{epoch}_fold{fold}_{INPUT_SIZE[0]}_BS={BATCH_SIZE}_f1={f1_score_meter.avg:.3f}.pth\")) # only save best to prevent output memory exceed error\n                best_f1 = f1_score_meter.avg\n                stale = 0\n            else:\n                stale += 1\n                if stale > 300:\n                    logger.info(f\"No improvement {300} consecutive epochs, early stopping\")\n                    break\n            if epoch % SAVE_INTERVAL == 0 or epoch == MAX_EPOCHS:\n                logger.info(f\"Save model at epoch {epoch}, saving model\")\n                torch.save(to_save, os.path.join(weight_dir,f\"epoch_{epoch}_{fold}.pth\"))\n","metadata":{"id":"0FAGnxCDwd1u","execution":{"iopub.status.busy":"2024-06-16T10:30:37.976284Z","iopub.execute_input":"2024-06-16T10:30:37.976647Z","iopub.status.idle":"2024-06-16T10:30:38.001076Z","shell.execute_reply.started":"2024-06-16T10:30:37.976615Z","shell.execute_reply":"2024-06-16T10:30:38.000138Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"train(folds)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"gOVH1a0CwrDq","outputId":"dc1b5074-55da-4f77-d316-db86437b7bd2","execution":{"iopub.status.busy":"2024-06-16T10:30:38.674044Z","iopub.execute_input":"2024-06-16T10:30:38.674645Z","iopub.status.idle":"2024-06-16T10:48:06.530672Z","shell.execute_reply.started":"2024-06-16T10:30:38.674612Z","shell.execute_reply":"2024-06-16T10:48:06.529622Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 159MB/s] \nDownloading: \"https://download.pytorch.org/models/resnext50_32x4d-1a0047aa.pth\" to /root/.cache/torch/hub/checkpoints/resnext50_32x4d-1a0047aa.pth\n100%|██████████| 95.8M/95.8M [00:01<00:00, 95.1MB/s]\nDownloading: \"https://download.pytorch.org/models/wide_resnet50_2-9ba9bcbe.pth\" to /root/.cache/torch/hub/checkpoints/wide_resnet50_2-9ba9bcbe.pth\n100%|██████████| 263M/263M [00:01<00:00, 161MB/s]  \nDownloading: \"https://download.pytorch.org/models/efficientnet_b4_rwightman-7eb33cd5.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b4_rwightman-7eb33cd5.pth\n100%|██████████| 74.5M/74.5M [00:00<00:00, 148MB/s] \n2024-06-16 10:30:46,679 classification_efficientnet-b4 INFO: ==========Hyperparameters==========\n2024-06-16 10:30:46,680 classification_efficientnet-b4 INFO: Device: cuda\n2024-06-16 10:30:46,680 classification_efficientnet-b4 INFO: Architecture: deeplabv3plus\n2024-06-16 10:30:46,681 classification_efficientnet-b4 INFO: Encoder: efficientnet-b4\n2024-06-16 10:30:46,682 classification_efficientnet-b4 INFO: Encoder weight: imagenet\n2024-06-16 10:30:46,682 classification_efficientnet-b4 INFO: Input size: (448, 448)\n2024-06-16 10:30:46,683 classification_efficientnet-b4 INFO: Batch size: 8\n2024-06-16 10:30:46,684 classification_efficientnet-b4 INFO: Base learning rate: 0.01\n2024-06-16 10:30:46,684 classification_efficientnet-b4 INFO: Max epochs: 10\n2024-06-16 10:30:46,686 classification_efficientnet-b4 INFO: Weight decay: 1e-05\n2024-06-16 10:30:46,687 classification_efficientnet-b4 INFO: ===================================\n2024-06-16 10:30:46,692 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:30:48,261 classification_efficientnet-b4 INFO: Epoch[1] - Fold[0] - Iteration[0/52] Loss: 0.083\n2024-06-16 10:30:53,998 classification_efficientnet-b4 INFO: Epoch[1] - Fold[0] - Iteration[10/52] Loss: 0.032\n2024-06-16 10:30:59,656 classification_efficientnet-b4 INFO: Epoch[1] - Fold[0] - Iteration[20/52] Loss: 0.029\n2024-06-16 10:31:05,282 classification_efficientnet-b4 INFO: Epoch[1] - Fold[0] - Iteration[30/52] Loss: 0.082\n2024-06-16 10:31:10,860 classification_efficientnet-b4 INFO: Epoch[1] - Fold[0] - Iteration[40/52] Loss: 0.029\n2024-06-16 10:31:16,513 classification_efficientnet-b4 INFO: Epoch[1] - Fold[0] - Iteration[50/52] Loss: 0.097\n2024-06-16 10:31:17,019 classification_efficientnet-b4 INFO: Training Result: Epoch 1/10 Fold 0/3, Loss: 0.120, Time epoch: 30.331s\n2024-06-16 10:31:24,226 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.048, Accuracy: 0.635 F1-Score: 0.497, Precision: 0.565, Recall: 0.464\n2024-06-16 10:31:24,233 classification_efficientnet-b4 INFO: Best model found at epoch 1, saving model\n2024-06-16 10:31:24,655 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:31:25,158 classification_efficientnet-b4 INFO: Epoch[2] - Fold[0] - Iteration[0/52] Loss: 0.048\n2024-06-16 10:31:30,289 classification_efficientnet-b4 INFO: Epoch[2] - Fold[0] - Iteration[10/52] Loss: 0.068\n2024-06-16 10:31:35,465 classification_efficientnet-b4 INFO: Epoch[2] - Fold[0] - Iteration[20/52] Loss: 0.055\n2024-06-16 10:31:40,621 classification_efficientnet-b4 INFO: Epoch[2] - Fold[0] - Iteration[30/52] Loss: 0.076\n2024-06-16 10:31:45,749 classification_efficientnet-b4 INFO: Epoch[2] - Fold[0] - Iteration[40/52] Loss: 0.028\n2024-06-16 10:31:50,998 classification_efficientnet-b4 INFO: Epoch[2] - Fold[0] - Iteration[50/52] Loss: 0.036\n2024-06-16 10:31:51,472 classification_efficientnet-b4 INFO: Training Result: Epoch 2/10 Fold 0/3, Loss: 0.055, Time epoch: 26.821s\n2024-06-16 10:31:57,589 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.048, Accuracy: 0.562 F1-Score: 0.553, Precision: 0.550, Recall: 0.558\n2024-06-16 10:31:57,597 classification_efficientnet-b4 INFO: Best model found at epoch 2, saving model\n2024-06-16 10:31:58,013 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:31:58,563 classification_efficientnet-b4 INFO: Epoch[3] - Fold[0] - Iteration[0/52] Loss: 0.045\n2024-06-16 10:32:03,769 classification_efficientnet-b4 INFO: Epoch[3] - Fold[0] - Iteration[10/52] Loss: 0.059\n2024-06-16 10:32:09,025 classification_efficientnet-b4 INFO: Epoch[3] - Fold[0] - Iteration[20/52] Loss: 0.026\n2024-06-16 10:32:14,239 classification_efficientnet-b4 INFO: Epoch[3] - Fold[0] - Iteration[30/52] Loss: 0.054\n2024-06-16 10:32:19,543 classification_efficientnet-b4 INFO: Epoch[3] - Fold[0] - Iteration[40/52] Loss: 0.034\n2024-06-16 10:32:24,793 classification_efficientnet-b4 INFO: Epoch[3] - Fold[0] - Iteration[50/52] Loss: 0.027\n2024-06-16 10:32:25,247 classification_efficientnet-b4 INFO: Training Result: Epoch 3/10 Fold 0/3, Loss: 0.046, Time epoch: 27.237s\n2024-06-16 10:32:31,391 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.044, Accuracy: 0.577 F1-Score: 0.566, Precision: 0.608, Recall: 0.565\n2024-06-16 10:32:31,398 classification_efficientnet-b4 INFO: Best model found at epoch 3, saving model\n2024-06-16 10:32:31,818 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:32:32,344 classification_efficientnet-b4 INFO: Epoch[4] - Fold[0] - Iteration[0/52] Loss: 0.035\n2024-06-16 10:32:37,642 classification_efficientnet-b4 INFO: Epoch[4] - Fold[0] - Iteration[10/52] Loss: 0.054\n2024-06-16 10:32:42,970 classification_efficientnet-b4 INFO: Epoch[4] - Fold[0] - Iteration[20/52] Loss: 0.021\n2024-06-16 10:32:48,301 classification_efficientnet-b4 INFO: Epoch[4] - Fold[0] - Iteration[30/52] Loss: 0.037\n2024-06-16 10:32:53,650 classification_efficientnet-b4 INFO: Epoch[4] - Fold[0] - Iteration[40/52] Loss: 0.033\n2024-06-16 10:32:59,010 classification_efficientnet-b4 INFO: Epoch[4] - Fold[0] - Iteration[50/52] Loss: 0.066\n2024-06-16 10:32:59,498 classification_efficientnet-b4 INFO: Training Result: Epoch 4/10 Fold 0/3, Loss: 0.041, Time epoch: 27.684s\n2024-06-16 10:33:05,800 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.078, Accuracy: 0.534 F1-Score: 0.368, Precision: 0.464, Recall: 0.327\n2024-06-16 10:33:05,812 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:33:06,355 classification_efficientnet-b4 INFO: Epoch[5] - Fold[0] - Iteration[0/52] Loss: 0.035\n2024-06-16 10:33:11,669 classification_efficientnet-b4 INFO: Epoch[5] - Fold[0] - Iteration[10/52] Loss: 0.139\n2024-06-16 10:33:17,072 classification_efficientnet-b4 INFO: Epoch[5] - Fold[0] - Iteration[20/52] Loss: 0.049\n2024-06-16 10:33:22,581 classification_efficientnet-b4 INFO: Epoch[5] - Fold[0] - Iteration[30/52] Loss: 0.024\n2024-06-16 10:33:27,917 classification_efficientnet-b4 INFO: Epoch[5] - Fold[0] - Iteration[40/52] Loss: 0.034\n2024-06-16 10:33:33,281 classification_efficientnet-b4 INFO: Epoch[5] - Fold[0] - Iteration[50/52] Loss: 0.034\n2024-06-16 10:33:33,749 classification_efficientnet-b4 INFO: Training Result: Epoch 5/10 Fold 0/3, Loss: 0.053, Time epoch: 27.941s\n2024-06-16 10:33:39,900 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.072, Accuracy: 0.601 F1-Score: 0.407, Precision: 0.444, Recall: 0.382\n2024-06-16 10:33:40,148 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:33:40,684 classification_efficientnet-b4 INFO: Epoch[6] - Fold[0] - Iteration[0/52] Loss: 0.034\n2024-06-16 10:33:46,039 classification_efficientnet-b4 INFO: Epoch[6] - Fold[0] - Iteration[10/52] Loss: 0.037\n2024-06-16 10:33:51,535 classification_efficientnet-b4 INFO: Epoch[6] - Fold[0] - Iteration[20/52] Loss: 0.026\n2024-06-16 10:33:56,857 classification_efficientnet-b4 INFO: Epoch[6] - Fold[0] - Iteration[30/52] Loss: 0.052\n2024-06-16 10:34:02,190 classification_efficientnet-b4 INFO: Epoch[6] - Fold[0] - Iteration[40/52] Loss: 0.018\n2024-06-16 10:34:07,521 classification_efficientnet-b4 INFO: Epoch[6] - Fold[0] - Iteration[50/52] Loss: 0.029\n2024-06-16 10:34:07,982 classification_efficientnet-b4 INFO: Training Result: Epoch 6/10 Fold 0/3, Loss: 0.042, Time epoch: 27.838s\n2024-06-16 10:34:14,137 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.066, Accuracy: 0.562 F1-Score: 0.553, Precision: 0.550, Recall: 0.558\n2024-06-16 10:34:14,148 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:34:14,667 classification_efficientnet-b4 INFO: Epoch[7] - Fold[0] - Iteration[0/52] Loss: 0.035\n2024-06-16 10:34:20,025 classification_efficientnet-b4 INFO: Epoch[7] - Fold[0] - Iteration[10/52] Loss: 0.130\n2024-06-16 10:34:25,421 classification_efficientnet-b4 INFO: Epoch[7] - Fold[0] - Iteration[20/52] Loss: 0.028\n2024-06-16 10:34:30,738 classification_efficientnet-b4 INFO: Epoch[7] - Fold[0] - Iteration[30/52] Loss: 0.064\n2024-06-16 10:34:36,030 classification_efficientnet-b4 INFO: Epoch[7] - Fold[0] - Iteration[40/52] Loss: 0.038\n2024-06-16 10:34:41,426 classification_efficientnet-b4 INFO: Epoch[7] - Fold[0] - Iteration[50/52] Loss: 0.112\n2024-06-16 10:34:41,896 classification_efficientnet-b4 INFO: Training Result: Epoch 7/10 Fold 0/3, Loss: 0.044, Time epoch: 27.752s\n2024-06-16 10:34:48,052 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.064, Accuracy: 0.663 F1-Score: 0.547, Precision: 0.563, Recall: 0.537\n2024-06-16 10:34:48,065 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:34:48,599 classification_efficientnet-b4 INFO: Epoch[8] - Fold[0] - Iteration[0/52] Loss: 0.036\n2024-06-16 10:34:53,990 classification_efficientnet-b4 INFO: Epoch[8] - Fold[0] - Iteration[10/52] Loss: 0.026\n2024-06-16 10:34:59,310 classification_efficientnet-b4 INFO: Epoch[8] - Fold[0] - Iteration[20/52] Loss: 0.025\n2024-06-16 10:35:04,741 classification_efficientnet-b4 INFO: Epoch[8] - Fold[0] - Iteration[30/52] Loss: 0.015\n2024-06-16 10:35:10,071 classification_efficientnet-b4 INFO: Epoch[8] - Fold[0] - Iteration[40/52] Loss: 0.030\n2024-06-16 10:35:15,433 classification_efficientnet-b4 INFO: Epoch[8] - Fold[0] - Iteration[50/52] Loss: 0.072\n2024-06-16 10:35:15,902 classification_efficientnet-b4 INFO: Training Result: Epoch 8/10 Fold 0/3, Loss: 0.039, Time epoch: 27.841s\n2024-06-16 10:35:22,253 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.060, Accuracy: 0.601 F1-Score: 0.549, Precision: 0.589, Recall: 0.538\n2024-06-16 10:35:22,265 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:35:22,832 classification_efficientnet-b4 INFO: Epoch[9] - Fold[0] - Iteration[0/52] Loss: 0.042\n2024-06-16 10:35:28,525 classification_efficientnet-b4 INFO: Epoch[9] - Fold[0] - Iteration[10/52] Loss: 0.089\n2024-06-16 10:35:33,849 classification_efficientnet-b4 INFO: Epoch[9] - Fold[0] - Iteration[20/52] Loss: 0.027\n2024-06-16 10:35:39,204 classification_efficientnet-b4 INFO: Epoch[9] - Fold[0] - Iteration[30/52] Loss: 0.042\n2024-06-16 10:35:44,582 classification_efficientnet-b4 INFO: Epoch[9] - Fold[0] - Iteration[40/52] Loss: 0.074\n2024-06-16 10:35:49,896 classification_efficientnet-b4 INFO: Epoch[9] - Fold[0] - Iteration[50/52] Loss: 0.051\n2024-06-16 10:35:50,373 classification_efficientnet-b4 INFO: Training Result: Epoch 9/10 Fold 0/3, Loss: 0.044, Time epoch: 28.111s\n2024-06-16 10:35:56,655 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.057, Accuracy: 0.716 F1-Score: 0.525, Precision: 0.609, Recall: 0.495\n2024-06-16 10:35:56,667 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:35:57,206 classification_efficientnet-b4 INFO: Epoch[10] - Fold[0] - Iteration[0/52] Loss: 0.030\n2024-06-16 10:36:02,594 classification_efficientnet-b4 INFO: Epoch[10] - Fold[0] - Iteration[10/52] Loss: 0.054\n2024-06-16 10:36:07,911 classification_efficientnet-b4 INFO: Epoch[10] - Fold[0] - Iteration[20/52] Loss: 0.038\n2024-06-16 10:36:13,257 classification_efficientnet-b4 INFO: Epoch[10] - Fold[0] - Iteration[30/52] Loss: 0.020\n2024-06-16 10:36:18,661 classification_efficientnet-b4 INFO: Epoch[10] - Fold[0] - Iteration[40/52] Loss: 0.038\n2024-06-16 10:36:24,031 classification_efficientnet-b4 INFO: Epoch[10] - Fold[0] - Iteration[50/52] Loss: 0.067\n2024-06-16 10:36:24,500 classification_efficientnet-b4 INFO: Training Result: Epoch 10/10 Fold 0/3, Loss: 0.037, Time epoch: 27.837s\n2024-06-16 10:36:30,778 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.057, Accuracy: 0.553 F1-Score: 0.390, Precision: 0.442, Recall: 0.359\n2024-06-16 10:36:30,787 classification_efficientnet-b4 INFO: Save model at epoch 10, saving model\n2024-06-16 10:36:34,170 classification_efficientnet-b4 INFO: ==========Hyperparameters==========\n2024-06-16 10:36:34,170 classification_efficientnet-b4 INFO: ==========Hyperparameters==========\n2024-06-16 10:36:34,172 classification_efficientnet-b4 INFO: Device: cuda\n2024-06-16 10:36:34,172 classification_efficientnet-b4 INFO: Device: cuda\n2024-06-16 10:36:34,174 classification_efficientnet-b4 INFO: Architecture: deeplabv3plus\n2024-06-16 10:36:34,174 classification_efficientnet-b4 INFO: Architecture: deeplabv3plus\n2024-06-16 10:36:34,176 classification_efficientnet-b4 INFO: Encoder: efficientnet-b4\n2024-06-16 10:36:34,176 classification_efficientnet-b4 INFO: Encoder: efficientnet-b4\n2024-06-16 10:36:34,177 classification_efficientnet-b4 INFO: Encoder weight: imagenet\n2024-06-16 10:36:34,177 classification_efficientnet-b4 INFO: Encoder weight: imagenet\n2024-06-16 10:36:34,179 classification_efficientnet-b4 INFO: Input size: (448, 448)\n2024-06-16 10:36:34,179 classification_efficientnet-b4 INFO: Input size: (448, 448)\n2024-06-16 10:36:34,181 classification_efficientnet-b4 INFO: Batch size: 8\n2024-06-16 10:36:34,181 classification_efficientnet-b4 INFO: Batch size: 8\n2024-06-16 10:36:34,183 classification_efficientnet-b4 INFO: Base learning rate: 0.01\n2024-06-16 10:36:34,183 classification_efficientnet-b4 INFO: Base learning rate: 0.01\n2024-06-16 10:36:34,184 classification_efficientnet-b4 INFO: Max epochs: 10\n2024-06-16 10:36:34,184 classification_efficientnet-b4 INFO: Max epochs: 10\n2024-06-16 10:36:34,186 classification_efficientnet-b4 INFO: Weight decay: 1e-05\n2024-06-16 10:36:34,186 classification_efficientnet-b4 INFO: Weight decay: 1e-05\n2024-06-16 10:36:34,188 classification_efficientnet-b4 INFO: ===================================\n2024-06-16 10:36:34,188 classification_efficientnet-b4 INFO: ===================================\n2024-06-16 10:36:34,193 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:36:34,193 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:36:34,781 classification_efficientnet-b4 INFO: Epoch[1] - Fold[1] - Iteration[0/52] Loss: 0.060\n2024-06-16 10:36:34,781 classification_efficientnet-b4 INFO: Epoch[1] - Fold[1] - Iteration[0/52] Loss: 0.060\n2024-06-16 10:36:40,128 classification_efficientnet-b4 INFO: Epoch[1] - Fold[1] - Iteration[10/52] Loss: 0.130\n2024-06-16 10:36:40,128 classification_efficientnet-b4 INFO: Epoch[1] - Fold[1] - Iteration[10/52] Loss: 0.130\n2024-06-16 10:36:45,496 classification_efficientnet-b4 INFO: Epoch[1] - Fold[1] - Iteration[20/52] Loss: 0.136\n2024-06-16 10:36:45,496 classification_efficientnet-b4 INFO: Epoch[1] - Fold[1] - Iteration[20/52] Loss: 0.136\n2024-06-16 10:36:50,899 classification_efficientnet-b4 INFO: Epoch[1] - Fold[1] - Iteration[30/52] Loss: 0.032\n2024-06-16 10:36:50,899 classification_efficientnet-b4 INFO: Epoch[1] - Fold[1] - Iteration[30/52] Loss: 0.032\n2024-06-16 10:36:56,303 classification_efficientnet-b4 INFO: Epoch[1] - Fold[1] - Iteration[40/52] Loss: 0.043\n2024-06-16 10:36:56,303 classification_efficientnet-b4 INFO: Epoch[1] - Fold[1] - Iteration[40/52] Loss: 0.043\n2024-06-16 10:37:01,742 classification_efficientnet-b4 INFO: Epoch[1] - Fold[1] - Iteration[50/52] Loss: 0.084\n2024-06-16 10:37:01,742 classification_efficientnet-b4 INFO: Epoch[1] - Fold[1] - Iteration[50/52] Loss: 0.084\n2024-06-16 10:37:02,222 classification_efficientnet-b4 INFO: Training Result: Epoch 1/10 Fold 1/3, Loss: 0.153, Time epoch: 28.032s\n2024-06-16 10:37:02,222 classification_efficientnet-b4 INFO: Training Result: Epoch 1/10 Fold 1/3, Loss: 0.153, Time epoch: 28.032s\n2024-06-16 10:37:08,466 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.053, Accuracy: 0.558 F1-Score: 0.551, Precision: 0.548, Recall: 0.558\n2024-06-16 10:37:08,466 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.053, Accuracy: 0.558 F1-Score: 0.551, Precision: 0.548, Recall: 0.558\n2024-06-16 10:37:08,477 classification_efficientnet-b4 INFO: Best model found at epoch 1, saving model\n2024-06-16 10:37:08,477 classification_efficientnet-b4 INFO: Best model found at epoch 1, saving model\n2024-06-16 10:37:08,900 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:37:08,900 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:37:09,429 classification_efficientnet-b4 INFO: Epoch[2] - Fold[1] - Iteration[0/52] Loss: 0.031\n2024-06-16 10:37:09,429 classification_efficientnet-b4 INFO: Epoch[2] - Fold[1] - Iteration[0/52] Loss: 0.031\n2024-06-16 10:37:14,791 classification_efficientnet-b4 INFO: Epoch[2] - Fold[1] - Iteration[10/52] Loss: 0.086\n2024-06-16 10:37:14,791 classification_efficientnet-b4 INFO: Epoch[2] - Fold[1] - Iteration[10/52] Loss: 0.086\n2024-06-16 10:37:20,190 classification_efficientnet-b4 INFO: Epoch[2] - Fold[1] - Iteration[20/52] Loss: 0.046\n2024-06-16 10:37:20,190 classification_efficientnet-b4 INFO: Epoch[2] - Fold[1] - Iteration[20/52] Loss: 0.046\n2024-06-16 10:37:25,558 classification_efficientnet-b4 INFO: Epoch[2] - Fold[1] - Iteration[30/52] Loss: 0.108\n2024-06-16 10:37:25,558 classification_efficientnet-b4 INFO: Epoch[2] - Fold[1] - Iteration[30/52] Loss: 0.108\n2024-06-16 10:37:30,986 classification_efficientnet-b4 INFO: Epoch[2] - Fold[1] - Iteration[40/52] Loss: 0.054\n2024-06-16 10:37:30,986 classification_efficientnet-b4 INFO: Epoch[2] - Fold[1] - Iteration[40/52] Loss: 0.054\n2024-06-16 10:37:36,356 classification_efficientnet-b4 INFO: Epoch[2] - Fold[1] - Iteration[50/52] Loss: 0.053\n2024-06-16 10:37:36,356 classification_efficientnet-b4 INFO: Epoch[2] - Fold[1] - Iteration[50/52] Loss: 0.053\n2024-06-16 10:37:36,828 classification_efficientnet-b4 INFO: Training Result: Epoch 2/10 Fold 1/3, Loss: 0.052, Time epoch: 27.931s\n2024-06-16 10:37:36,828 classification_efficientnet-b4 INFO: Training Result: Epoch 2/10 Fold 1/3, Loss: 0.052, Time epoch: 27.931s\n2024-06-16 10:37:43,049 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.048, Accuracy: 0.558 F1-Score: 0.551, Precision: 0.548, Recall: 0.558\n2024-06-16 10:37:43,049 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.048, Accuracy: 0.558 F1-Score: 0.551, Precision: 0.548, Recall: 0.558\n2024-06-16 10:37:43,063 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:37:43,063 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:37:43,618 classification_efficientnet-b4 INFO: Epoch[3] - Fold[1] - Iteration[0/52] Loss: 0.035\n2024-06-16 10:37:43,618 classification_efficientnet-b4 INFO: Epoch[3] - Fold[1] - Iteration[0/52] Loss: 0.035\n2024-06-16 10:37:48,953 classification_efficientnet-b4 INFO: Epoch[3] - Fold[1] - Iteration[10/52] Loss: 0.038\n2024-06-16 10:37:48,953 classification_efficientnet-b4 INFO: Epoch[3] - Fold[1] - Iteration[10/52] Loss: 0.038\n2024-06-16 10:37:54,348 classification_efficientnet-b4 INFO: Epoch[3] - Fold[1] - Iteration[20/52] Loss: 0.043\n2024-06-16 10:37:54,348 classification_efficientnet-b4 INFO: Epoch[3] - Fold[1] - Iteration[20/52] Loss: 0.043\n2024-06-16 10:37:59,746 classification_efficientnet-b4 INFO: Epoch[3] - Fold[1] - Iteration[30/52] Loss: 0.035\n2024-06-16 10:37:59,746 classification_efficientnet-b4 INFO: Epoch[3] - Fold[1] - Iteration[30/52] Loss: 0.035\n2024-06-16 10:38:05,312 classification_efficientnet-b4 INFO: Epoch[3] - Fold[1] - Iteration[40/52] Loss: 0.065\n2024-06-16 10:38:05,312 classification_efficientnet-b4 INFO: Epoch[3] - Fold[1] - Iteration[40/52] Loss: 0.065\n2024-06-16 10:38:10,736 classification_efficientnet-b4 INFO: Epoch[3] - Fold[1] - Iteration[50/52] Loss: 0.057\n2024-06-16 10:38:10,736 classification_efficientnet-b4 INFO: Epoch[3] - Fold[1] - Iteration[50/52] Loss: 0.057\n2024-06-16 10:38:11,220 classification_efficientnet-b4 INFO: Training Result: Epoch 3/10 Fold 1/3, Loss: 0.039, Time epoch: 28.161s\n2024-06-16 10:38:11,220 classification_efficientnet-b4 INFO: Training Result: Epoch 3/10 Fold 1/3, Loss: 0.039, Time epoch: 28.161s\n2024-06-16 10:38:17,482 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.061, Accuracy: 0.731 F1-Score: 0.470, Precision: 0.535, Recall: 0.433\n2024-06-16 10:38:17,482 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.061, Accuracy: 0.731 F1-Score: 0.470, Precision: 0.535, Recall: 0.433\n2024-06-16 10:38:17,495 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:38:17,495 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:38:18,049 classification_efficientnet-b4 INFO: Epoch[4] - Fold[1] - Iteration[0/52] Loss: 0.011\n2024-06-16 10:38:18,049 classification_efficientnet-b4 INFO: Epoch[4] - Fold[1] - Iteration[0/52] Loss: 0.011\n2024-06-16 10:38:23,438 classification_efficientnet-b4 INFO: Epoch[4] - Fold[1] - Iteration[10/52] Loss: 0.021\n2024-06-16 10:38:23,438 classification_efficientnet-b4 INFO: Epoch[4] - Fold[1] - Iteration[10/52] Loss: 0.021\n2024-06-16 10:38:28,836 classification_efficientnet-b4 INFO: Epoch[4] - Fold[1] - Iteration[20/52] Loss: 0.062\n2024-06-16 10:38:28,836 classification_efficientnet-b4 INFO: Epoch[4] - Fold[1] - Iteration[20/52] Loss: 0.062\n2024-06-16 10:38:34,234 classification_efficientnet-b4 INFO: Epoch[4] - Fold[1] - Iteration[30/52] Loss: 0.037\n2024-06-16 10:38:34,234 classification_efficientnet-b4 INFO: Epoch[4] - Fold[1] - Iteration[30/52] Loss: 0.037\n2024-06-16 10:38:39,654 classification_efficientnet-b4 INFO: Epoch[4] - Fold[1] - Iteration[40/52] Loss: 0.041\n2024-06-16 10:38:39,654 classification_efficientnet-b4 INFO: Epoch[4] - Fold[1] - Iteration[40/52] Loss: 0.041\n2024-06-16 10:38:44,997 classification_efficientnet-b4 INFO: Epoch[4] - Fold[1] - Iteration[50/52] Loss: 0.086\n2024-06-16 10:38:44,997 classification_efficientnet-b4 INFO: Epoch[4] - Fold[1] - Iteration[50/52] Loss: 0.086\n2024-06-16 10:38:45,469 classification_efficientnet-b4 INFO: Training Result: Epoch 4/10 Fold 1/3, Loss: 0.042, Time epoch: 27.978s\n2024-06-16 10:38:45,469 classification_efficientnet-b4 INFO: Training Result: Epoch 4/10 Fold 1/3, Loss: 0.042, Time epoch: 27.978s\n2024-06-16 10:38:51,816 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.055, Accuracy: 0.567 F1-Score: 0.557, Precision: 0.582, Recall: 0.555\n2024-06-16 10:38:51,816 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.055, Accuracy: 0.567 F1-Score: 0.557, Precision: 0.582, Recall: 0.555\n2024-06-16 10:38:51,825 classification_efficientnet-b4 INFO: Best model found at epoch 4, saving model\n2024-06-16 10:38:51,825 classification_efficientnet-b4 INFO: Best model found at epoch 4, saving model\n2024-06-16 10:38:52,245 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:38:52,245 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:38:52,796 classification_efficientnet-b4 INFO: Epoch[5] - Fold[1] - Iteration[0/52] Loss: 0.022\n2024-06-16 10:38:52,796 classification_efficientnet-b4 INFO: Epoch[5] - Fold[1] - Iteration[0/52] Loss: 0.022\n2024-06-16 10:38:58,293 classification_efficientnet-b4 INFO: Epoch[5] - Fold[1] - Iteration[10/52] Loss: 0.010\n2024-06-16 10:38:58,293 classification_efficientnet-b4 INFO: Epoch[5] - Fold[1] - Iteration[10/52] Loss: 0.010\n2024-06-16 10:39:03,731 classification_efficientnet-b4 INFO: Epoch[5] - Fold[1] - Iteration[20/52] Loss: 0.020\n2024-06-16 10:39:03,731 classification_efficientnet-b4 INFO: Epoch[5] - Fold[1] - Iteration[20/52] Loss: 0.020\n2024-06-16 10:39:09,059 classification_efficientnet-b4 INFO: Epoch[5] - Fold[1] - Iteration[30/52] Loss: 0.014\n2024-06-16 10:39:09,059 classification_efficientnet-b4 INFO: Epoch[5] - Fold[1] - Iteration[30/52] Loss: 0.014\n2024-06-16 10:39:14,430 classification_efficientnet-b4 INFO: Epoch[5] - Fold[1] - Iteration[40/52] Loss: 0.057\n2024-06-16 10:39:14,430 classification_efficientnet-b4 INFO: Epoch[5] - Fold[1] - Iteration[40/52] Loss: 0.057\n2024-06-16 10:39:19,827 classification_efficientnet-b4 INFO: Epoch[5] - Fold[1] - Iteration[50/52] Loss: 0.034\n2024-06-16 10:39:19,827 classification_efficientnet-b4 INFO: Epoch[5] - Fold[1] - Iteration[50/52] Loss: 0.034\n2024-06-16 10:39:20,329 classification_efficientnet-b4 INFO: Training Result: Epoch 5/10 Fold 1/3, Loss: 0.035, Time epoch: 28.088s\n2024-06-16 10:39:20,329 classification_efficientnet-b4 INFO: Training Result: Epoch 5/10 Fold 1/3, Loss: 0.035, Time epoch: 28.088s\n2024-06-16 10:39:26,504 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.050, Accuracy: 0.596 F1-Score: 0.579, Precision: 0.606, Recall: 0.577\n2024-06-16 10:39:26,504 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.050, Accuracy: 0.596 F1-Score: 0.579, Precision: 0.606, Recall: 0.577\n2024-06-16 10:39:26,513 classification_efficientnet-b4 INFO: Best model found at epoch 5, saving model\n2024-06-16 10:39:26,513 classification_efficientnet-b4 INFO: Best model found at epoch 5, saving model\n2024-06-16 10:39:26,931 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:39:26,931 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:39:27,489 classification_efficientnet-b4 INFO: Epoch[6] - Fold[1] - Iteration[0/52] Loss: 0.060\n2024-06-16 10:39:27,489 classification_efficientnet-b4 INFO: Epoch[6] - Fold[1] - Iteration[0/52] Loss: 0.060\n2024-06-16 10:39:32,866 classification_efficientnet-b4 INFO: Epoch[6] - Fold[1] - Iteration[10/52] Loss: 0.040\n2024-06-16 10:39:32,866 classification_efficientnet-b4 INFO: Epoch[6] - Fold[1] - Iteration[10/52] Loss: 0.040\n2024-06-16 10:39:38,302 classification_efficientnet-b4 INFO: Epoch[6] - Fold[1] - Iteration[20/52] Loss: 0.042\n2024-06-16 10:39:38,302 classification_efficientnet-b4 INFO: Epoch[6] - Fold[1] - Iteration[20/52] Loss: 0.042\n2024-06-16 10:39:43,711 classification_efficientnet-b4 INFO: Epoch[6] - Fold[1] - Iteration[30/52] Loss: 0.049\n2024-06-16 10:39:43,711 classification_efficientnet-b4 INFO: Epoch[6] - Fold[1] - Iteration[30/52] Loss: 0.049\n2024-06-16 10:39:49,066 classification_efficientnet-b4 INFO: Epoch[6] - Fold[1] - Iteration[40/52] Loss: 0.027\n2024-06-16 10:39:49,066 classification_efficientnet-b4 INFO: Epoch[6] - Fold[1] - Iteration[40/52] Loss: 0.027\n2024-06-16 10:39:54,496 classification_efficientnet-b4 INFO: Epoch[6] - Fold[1] - Iteration[50/52] Loss: 0.022\n2024-06-16 10:39:54,496 classification_efficientnet-b4 INFO: Epoch[6] - Fold[1] - Iteration[50/52] Loss: 0.022\n2024-06-16 10:39:54,980 classification_efficientnet-b4 INFO: Training Result: Epoch 6/10 Fold 1/3, Loss: 0.034, Time epoch: 28.053s\n2024-06-16 10:39:54,980 classification_efficientnet-b4 INFO: Training Result: Epoch 6/10 Fold 1/3, Loss: 0.034, Time epoch: 28.053s\n2024-06-16 10:40:01,194 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.047, Accuracy: 0.635 F1-Score: 0.608, Precision: 0.671, Recall: 0.598\n2024-06-16 10:40:01,194 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.047, Accuracy: 0.635 F1-Score: 0.608, Precision: 0.671, Recall: 0.598\n2024-06-16 10:40:01,204 classification_efficientnet-b4 INFO: Best model found at epoch 6, saving model\n2024-06-16 10:40:01,204 classification_efficientnet-b4 INFO: Best model found at epoch 6, saving model\n2024-06-16 10:40:01,640 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:40:01,640 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:40:02,174 classification_efficientnet-b4 INFO: Epoch[7] - Fold[1] - Iteration[0/52] Loss: 0.011\n2024-06-16 10:40:02,174 classification_efficientnet-b4 INFO: Epoch[7] - Fold[1] - Iteration[0/52] Loss: 0.011\n2024-06-16 10:40:07,566 classification_efficientnet-b4 INFO: Epoch[7] - Fold[1] - Iteration[10/52] Loss: 0.022\n2024-06-16 10:40:07,566 classification_efficientnet-b4 INFO: Epoch[7] - Fold[1] - Iteration[10/52] Loss: 0.022\n2024-06-16 10:40:12,930 classification_efficientnet-b4 INFO: Epoch[7] - Fold[1] - Iteration[20/52] Loss: 0.008\n2024-06-16 10:40:12,930 classification_efficientnet-b4 INFO: Epoch[7] - Fold[1] - Iteration[20/52] Loss: 0.008\n2024-06-16 10:40:18,353 classification_efficientnet-b4 INFO: Epoch[7] - Fold[1] - Iteration[30/52] Loss: 0.015\n2024-06-16 10:40:18,353 classification_efficientnet-b4 INFO: Epoch[7] - Fold[1] - Iteration[30/52] Loss: 0.015\n2024-06-16 10:40:23,697 classification_efficientnet-b4 INFO: Epoch[7] - Fold[1] - Iteration[40/52] Loss: 0.019\n2024-06-16 10:40:23,697 classification_efficientnet-b4 INFO: Epoch[7] - Fold[1] - Iteration[40/52] Loss: 0.019\n2024-06-16 10:40:28,985 classification_efficientnet-b4 INFO: Epoch[7] - Fold[1] - Iteration[50/52] Loss: 0.010\n2024-06-16 10:40:28,985 classification_efficientnet-b4 INFO: Epoch[7] - Fold[1] - Iteration[50/52] Loss: 0.010\n2024-06-16 10:40:29,454 classification_efficientnet-b4 INFO: Training Result: Epoch 7/10 Fold 1/3, Loss: 0.026, Time epoch: 27.818s\n2024-06-16 10:40:29,454 classification_efficientnet-b4 INFO: Training Result: Epoch 7/10 Fold 1/3, Loss: 0.026, Time epoch: 27.818s\n2024-06-16 10:40:35,651 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.049, Accuracy: 0.654 F1-Score: 0.336, Precision: 0.424, Recall: 0.286\n2024-06-16 10:40:35,651 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.049, Accuracy: 0.654 F1-Score: 0.336, Precision: 0.424, Recall: 0.286\n2024-06-16 10:40:35,664 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:40:35,664 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:40:36,230 classification_efficientnet-b4 INFO: Epoch[8] - Fold[1] - Iteration[0/52] Loss: 0.051\n2024-06-16 10:40:36,230 classification_efficientnet-b4 INFO: Epoch[8] - Fold[1] - Iteration[0/52] Loss: 0.051\n2024-06-16 10:40:41,647 classification_efficientnet-b4 INFO: Epoch[8] - Fold[1] - Iteration[10/52] Loss: 0.014\n2024-06-16 10:40:41,647 classification_efficientnet-b4 INFO: Epoch[8] - Fold[1] - Iteration[10/52] Loss: 0.014\n2024-06-16 10:40:46,985 classification_efficientnet-b4 INFO: Epoch[8] - Fold[1] - Iteration[20/52] Loss: 0.005\n2024-06-16 10:40:46,985 classification_efficientnet-b4 INFO: Epoch[8] - Fold[1] - Iteration[20/52] Loss: 0.005\n2024-06-16 10:40:52,402 classification_efficientnet-b4 INFO: Epoch[8] - Fold[1] - Iteration[30/52] Loss: 0.007\n2024-06-16 10:40:52,402 classification_efficientnet-b4 INFO: Epoch[8] - Fold[1] - Iteration[30/52] Loss: 0.007\n2024-06-16 10:40:57,785 classification_efficientnet-b4 INFO: Epoch[8] - Fold[1] - Iteration[40/52] Loss: 0.046\n2024-06-16 10:40:57,785 classification_efficientnet-b4 INFO: Epoch[8] - Fold[1] - Iteration[40/52] Loss: 0.046\n2024-06-16 10:41:03,173 classification_efficientnet-b4 INFO: Epoch[8] - Fold[1] - Iteration[50/52] Loss: 0.051\n2024-06-16 10:41:03,173 classification_efficientnet-b4 INFO: Epoch[8] - Fold[1] - Iteration[50/52] Loss: 0.051\n2024-06-16 10:41:03,652 classification_efficientnet-b4 INFO: Training Result: Epoch 8/10 Fold 1/3, Loss: 0.029, Time epoch: 27.992s\n2024-06-16 10:41:03,652 classification_efficientnet-b4 INFO: Training Result: Epoch 8/10 Fold 1/3, Loss: 0.029, Time epoch: 27.992s\n2024-06-16 10:41:09,915 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.047, Accuracy: 0.596 F1-Score: 0.516, Precision: 0.561, Recall: 0.502\n2024-06-16 10:41:09,915 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.047, Accuracy: 0.596 F1-Score: 0.516, Precision: 0.561, Recall: 0.502\n2024-06-16 10:41:09,933 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:41:09,933 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:41:10,466 classification_efficientnet-b4 INFO: Epoch[9] - Fold[1] - Iteration[0/52] Loss: 0.076\n2024-06-16 10:41:10,466 classification_efficientnet-b4 INFO: Epoch[9] - Fold[1] - Iteration[0/52] Loss: 0.076\n2024-06-16 10:41:15,865 classification_efficientnet-b4 INFO: Epoch[9] - Fold[1] - Iteration[10/52] Loss: 0.043\n2024-06-16 10:41:15,865 classification_efficientnet-b4 INFO: Epoch[9] - Fold[1] - Iteration[10/52] Loss: 0.043\n2024-06-16 10:41:21,156 classification_efficientnet-b4 INFO: Epoch[9] - Fold[1] - Iteration[20/52] Loss: 0.033\n2024-06-16 10:41:21,156 classification_efficientnet-b4 INFO: Epoch[9] - Fold[1] - Iteration[20/52] Loss: 0.033\n2024-06-16 10:41:26,513 classification_efficientnet-b4 INFO: Epoch[9] - Fold[1] - Iteration[30/52] Loss: 0.021\n2024-06-16 10:41:26,513 classification_efficientnet-b4 INFO: Epoch[9] - Fold[1] - Iteration[30/52] Loss: 0.021\n2024-06-16 10:41:31,809 classification_efficientnet-b4 INFO: Epoch[9] - Fold[1] - Iteration[40/52] Loss: 0.030\n2024-06-16 10:41:31,809 classification_efficientnet-b4 INFO: Epoch[9] - Fold[1] - Iteration[40/52] Loss: 0.030\n2024-06-16 10:41:37,181 classification_efficientnet-b4 INFO: Epoch[9] - Fold[1] - Iteration[50/52] Loss: 0.053\n2024-06-16 10:41:37,181 classification_efficientnet-b4 INFO: Epoch[9] - Fold[1] - Iteration[50/52] Loss: 0.053\n2024-06-16 10:41:37,670 classification_efficientnet-b4 INFO: Training Result: Epoch 9/10 Fold 1/3, Loss: 0.038, Time epoch: 27.741s\n2024-06-16 10:41:37,670 classification_efficientnet-b4 INFO: Training Result: Epoch 9/10 Fold 1/3, Loss: 0.038, Time epoch: 27.741s\n2024-06-16 10:41:44,012 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.047, Accuracy: 0.659 F1-Score: 0.417, Precision: 0.510, Recall: 0.385\n2024-06-16 10:41:44,012 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.047, Accuracy: 0.659 F1-Score: 0.417, Precision: 0.510, Recall: 0.385\n2024-06-16 10:41:44,025 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:41:44,025 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:41:44,576 classification_efficientnet-b4 INFO: Epoch[10] - Fold[1] - Iteration[0/52] Loss: 0.016\n2024-06-16 10:41:44,576 classification_efficientnet-b4 INFO: Epoch[10] - Fold[1] - Iteration[0/52] Loss: 0.016\n2024-06-16 10:41:50,054 classification_efficientnet-b4 INFO: Epoch[10] - Fold[1] - Iteration[10/52] Loss: 0.085\n2024-06-16 10:41:50,054 classification_efficientnet-b4 INFO: Epoch[10] - Fold[1] - Iteration[10/52] Loss: 0.085\n2024-06-16 10:41:55,470 classification_efficientnet-b4 INFO: Epoch[10] - Fold[1] - Iteration[20/52] Loss: 0.106\n2024-06-16 10:41:55,470 classification_efficientnet-b4 INFO: Epoch[10] - Fold[1] - Iteration[20/52] Loss: 0.106\n2024-06-16 10:42:00,915 classification_efficientnet-b4 INFO: Epoch[10] - Fold[1] - Iteration[30/52] Loss: 0.074\n2024-06-16 10:42:00,915 classification_efficientnet-b4 INFO: Epoch[10] - Fold[1] - Iteration[30/52] Loss: 0.074\n2024-06-16 10:42:06,291 classification_efficientnet-b4 INFO: Epoch[10] - Fold[1] - Iteration[40/52] Loss: 0.064\n2024-06-16 10:42:06,291 classification_efficientnet-b4 INFO: Epoch[10] - Fold[1] - Iteration[40/52] Loss: 0.064\n2024-06-16 10:42:11,690 classification_efficientnet-b4 INFO: Epoch[10] - Fold[1] - Iteration[50/52] Loss: 0.054\n2024-06-16 10:42:11,690 classification_efficientnet-b4 INFO: Epoch[10] - Fold[1] - Iteration[50/52] Loss: 0.054\n2024-06-16 10:42:12,160 classification_efficientnet-b4 INFO: Training Result: Epoch 10/10 Fold 1/3, Loss: 0.056, Time epoch: 28.138s\n2024-06-16 10:42:12,160 classification_efficientnet-b4 INFO: Training Result: Epoch 10/10 Fold 1/3, Loss: 0.056, Time epoch: 28.138s\n2024-06-16 10:42:18,363 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.046, Accuracy: 0.558 F1-Score: 0.551, Precision: 0.548, Recall: 0.558\n2024-06-16 10:42:18,363 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.046, Accuracy: 0.558 F1-Score: 0.551, Precision: 0.548, Recall: 0.558\n2024-06-16 10:42:18,371 classification_efficientnet-b4 INFO: Save model at epoch 10, saving model\n2024-06-16 10:42:18,371 classification_efficientnet-b4 INFO: Save model at epoch 10, saving model\n2024-06-16 10:42:21,778 classification_efficientnet-b4 INFO: ==========Hyperparameters==========\n2024-06-16 10:42:21,778 classification_efficientnet-b4 INFO: ==========Hyperparameters==========\n2024-06-16 10:42:21,778 classification_efficientnet-b4 INFO: ==========Hyperparameters==========\n2024-06-16 10:42:21,781 classification_efficientnet-b4 INFO: Device: cuda\n2024-06-16 10:42:21,781 classification_efficientnet-b4 INFO: Device: cuda\n2024-06-16 10:42:21,781 classification_efficientnet-b4 INFO: Device: cuda\n2024-06-16 10:42:21,784 classification_efficientnet-b4 INFO: Architecture: deeplabv3plus\n2024-06-16 10:42:21,784 classification_efficientnet-b4 INFO: Architecture: deeplabv3plus\n2024-06-16 10:42:21,784 classification_efficientnet-b4 INFO: Architecture: deeplabv3plus\n2024-06-16 10:42:21,787 classification_efficientnet-b4 INFO: Encoder: efficientnet-b4\n2024-06-16 10:42:21,787 classification_efficientnet-b4 INFO: Encoder: efficientnet-b4\n2024-06-16 10:42:21,787 classification_efficientnet-b4 INFO: Encoder: efficientnet-b4\n2024-06-16 10:42:21,789 classification_efficientnet-b4 INFO: Encoder weight: imagenet\n2024-06-16 10:42:21,789 classification_efficientnet-b4 INFO: Encoder weight: imagenet\n2024-06-16 10:42:21,789 classification_efficientnet-b4 INFO: Encoder weight: imagenet\n2024-06-16 10:42:21,792 classification_efficientnet-b4 INFO: Input size: (448, 448)\n2024-06-16 10:42:21,792 classification_efficientnet-b4 INFO: Input size: (448, 448)\n2024-06-16 10:42:21,792 classification_efficientnet-b4 INFO: Input size: (448, 448)\n2024-06-16 10:42:21,794 classification_efficientnet-b4 INFO: Batch size: 8\n2024-06-16 10:42:21,794 classification_efficientnet-b4 INFO: Batch size: 8\n2024-06-16 10:42:21,794 classification_efficientnet-b4 INFO: Batch size: 8\n2024-06-16 10:42:21,797 classification_efficientnet-b4 INFO: Base learning rate: 0.01\n2024-06-16 10:42:21,797 classification_efficientnet-b4 INFO: Base learning rate: 0.01\n2024-06-16 10:42:21,797 classification_efficientnet-b4 INFO: Base learning rate: 0.01\n2024-06-16 10:42:21,799 classification_efficientnet-b4 INFO: Max epochs: 10\n2024-06-16 10:42:21,799 classification_efficientnet-b4 INFO: Max epochs: 10\n2024-06-16 10:42:21,799 classification_efficientnet-b4 INFO: Max epochs: 10\n2024-06-16 10:42:21,801 classification_efficientnet-b4 INFO: Weight decay: 1e-05\n2024-06-16 10:42:21,801 classification_efficientnet-b4 INFO: Weight decay: 1e-05\n2024-06-16 10:42:21,801 classification_efficientnet-b4 INFO: Weight decay: 1e-05\n2024-06-16 10:42:21,803 classification_efficientnet-b4 INFO: ===================================\n2024-06-16 10:42:21,803 classification_efficientnet-b4 INFO: ===================================\n2024-06-16 10:42:21,803 classification_efficientnet-b4 INFO: ===================================\n2024-06-16 10:42:21,810 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:42:21,810 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:42:21,810 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:42:22,393 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[0/52] Loss: 0.094\n2024-06-16 10:42:22,393 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[0/52] Loss: 0.094\n2024-06-16 10:42:22,393 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[0/52] Loss: 0.094\n2024-06-16 10:42:27,755 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[10/52] Loss: 0.126\n2024-06-16 10:42:27,755 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[10/52] Loss: 0.126\n2024-06-16 10:42:27,755 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[10/52] Loss: 0.126\n2024-06-16 10:42:33,142 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[20/52] Loss: 0.069\n2024-06-16 10:42:33,142 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[20/52] Loss: 0.069\n2024-06-16 10:42:33,142 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[20/52] Loss: 0.069\n2024-06-16 10:42:38,480 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[30/52] Loss: 0.109\n2024-06-16 10:42:38,480 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[30/52] Loss: 0.109\n2024-06-16 10:42:38,480 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[30/52] Loss: 0.109\n2024-06-16 10:42:43,919 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[40/52] Loss: 0.079\n2024-06-16 10:42:43,919 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[40/52] Loss: 0.079\n2024-06-16 10:42:43,919 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[40/52] Loss: 0.079\n2024-06-16 10:42:49,307 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[50/52] Loss: 0.039\n2024-06-16 10:42:49,307 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[50/52] Loss: 0.039\n2024-06-16 10:42:49,307 classification_efficientnet-b4 INFO: Epoch[1] - Fold[2] - Iteration[50/52] Loss: 0.039\n2024-06-16 10:42:49,849 classification_efficientnet-b4 INFO: Training Result: Epoch 1/10 Fold 2/3, Loss: 0.110, Time epoch: 28.042s\n2024-06-16 10:42:49,849 classification_efficientnet-b4 INFO: Training Result: Epoch 1/10 Fold 2/3, Loss: 0.110, Time epoch: 28.042s\n2024-06-16 10:42:49,849 classification_efficientnet-b4 INFO: Training Result: Epoch 1/10 Fold 2/3, Loss: 0.110, Time epoch: 28.042s\n2024-06-16 10:42:56,054 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.210, Accuracy: 0.560 F1-Score: 0.554, Precision: 0.551, Recall: 0.560\n2024-06-16 10:42:56,054 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.210, Accuracy: 0.560 F1-Score: 0.554, Precision: 0.551, Recall: 0.560\n2024-06-16 10:42:56,054 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.210, Accuracy: 0.560 F1-Score: 0.554, Precision: 0.551, Recall: 0.560\n2024-06-16 10:42:56,069 classification_efficientnet-b4 INFO: Best model found at epoch 1, saving model\n2024-06-16 10:42:56,069 classification_efficientnet-b4 INFO: Best model found at epoch 1, saving model\n2024-06-16 10:42:56,069 classification_efficientnet-b4 INFO: Best model found at epoch 1, saving model\n2024-06-16 10:42:56,497 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:42:56,497 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:42:56,497 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:42:57,045 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[0/52] Loss: 0.044\n2024-06-16 10:42:57,045 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[0/52] Loss: 0.044\n2024-06-16 10:42:57,045 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[0/52] Loss: 0.044\n2024-06-16 10:43:02,430 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[10/52] Loss: 0.031\n2024-06-16 10:43:02,430 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[10/52] Loss: 0.031\n2024-06-16 10:43:02,430 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[10/52] Loss: 0.031\n2024-06-16 10:43:07,807 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[20/52] Loss: 0.058\n2024-06-16 10:43:07,807 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[20/52] Loss: 0.058\n2024-06-16 10:43:07,807 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[20/52] Loss: 0.058\n2024-06-16 10:43:13,212 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[30/52] Loss: 0.051\n2024-06-16 10:43:13,212 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[30/52] Loss: 0.051\n2024-06-16 10:43:13,212 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[30/52] Loss: 0.051\n2024-06-16 10:43:18,571 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[40/52] Loss: 0.022\n2024-06-16 10:43:18,571 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[40/52] Loss: 0.022\n2024-06-16 10:43:18,571 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[40/52] Loss: 0.022\n2024-06-16 10:43:23,955 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[50/52] Loss: 0.046\n2024-06-16 10:43:23,955 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[50/52] Loss: 0.046\n2024-06-16 10:43:23,955 classification_efficientnet-b4 INFO: Epoch[2] - Fold[2] - Iteration[50/52] Loss: 0.046\n2024-06-16 10:43:24,495 classification_efficientnet-b4 INFO: Training Result: Epoch 2/10 Fold 2/3, Loss: 0.049, Time epoch: 28.002s\n2024-06-16 10:43:24,495 classification_efficientnet-b4 INFO: Training Result: Epoch 2/10 Fold 2/3, Loss: 0.049, Time epoch: 28.002s\n2024-06-16 10:43:24,495 classification_efficientnet-b4 INFO: Training Result: Epoch 2/10 Fold 2/3, Loss: 0.049, Time epoch: 28.002s\n2024-06-16 10:43:30,677 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.128, Accuracy: 0.560 F1-Score: 0.554, Precision: 0.551, Recall: 0.560\n2024-06-16 10:43:30,677 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.128, Accuracy: 0.560 F1-Score: 0.554, Precision: 0.551, Recall: 0.560\n2024-06-16 10:43:30,677 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.128, Accuracy: 0.560 F1-Score: 0.554, Precision: 0.551, Recall: 0.560\n2024-06-16 10:43:30,691 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:43:30,691 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:43:30,691 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:43:31,228 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[0/52] Loss: 0.041\n2024-06-16 10:43:31,228 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[0/52] Loss: 0.041\n2024-06-16 10:43:31,228 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[0/52] Loss: 0.041\n2024-06-16 10:43:36,602 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[10/52] Loss: 0.051\n2024-06-16 10:43:36,602 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[10/52] Loss: 0.051\n2024-06-16 10:43:36,602 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[10/52] Loss: 0.051\n2024-06-16 10:43:41,994 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[20/52] Loss: 0.009\n2024-06-16 10:43:41,994 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[20/52] Loss: 0.009\n2024-06-16 10:43:41,994 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[20/52] Loss: 0.009\n2024-06-16 10:43:47,487 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[30/52] Loss: 0.103\n2024-06-16 10:43:47,487 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[30/52] Loss: 0.103\n2024-06-16 10:43:47,487 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[30/52] Loss: 0.103\n2024-06-16 10:43:52,893 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[40/52] Loss: 0.032\n2024-06-16 10:43:52,893 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[40/52] Loss: 0.032\n2024-06-16 10:43:52,893 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[40/52] Loss: 0.032\n2024-06-16 10:43:58,278 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[50/52] Loss: 0.045\n2024-06-16 10:43:58,278 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[50/52] Loss: 0.045\n2024-06-16 10:43:58,278 classification_efficientnet-b4 INFO: Epoch[3] - Fold[2] - Iteration[50/52] Loss: 0.045\n2024-06-16 10:43:58,802 classification_efficientnet-b4 INFO: Training Result: Epoch 3/10 Fold 2/3, Loss: 0.046, Time epoch: 28.115s\n2024-06-16 10:43:58,802 classification_efficientnet-b4 INFO: Training Result: Epoch 3/10 Fold 2/3, Loss: 0.046, Time epoch: 28.115s\n2024-06-16 10:43:58,802 classification_efficientnet-b4 INFO: Training Result: Epoch 3/10 Fold 2/3, Loss: 0.046, Time epoch: 28.115s\n2024-06-16 10:44:05,064 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.114, Accuracy: 0.565 F1-Score: 0.542, Precision: 0.570, Recall: 0.543\n2024-06-16 10:44:05,064 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.114, Accuracy: 0.565 F1-Score: 0.542, Precision: 0.570, Recall: 0.543\n2024-06-16 10:44:05,064 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.114, Accuracy: 0.565 F1-Score: 0.542, Precision: 0.570, Recall: 0.543\n2024-06-16 10:44:05,080 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:44:05,080 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:44:05,080 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:44:05,606 classification_efficientnet-b4 INFO: Epoch[4] - Fold[2] - Iteration[0/52] Loss: 0.041\n2024-06-16 10:44:05,606 classification_efficientnet-b4 INFO: Epoch[4] - Fold[2] - Iteration[0/52] Loss: 0.041\n2024-06-16 10:44:05,606 classification_efficientnet-b4 INFO: Epoch[4] - Fold[2] - Iteration[0/52] Loss: 0.041\n2024-06-16 10:44:10,998 classification_efficientnet-b4 INFO: Epoch[4] - Fold[2] - Iteration[10/52] Loss: 0.082\n2024-06-16 10:44:10,998 classification_efficientnet-b4 INFO: Epoch[4] - Fold[2] - Iteration[10/52] Loss: 0.082\n2024-06-16 10:44:10,998 classification_efficientnet-b4 INFO: Epoch[4] - Fold[2] - Iteration[10/52] Loss: 0.082\n2024-06-16 10:44:16,364 classification_efficientnet-b4 INFO: Epoch[4] - Fold[2] - Iteration[20/52] Loss: 0.035\n2024-06-16 10:44:16,364 classification_efficientnet-b4 INFO: Epoch[4] - Fold[2] - Iteration[20/52] Loss: 0.035\n2024-06-16 10:44:16,364 classification_efficientnet-b4 INFO: Epoch[4] - Fold[2] - Iteration[20/52] Loss: 0.035\n2024-06-16 10:44:21,753 classification_efficientnet-b4 INFO: Epoch[4] - Fold[2] - Iteration[30/52] Loss: 0.044\n2024-06-16 10:44:21,753 classification_efficientnet-b4 INFO: Epoch[4] - Fold[2] - Iteration[30/52] Loss: 0.044\n2024-06-16 10:44:21,753 classification_efficientnet-b4 INFO: Epoch[4] - Fold[2] - Iteration[30/52] Loss: 0.044\n2024-06-16 10:44:27,172 classification_efficientnet-b4 INFO: Epoch[4] - Fold[2] - Iteration[40/52] Loss: 0.043\n2024-06-16 10:44:27,172 classification_efficientnet-b4 INFO: Epoch[4] - Fold[2] - Iteration[40/52] Loss: 0.043\n2024-06-16 10:44:27,172 classification_efficientnet-b4 INFO: Epoch[4] - Fold[2] - Iteration[40/52] Loss: 0.043\n2024-06-16 10:44:32,580 classification_efficientnet-b4 INFO: Epoch[4] - Fold[2] - Iteration[50/52] Loss: 0.095\n2024-06-16 10:44:32,580 classification_efficientnet-b4 INFO: Epoch[4] - Fold[2] - Iteration[50/52] Loss: 0.095\n2024-06-16 10:44:32,580 classification_efficientnet-b4 INFO: Epoch[4] - Fold[2] - Iteration[50/52] Loss: 0.095\n2024-06-16 10:44:33,122 classification_efficientnet-b4 INFO: Training Result: Epoch 4/10 Fold 2/3, Loss: 0.055, Time epoch: 28.046s\n2024-06-16 10:44:33,122 classification_efficientnet-b4 INFO: Training Result: Epoch 4/10 Fold 2/3, Loss: 0.055, Time epoch: 28.046s\n2024-06-16 10:44:33,122 classification_efficientnet-b4 INFO: Training Result: Epoch 4/10 Fold 2/3, Loss: 0.055, Time epoch: 28.046s\n2024-06-16 10:44:39,508 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.097, Accuracy: 0.570 F1-Score: 0.563, Precision: 0.589, Recall: 0.565\n2024-06-16 10:44:39,508 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.097, Accuracy: 0.570 F1-Score: 0.563, Precision: 0.589, Recall: 0.565\n2024-06-16 10:44:39,508 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.097, Accuracy: 0.570 F1-Score: 0.563, Precision: 0.589, Recall: 0.565\n2024-06-16 10:44:39,519 classification_efficientnet-b4 INFO: Best model found at epoch 4, saving model\n2024-06-16 10:44:39,519 classification_efficientnet-b4 INFO: Best model found at epoch 4, saving model\n2024-06-16 10:44:39,519 classification_efficientnet-b4 INFO: Best model found at epoch 4, saving model\n2024-06-16 10:44:39,988 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:44:39,988 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:44:39,988 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:44:40,519 classification_efficientnet-b4 INFO: Epoch[5] - Fold[2] - Iteration[0/52] Loss: 0.027\n2024-06-16 10:44:40,519 classification_efficientnet-b4 INFO: Epoch[5] - Fold[2] - Iteration[0/52] Loss: 0.027\n2024-06-16 10:44:40,519 classification_efficientnet-b4 INFO: Epoch[5] - Fold[2] - Iteration[0/52] Loss: 0.027\n2024-06-16 10:44:45,916 classification_efficientnet-b4 INFO: Epoch[5] - Fold[2] - Iteration[10/52] Loss: 0.051\n2024-06-16 10:44:45,916 classification_efficientnet-b4 INFO: Epoch[5] - Fold[2] - Iteration[10/52] Loss: 0.051\n2024-06-16 10:44:45,916 classification_efficientnet-b4 INFO: Epoch[5] - Fold[2] - Iteration[10/52] Loss: 0.051\n2024-06-16 10:44:51,319 classification_efficientnet-b4 INFO: Epoch[5] - Fold[2] - Iteration[20/52] Loss: 0.033\n2024-06-16 10:44:51,319 classification_efficientnet-b4 INFO: Epoch[5] - Fold[2] - Iteration[20/52] Loss: 0.033\n2024-06-16 10:44:51,319 classification_efficientnet-b4 INFO: Epoch[5] - Fold[2] - Iteration[20/52] Loss: 0.033\n2024-06-16 10:44:56,776 classification_efficientnet-b4 INFO: Epoch[5] - Fold[2] - Iteration[30/52] Loss: 0.076\n2024-06-16 10:44:56,776 classification_efficientnet-b4 INFO: Epoch[5] - Fold[2] - Iteration[30/52] Loss: 0.076\n2024-06-16 10:44:56,776 classification_efficientnet-b4 INFO: Epoch[5] - Fold[2] - Iteration[30/52] Loss: 0.076\n2024-06-16 10:45:02,225 classification_efficientnet-b4 INFO: Epoch[5] - Fold[2] - Iteration[40/52] Loss: 0.060\n2024-06-16 10:45:02,225 classification_efficientnet-b4 INFO: Epoch[5] - Fold[2] - Iteration[40/52] Loss: 0.060\n2024-06-16 10:45:02,225 classification_efficientnet-b4 INFO: Epoch[5] - Fold[2] - Iteration[40/52] Loss: 0.060\n2024-06-16 10:45:07,647 classification_efficientnet-b4 INFO: Epoch[5] - Fold[2] - Iteration[50/52] Loss: 0.041\n2024-06-16 10:45:07,647 classification_efficientnet-b4 INFO: Epoch[5] - Fold[2] - Iteration[50/52] Loss: 0.041\n2024-06-16 10:45:07,647 classification_efficientnet-b4 INFO: Epoch[5] - Fold[2] - Iteration[50/52] Loss: 0.041\n2024-06-16 10:45:08,208 classification_efficientnet-b4 INFO: Training Result: Epoch 5/10 Fold 2/3, Loss: 0.053, Time epoch: 28.224s\n2024-06-16 10:45:08,208 classification_efficientnet-b4 INFO: Training Result: Epoch 5/10 Fold 2/3, Loss: 0.053, Time epoch: 28.224s\n2024-06-16 10:45:08,208 classification_efficientnet-b4 INFO: Training Result: Epoch 5/10 Fold 2/3, Loss: 0.053, Time epoch: 28.224s\n2024-06-16 10:45:14,504 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.098, Accuracy: 0.560 F1-Score: 0.554, Precision: 0.551, Recall: 0.560\n2024-06-16 10:45:14,504 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.098, Accuracy: 0.560 F1-Score: 0.554, Precision: 0.551, Recall: 0.560\n2024-06-16 10:45:14,504 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.098, Accuracy: 0.560 F1-Score: 0.554, Precision: 0.551, Recall: 0.560\n2024-06-16 10:45:14,519 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:45:14,519 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:45:14,519 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:45:15,051 classification_efficientnet-b4 INFO: Epoch[6] - Fold[2] - Iteration[0/52] Loss: 0.154\n2024-06-16 10:45:15,051 classification_efficientnet-b4 INFO: Epoch[6] - Fold[2] - Iteration[0/52] Loss: 0.154\n2024-06-16 10:45:15,051 classification_efficientnet-b4 INFO: Epoch[6] - Fold[2] - Iteration[0/52] Loss: 0.154\n2024-06-16 10:45:20,517 classification_efficientnet-b4 INFO: Epoch[6] - Fold[2] - Iteration[10/52] Loss: 0.052\n2024-06-16 10:45:20,517 classification_efficientnet-b4 INFO: Epoch[6] - Fold[2] - Iteration[10/52] Loss: 0.052\n2024-06-16 10:45:20,517 classification_efficientnet-b4 INFO: Epoch[6] - Fold[2] - Iteration[10/52] Loss: 0.052\n2024-06-16 10:45:25,833 classification_efficientnet-b4 INFO: Epoch[6] - Fold[2] - Iteration[20/52] Loss: 0.060\n2024-06-16 10:45:25,833 classification_efficientnet-b4 INFO: Epoch[6] - Fold[2] - Iteration[20/52] Loss: 0.060\n2024-06-16 10:45:25,833 classification_efficientnet-b4 INFO: Epoch[6] - Fold[2] - Iteration[20/52] Loss: 0.060\n2024-06-16 10:45:31,199 classification_efficientnet-b4 INFO: Epoch[6] - Fold[2] - Iteration[30/52] Loss: 0.035\n2024-06-16 10:45:31,199 classification_efficientnet-b4 INFO: Epoch[6] - Fold[2] - Iteration[30/52] Loss: 0.035\n2024-06-16 10:45:31,199 classification_efficientnet-b4 INFO: Epoch[6] - Fold[2] - Iteration[30/52] Loss: 0.035\n2024-06-16 10:45:36,591 classification_efficientnet-b4 INFO: Epoch[6] - Fold[2] - Iteration[40/52] Loss: 0.051\n2024-06-16 10:45:36,591 classification_efficientnet-b4 INFO: Epoch[6] - Fold[2] - Iteration[40/52] Loss: 0.051\n2024-06-16 10:45:36,591 classification_efficientnet-b4 INFO: Epoch[6] - Fold[2] - Iteration[40/52] Loss: 0.051\n2024-06-16 10:45:42,024 classification_efficientnet-b4 INFO: Epoch[6] - Fold[2] - Iteration[50/52] Loss: 0.040\n2024-06-16 10:45:42,024 classification_efficientnet-b4 INFO: Epoch[6] - Fold[2] - Iteration[50/52] Loss: 0.040\n2024-06-16 10:45:42,024 classification_efficientnet-b4 INFO: Epoch[6] - Fold[2] - Iteration[50/52] Loss: 0.040\n2024-06-16 10:45:42,574 classification_efficientnet-b4 INFO: Training Result: Epoch 6/10 Fold 2/3, Loss: 0.066, Time epoch: 28.058s\n2024-06-16 10:45:42,574 classification_efficientnet-b4 INFO: Training Result: Epoch 6/10 Fold 2/3, Loss: 0.066, Time epoch: 28.058s\n2024-06-16 10:45:42,574 classification_efficientnet-b4 INFO: Training Result: Epoch 6/10 Fold 2/3, Loss: 0.066, Time epoch: 28.058s\n2024-06-16 10:45:48,998 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.092, Accuracy: 0.556 F1-Score: 0.533, Precision: 0.531, Recall: 0.539\n2024-06-16 10:45:48,998 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.092, Accuracy: 0.556 F1-Score: 0.533, Precision: 0.531, Recall: 0.539\n2024-06-16 10:45:48,998 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.092, Accuracy: 0.556 F1-Score: 0.533, Precision: 0.531, Recall: 0.539\n2024-06-16 10:45:49,013 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:45:49,013 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:45:49,013 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:45:49,567 classification_efficientnet-b4 INFO: Epoch[7] - Fold[2] - Iteration[0/52] Loss: 0.017\n2024-06-16 10:45:49,567 classification_efficientnet-b4 INFO: Epoch[7] - Fold[2] - Iteration[0/52] Loss: 0.017\n2024-06-16 10:45:49,567 classification_efficientnet-b4 INFO: Epoch[7] - Fold[2] - Iteration[0/52] Loss: 0.017\n2024-06-16 10:45:54,921 classification_efficientnet-b4 INFO: Epoch[7] - Fold[2] - Iteration[10/52] Loss: 0.066\n2024-06-16 10:45:54,921 classification_efficientnet-b4 INFO: Epoch[7] - Fold[2] - Iteration[10/52] Loss: 0.066\n2024-06-16 10:45:54,921 classification_efficientnet-b4 INFO: Epoch[7] - Fold[2] - Iteration[10/52] Loss: 0.066\n2024-06-16 10:46:00,354 classification_efficientnet-b4 INFO: Epoch[7] - Fold[2] - Iteration[20/52] Loss: 0.086\n2024-06-16 10:46:00,354 classification_efficientnet-b4 INFO: Epoch[7] - Fold[2] - Iteration[20/52] Loss: 0.086\n2024-06-16 10:46:00,354 classification_efficientnet-b4 INFO: Epoch[7] - Fold[2] - Iteration[20/52] Loss: 0.086\n2024-06-16 10:46:05,801 classification_efficientnet-b4 INFO: Epoch[7] - Fold[2] - Iteration[30/52] Loss: 0.035\n2024-06-16 10:46:05,801 classification_efficientnet-b4 INFO: Epoch[7] - Fold[2] - Iteration[30/52] Loss: 0.035\n2024-06-16 10:46:05,801 classification_efficientnet-b4 INFO: Epoch[7] - Fold[2] - Iteration[30/52] Loss: 0.035\n2024-06-16 10:46:11,165 classification_efficientnet-b4 INFO: Epoch[7] - Fold[2] - Iteration[40/52] Loss: 0.039\n2024-06-16 10:46:11,165 classification_efficientnet-b4 INFO: Epoch[7] - Fold[2] - Iteration[40/52] Loss: 0.039\n2024-06-16 10:46:11,165 classification_efficientnet-b4 INFO: Epoch[7] - Fold[2] - Iteration[40/52] Loss: 0.039\n2024-06-16 10:46:16,535 classification_efficientnet-b4 INFO: Epoch[7] - Fold[2] - Iteration[50/52] Loss: 0.042\n2024-06-16 10:46:16,535 classification_efficientnet-b4 INFO: Epoch[7] - Fold[2] - Iteration[50/52] Loss: 0.042\n2024-06-16 10:46:16,535 classification_efficientnet-b4 INFO: Epoch[7] - Fold[2] - Iteration[50/52] Loss: 0.042\n2024-06-16 10:46:17,064 classification_efficientnet-b4 INFO: Training Result: Epoch 7/10 Fold 2/3, Loss: 0.062, Time epoch: 28.055s\n2024-06-16 10:46:17,064 classification_efficientnet-b4 INFO: Training Result: Epoch 7/10 Fold 2/3, Loss: 0.062, Time epoch: 28.055s\n2024-06-16 10:46:17,064 classification_efficientnet-b4 INFO: Training Result: Epoch 7/10 Fold 2/3, Loss: 0.062, Time epoch: 28.055s\n2024-06-16 10:46:23,436 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.086, Accuracy: 0.560 F1-Score: 0.554, Precision: 0.551, Recall: 0.560\n2024-06-16 10:46:23,436 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.086, Accuracy: 0.560 F1-Score: 0.554, Precision: 0.551, Recall: 0.560\n2024-06-16 10:46:23,436 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.086, Accuracy: 0.560 F1-Score: 0.554, Precision: 0.551, Recall: 0.560\n2024-06-16 10:46:23,450 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:46:23,450 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:46:23,450 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:46:24,000 classification_efficientnet-b4 INFO: Epoch[8] - Fold[2] - Iteration[0/52] Loss: 0.056\n2024-06-16 10:46:24,000 classification_efficientnet-b4 INFO: Epoch[8] - Fold[2] - Iteration[0/52] Loss: 0.056\n2024-06-16 10:46:24,000 classification_efficientnet-b4 INFO: Epoch[8] - Fold[2] - Iteration[0/52] Loss: 0.056\n2024-06-16 10:46:29,337 classification_efficientnet-b4 INFO: Epoch[8] - Fold[2] - Iteration[10/52] Loss: 0.038\n2024-06-16 10:46:29,337 classification_efficientnet-b4 INFO: Epoch[8] - Fold[2] - Iteration[10/52] Loss: 0.038\n2024-06-16 10:46:29,337 classification_efficientnet-b4 INFO: Epoch[8] - Fold[2] - Iteration[10/52] Loss: 0.038\n2024-06-16 10:46:34,665 classification_efficientnet-b4 INFO: Epoch[8] - Fold[2] - Iteration[20/52] Loss: 0.057\n2024-06-16 10:46:34,665 classification_efficientnet-b4 INFO: Epoch[8] - Fold[2] - Iteration[20/52] Loss: 0.057\n2024-06-16 10:46:34,665 classification_efficientnet-b4 INFO: Epoch[8] - Fold[2] - Iteration[20/52] Loss: 0.057\n2024-06-16 10:46:40,059 classification_efficientnet-b4 INFO: Epoch[8] - Fold[2] - Iteration[30/52] Loss: 0.039\n2024-06-16 10:46:40,059 classification_efficientnet-b4 INFO: Epoch[8] - Fold[2] - Iteration[30/52] Loss: 0.039\n2024-06-16 10:46:40,059 classification_efficientnet-b4 INFO: Epoch[8] - Fold[2] - Iteration[30/52] Loss: 0.039\n2024-06-16 10:46:45,458 classification_efficientnet-b4 INFO: Epoch[8] - Fold[2] - Iteration[40/52] Loss: 0.031\n2024-06-16 10:46:45,458 classification_efficientnet-b4 INFO: Epoch[8] - Fold[2] - Iteration[40/52] Loss: 0.031\n2024-06-16 10:46:45,458 classification_efficientnet-b4 INFO: Epoch[8] - Fold[2] - Iteration[40/52] Loss: 0.031\n2024-06-16 10:46:50,938 classification_efficientnet-b4 INFO: Epoch[8] - Fold[2] - Iteration[50/52] Loss: 0.052\n2024-06-16 10:46:50,938 classification_efficientnet-b4 INFO: Epoch[8] - Fold[2] - Iteration[50/52] Loss: 0.052\n2024-06-16 10:46:50,938 classification_efficientnet-b4 INFO: Epoch[8] - Fold[2] - Iteration[50/52] Loss: 0.052\n2024-06-16 10:46:51,478 classification_efficientnet-b4 INFO: Training Result: Epoch 8/10 Fold 2/3, Loss: 0.051, Time epoch: 28.032s\n2024-06-16 10:46:51,478 classification_efficientnet-b4 INFO: Training Result: Epoch 8/10 Fold 2/3, Loss: 0.051, Time epoch: 28.032s\n2024-06-16 10:46:51,478 classification_efficientnet-b4 INFO: Training Result: Epoch 8/10 Fold 2/3, Loss: 0.051, Time epoch: 28.032s\n2024-06-16 10:46:57,716 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.083, Accuracy: 0.560 F1-Score: 0.554, Precision: 0.551, Recall: 0.560\n2024-06-16 10:46:57,716 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.083, Accuracy: 0.560 F1-Score: 0.554, Precision: 0.551, Recall: 0.560\n2024-06-16 10:46:57,716 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.083, Accuracy: 0.560 F1-Score: 0.554, Precision: 0.551, Recall: 0.560\n2024-06-16 10:46:57,729 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:46:57,729 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:46:57,729 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:46:58,278 classification_efficientnet-b4 INFO: Epoch[9] - Fold[2] - Iteration[0/52] Loss: 0.054\n2024-06-16 10:46:58,278 classification_efficientnet-b4 INFO: Epoch[9] - Fold[2] - Iteration[0/52] Loss: 0.054\n2024-06-16 10:46:58,278 classification_efficientnet-b4 INFO: Epoch[9] - Fold[2] - Iteration[0/52] Loss: 0.054\n2024-06-16 10:47:03,652 classification_efficientnet-b4 INFO: Epoch[9] - Fold[2] - Iteration[10/52] Loss: 0.057\n2024-06-16 10:47:03,652 classification_efficientnet-b4 INFO: Epoch[9] - Fold[2] - Iteration[10/52] Loss: 0.057\n2024-06-16 10:47:03,652 classification_efficientnet-b4 INFO: Epoch[9] - Fold[2] - Iteration[10/52] Loss: 0.057\n2024-06-16 10:47:09,066 classification_efficientnet-b4 INFO: Epoch[9] - Fold[2] - Iteration[20/52] Loss: 0.066\n2024-06-16 10:47:09,066 classification_efficientnet-b4 INFO: Epoch[9] - Fold[2] - Iteration[20/52] Loss: 0.066\n2024-06-16 10:47:09,066 classification_efficientnet-b4 INFO: Epoch[9] - Fold[2] - Iteration[20/52] Loss: 0.066\n2024-06-16 10:47:14,415 classification_efficientnet-b4 INFO: Epoch[9] - Fold[2] - Iteration[30/52] Loss: 0.058\n2024-06-16 10:47:14,415 classification_efficientnet-b4 INFO: Epoch[9] - Fold[2] - Iteration[30/52] Loss: 0.058\n2024-06-16 10:47:14,415 classification_efficientnet-b4 INFO: Epoch[9] - Fold[2] - Iteration[30/52] Loss: 0.058\n2024-06-16 10:47:19,782 classification_efficientnet-b4 INFO: Epoch[9] - Fold[2] - Iteration[40/52] Loss: 0.055\n2024-06-16 10:47:19,782 classification_efficientnet-b4 INFO: Epoch[9] - Fold[2] - Iteration[40/52] Loss: 0.055\n2024-06-16 10:47:19,782 classification_efficientnet-b4 INFO: Epoch[9] - Fold[2] - Iteration[40/52] Loss: 0.055\n2024-06-16 10:47:25,140 classification_efficientnet-b4 INFO: Epoch[9] - Fold[2] - Iteration[50/52] Loss: 0.044\n2024-06-16 10:47:25,140 classification_efficientnet-b4 INFO: Epoch[9] - Fold[2] - Iteration[50/52] Loss: 0.044\n2024-06-16 10:47:25,140 classification_efficientnet-b4 INFO: Epoch[9] - Fold[2] - Iteration[50/52] Loss: 0.044\n2024-06-16 10:47:25,676 classification_efficientnet-b4 INFO: Training Result: Epoch 9/10 Fold 2/3, Loss: 0.049, Time epoch: 27.950s\n2024-06-16 10:47:25,676 classification_efficientnet-b4 INFO: Training Result: Epoch 9/10 Fold 2/3, Loss: 0.049, Time epoch: 27.950s\n2024-06-16 10:47:25,676 classification_efficientnet-b4 INFO: Training Result: Epoch 9/10 Fold 2/3, Loss: 0.049, Time epoch: 27.950s\n2024-06-16 10:47:31,832 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.080, Accuracy: 0.560 F1-Score: 0.554, Precision: 0.551, Recall: 0.560\n2024-06-16 10:47:31,832 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.080, Accuracy: 0.560 F1-Score: 0.554, Precision: 0.551, Recall: 0.560\n2024-06-16 10:47:31,832 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.080, Accuracy: 0.560 F1-Score: 0.554, Precision: 0.551, Recall: 0.560\n2024-06-16 10:47:31,845 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:47:31,845 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:47:31,845 classification_efficientnet-b4 INFO: Start training\n2024-06-16 10:47:32,380 classification_efficientnet-b4 INFO: Epoch[10] - Fold[2] - Iteration[0/52] Loss: 0.059\n2024-06-16 10:47:32,380 classification_efficientnet-b4 INFO: Epoch[10] - Fold[2] - Iteration[0/52] Loss: 0.059\n2024-06-16 10:47:32,380 classification_efficientnet-b4 INFO: Epoch[10] - Fold[2] - Iteration[0/52] Loss: 0.059\n2024-06-16 10:47:37,716 classification_efficientnet-b4 INFO: Epoch[10] - Fold[2] - Iteration[10/52] Loss: 0.098\n2024-06-16 10:47:37,716 classification_efficientnet-b4 INFO: Epoch[10] - Fold[2] - Iteration[10/52] Loss: 0.098\n2024-06-16 10:47:37,716 classification_efficientnet-b4 INFO: Epoch[10] - Fold[2] - Iteration[10/52] Loss: 0.098\n2024-06-16 10:47:43,091 classification_efficientnet-b4 INFO: Epoch[10] - Fold[2] - Iteration[20/52] Loss: 0.030\n2024-06-16 10:47:43,091 classification_efficientnet-b4 INFO: Epoch[10] - Fold[2] - Iteration[20/52] Loss: 0.030\n2024-06-16 10:47:43,091 classification_efficientnet-b4 INFO: Epoch[10] - Fold[2] - Iteration[20/52] Loss: 0.030\n2024-06-16 10:47:48,438 classification_efficientnet-b4 INFO: Epoch[10] - Fold[2] - Iteration[30/52] Loss: 0.051\n2024-06-16 10:47:48,438 classification_efficientnet-b4 INFO: Epoch[10] - Fold[2] - Iteration[30/52] Loss: 0.051\n2024-06-16 10:47:48,438 classification_efficientnet-b4 INFO: Epoch[10] - Fold[2] - Iteration[30/52] Loss: 0.051\n2024-06-16 10:47:53,901 classification_efficientnet-b4 INFO: Epoch[10] - Fold[2] - Iteration[40/52] Loss: 0.069\n2024-06-16 10:47:53,901 classification_efficientnet-b4 INFO: Epoch[10] - Fold[2] - Iteration[40/52] Loss: 0.069\n2024-06-16 10:47:53,901 classification_efficientnet-b4 INFO: Epoch[10] - Fold[2] - Iteration[40/52] Loss: 0.069\n2024-06-16 10:47:59,327 classification_efficientnet-b4 INFO: Epoch[10] - Fold[2] - Iteration[50/52] Loss: 0.084\n2024-06-16 10:47:59,327 classification_efficientnet-b4 INFO: Epoch[10] - Fold[2] - Iteration[50/52] Loss: 0.084\n2024-06-16 10:47:59,327 classification_efficientnet-b4 INFO: Epoch[10] - Fold[2] - Iteration[50/52] Loss: 0.084\n2024-06-16 10:47:59,860 classification_efficientnet-b4 INFO: Training Result: Epoch 10/10 Fold 2/3, Loss: 0.068, Time epoch: 28.018s\n2024-06-16 10:47:59,860 classification_efficientnet-b4 INFO: Training Result: Epoch 10/10 Fold 2/3, Loss: 0.068, Time epoch: 28.018s\n2024-06-16 10:47:59,860 classification_efficientnet-b4 INFO: Training Result: Epoch 10/10 Fold 2/3, Loss: 0.068, Time epoch: 28.018s\n2024-06-16 10:48:06,084 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.078, Accuracy: 0.589 F1-Score: 0.321, Precision: 0.424, Recall: 0.282\n2024-06-16 10:48:06,084 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.078, Accuracy: 0.589 F1-Score: 0.321, Precision: 0.424, Recall: 0.282\n2024-06-16 10:48:06,084 classification_efficientnet-b4 INFO: Validation Result: Loss: 0.078, Accuracy: 0.589 F1-Score: 0.321, Precision: 0.424, Recall: 0.282\n2024-06-16 10:48:06,094 classification_efficientnet-b4 INFO: Save model at epoch 10, saving model\n2024-06-16 10:48:06,094 classification_efficientnet-b4 INFO: Save model at epoch 10, saving model\n2024-06-16 10:48:06,094 classification_efficientnet-b4 INFO: Save model at epoch 10, saving model\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 3.4. Train Segmentation","metadata":{"id":"0T2vcliPw5Bj"}},{"cell_type":"code","source":"","metadata":{"id":"wcIG0NgaELJi","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef train(folds):\n    for fold in range(N_FOLDS):\n        #TASK\n        TASK = \"segmentation\"\n\n        #Path\n        weight_dir, log_dir, logger_name = init_path(TASK)\n\n\n        #Model\n        model = segmentation_model().to(DEVICE)\n\n        #Loss & Optimizer\n        model = model.to(DEVICE)\n        dice_loss = smp.losses.DiceLoss(mode='binary', from_logits=True)\n        optimizer = optim.Adam(model.parameters(), lr=BASE_LR, weight_decay=1e-5)\n\n\n        #Meters\n        overall_meter = AverageMeter()\n        iou_meter = AverageMeter()\n        dice_meter = AverageMeter()\n        train_loss_meter = AverageMeter()\n        val_loss_meter = AverageMeter()\n        precision_meter = AverageMeter()\n        recall_meter = AverageMeter()\n        f1_score_meter = AverageMeter()\n        \n        train_images, val_images = folds[fold]\n        train_set = BUSI(DATASET_DIR, train_images, val_images, input_size=INPUT_SIZE,transform=transform, target_transform=None, is_train=True)\n        val_set = BUSI(DATASET_DIR, train_images, val_images, input_size=INPUT_SIZE,transform=transform, target_transform=None, is_train=False)\n\n        train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n        val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n\n        logger = setup_logger(logger_name, log_dir)\n        stale = 0\n        best_overall = 0\n        start_epoch = 1\n\n        if CHECKPOINT is not None:\n            if os.path.exists(CHECKPOINT):\n                checkpoint = torch.load(CHECKPOINT)\n                model.load_state_dict(checkpoint['model_state_dict'])\n                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n                start_epoch = checkpoint['epoch']\n                best_overall = checkpoint['best_overall']\n                logger.info(f\"Resume training from epoch {start_epoch}\")\n            else:\n                logger.info(f\"Checkpoint not found, start training from epoch 1\")\n        #Logging hyperparameters\n        logging_hyperparameters(logger)\n\n        for epoch in range(start_epoch, 1+MAX_EPOCHS):\n            start_time = time.time()\n            #Train\n            model.train()\n            #Reset meters\n            overall_meter.reset()\n            train_loss_meter.reset()\n            val_loss_meter.reset()\n\n            iou_meter.reset()\n            dice_meter.reset()\n            precision_meter.reset()\n            recall_meter.reset()\n            f1_score_meter.reset()\n\n            logger.info(\"Start training\")\n            for batch_idx, (image, mask, _) in enumerate(train_loader):\n                n = image.shape[0]\n                optimizer.zero_grad()\n                image = image.to(DEVICE)\n                mask = mask.to(DEVICE)\n\n                output = model(image) #Logits\n                #Cal loss\n                train_loss = dice_loss(output, mask)\n                train_loss.backward()\n                optimizer.step()\n\n                train_loss_meter.update(train_loss.item(),n)\n\n                if batch_idx % 10 == 0:\n                    logger.info(f\"Epoch[{epoch}] - Fold[{fold}] - Iteration[{batch_idx}/{len(train_loader)}] Loss: {train_loss:.3f}\")\n            end_time = time.time()\n            logger.info(f\"Training Result: Epoch {epoch}/{MAX_EPOCHS} - Fold {fold}/{N_FOLDS}, Loss: {train_loss_meter.avg:.3f}, Time epoch: {end_time-start_time:.3f}s\")\n\n            #Valid\n            model.eval()\n            with torch.no_grad():\n                for batch_idx, (image, mask, _) in enumerate(val_loader):\n                    n = image.shape[0]\n                    image = image.to(DEVICE)\n                    mask = mask.to(DEVICE)\n\n                    output = model(image)\n                    val_loss = dice_loss(output, mask)\n\n                    # #Calculate metrics\n                    mask = F.sigmoid(mask).round().long()\n                    tp, fp, fn, tn = smp.metrics.get_stats(output, mask, mode='binary', threshold=0.5)\n\n\n                    iou_score = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"macro\")\n\n                    dice_score = torch.mean((2*tp.sum(0)/(2*tp.sum(0) + fp.sum(0) + fn.sum(0) + 1e-5)))\n                    precision_score = smp.metrics.precision(tp, fp, fn, tn, reduction=\"macro\")\n                    recall_score = smp.metrics.recall(tp, fp, fn, tn, reduction=\"macro\")\n                    f1_score = smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"macro\")\n\n\n                    #Update meters\n                    val_loss_meter.update(val_loss.item(), n)\n\n                    iou_meter.update(iou_score.item(), n)\n                    dice_meter.update(dice_score.item(), n)\n                    precision_meter.update(precision_score.item(), n)\n                    recall_meter.update(recall_score.item(), n)\n                    f1_score_meter.update(f1_score.item(), n)\n\n                    #Overall score\n                    overall_score = (iou_score + dice_score + f1_score)/3\n                    overall_meter.update(overall_score.item(), n)\n\n            logger.info(f\"Validation Result: Dice Loss: {val_loss_meter.avg:.3f}, IoU: {iou_meter.avg:.3f}, Dice Score: {dice_meter.avg:.3f}, F1-Score: {f1_score_meter.avg:.3f}, Average Score: {overall_meter.avg:.3f}\")\n\n            #Save best model\n            to_save = {\n                    'epoch': epoch,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'best_overall': best_overall,\n                }\n            if overall_meter.avg > best_overall: # best base on IoU score\n                logger.info(f\"Best model found at epoch {epoch}, saving model\")\n\n                torch.save(to_save, os.path.join(weight_dir,f\"best_epoch{epoch}_fold{fold}_{INPUT_SIZE[0]}_BS={BATCH_SIZE}_average={overall_meter.avg:.3f}.pth\"))\n                best_overall = overall_meter.avg\n                stale = 0\n            else:\n                stale += 1\n                if stale > 300:\n                    logger.info(f\"No improvement {300} consecutive epochs, early stopping\")\n                    break\n            if epoch % SAVE_INTERVAL == 0 or epoch == MAX_EPOCHS:\n                logger.info(f\"Save model at epoch {epoch}, saving model\")\n                torch.save(to_save, os.path.join(weight_dir,f\"epoch{epoch}_fold{fold}.pth\"))\n","metadata":{"id":"mFcBifzMw8wB","execution":{"iopub.status.busy":"2024-06-16T10:48:36.783645Z","iopub.execute_input":"2024-06-16T10:48:36.783993Z","iopub.status.idle":"2024-06-16T10:48:36.810880Z","shell.execute_reply.started":"2024-06-16T10:48:36.783966Z","shell.execute_reply":"2024-06-16T10:48:36.809981Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"train(folds)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I76QY9xXw_ev","outputId":"4cc94ace-4f36-49ad-c37f-ae5c382feb4a","execution":{"iopub.status.busy":"2024-01-11T04:53:18.903320Z","iopub.execute_input":"2024-01-11T04:53:18.903619Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"2024-01-11 04:53:18,918 segmentation_efficientnet-b4_deeplabv3plus INFO: ==========Hyperparameters==========\n2024-01-11 04:53:18,919 segmentation_efficientnet-b4_deeplabv3plus INFO: Device: cuda\n2024-01-11 04:53:18,920 segmentation_efficientnet-b4_deeplabv3plus INFO: Architecture: deeplabv3plus\n2024-01-11 04:53:18,921 segmentation_efficientnet-b4_deeplabv3plus INFO: Encoder: efficientnet-b4\n2024-01-11 04:53:18,921 segmentation_efficientnet-b4_deeplabv3plus INFO: Encoder weight: imagenet\n2024-01-11 04:53:18,922 segmentation_efficientnet-b4_deeplabv3plus INFO: Input size: (448, 448)\n2024-01-11 04:53:18,923 segmentation_efficientnet-b4_deeplabv3plus INFO: Batch size: 8\n2024-01-11 04:53:18,925 segmentation_efficientnet-b4_deeplabv3plus INFO: Base learning rate: 0.01\n2024-01-11 04:53:18,925 segmentation_efficientnet-b4_deeplabv3plus INFO: Max epochs: 10\n2024-01-11 04:53:18,926 segmentation_efficientnet-b4_deeplabv3plus INFO: Weight decay: 1e-05\n2024-01-11 04:53:18,927 segmentation_efficientnet-b4_deeplabv3plus INFO: ===================================\n2024-01-11 04:53:18,932 segmentation_efficientnet-b4_deeplabv3plus INFO: Start training\n2024-01-11 04:53:19,705 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[1] - Fold[0] - Iteration[0/52] Loss: 0.542\n2024-01-11 04:53:26,403 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[1] - Fold[0] - Iteration[10/52] Loss: 0.109\n2024-01-11 04:53:33,128 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[1] - Fold[0] - Iteration[20/52] Loss: 0.040\n2024-01-11 04:53:39,813 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[1] - Fold[0] - Iteration[30/52] Loss: 0.071\n2024-01-11 04:53:46,557 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[1] - Fold[0] - Iteration[40/52] Loss: 0.063\n2024-01-11 04:53:53,252 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[1] - Fold[0] - Iteration[50/52] Loss: 0.056\n2024-01-11 04:53:53,841 segmentation_efficientnet-b4_deeplabv3plus INFO: Training Result: Epoch 1/10 - Fold 0/3, Loss: 0.097, Time epoch: 34.913s\n2024-01-11 04:54:01,402 segmentation_efficientnet-b4_deeplabv3plus INFO: Validation Result: Dice Loss: 0.083, IoU: 0.535, Dice Score: 0.478, F1-Score: 0.555, Average Score: 0.523\n2024-01-11 04:54:01,409 segmentation_efficientnet-b4_deeplabv3plus INFO: Best model found at epoch 1, saving model\n2024-01-11 04:54:01,890 segmentation_efficientnet-b4_deeplabv3plus INFO: Start training\n2024-01-11 04:54:02,585 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[2] - Fold[0] - Iteration[0/52] Loss: 0.096\n2024-01-11 04:54:09,364 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[2] - Fold[0] - Iteration[10/52] Loss: 0.072\n2024-01-11 04:54:16,056 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[2] - Fold[0] - Iteration[20/52] Loss: 0.087\n2024-01-11 04:54:22,788 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[2] - Fold[0] - Iteration[30/52] Loss: 0.072\n2024-01-11 04:54:29,424 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[2] - Fold[0] - Iteration[40/52] Loss: 0.159\n2024-01-11 04:54:36,136 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[2] - Fold[0] - Iteration[50/52] Loss: 0.056\n2024-01-11 04:54:36,728 segmentation_efficientnet-b4_deeplabv3plus INFO: Training Result: Epoch 2/10 - Fold 0/3, Loss: 0.075, Time epoch: 34.841s\n2024-01-11 04:54:44,249 segmentation_efficientnet-b4_deeplabv3plus INFO: Validation Result: Dice Loss: 0.083, IoU: 0.535, Dice Score: 0.478, F1-Score: 0.555, Average Score: 0.523\n2024-01-11 04:54:44,261 segmentation_efficientnet-b4_deeplabv3plus INFO: Start training\n2024-01-11 04:54:44,918 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[3] - Fold[0] - Iteration[0/52] Loss: 0.109\n2024-01-11 04:54:51,635 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[3] - Fold[0] - Iteration[10/52] Loss: 0.048\n2024-01-11 04:54:58,347 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[3] - Fold[0] - Iteration[20/52] Loss: 0.071\n2024-01-11 04:55:05,068 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[3] - Fold[0] - Iteration[30/52] Loss: 0.092\n2024-01-11 04:55:11,777 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[3] - Fold[0] - Iteration[40/52] Loss: 0.047\n2024-01-11 04:55:18,435 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[3] - Fold[0] - Iteration[50/52] Loss: 0.084\n2024-01-11 04:55:19,050 segmentation_efficientnet-b4_deeplabv3plus INFO: Training Result: Epoch 3/10 - Fold 0/3, Loss: 0.075, Time epoch: 34.793s\n2024-01-11 04:55:26,701 segmentation_efficientnet-b4_deeplabv3plus INFO: Validation Result: Dice Loss: 0.083, IoU: 0.535, Dice Score: 0.478, F1-Score: 0.555, Average Score: 0.523\n2024-01-11 04:55:26,712 segmentation_efficientnet-b4_deeplabv3plus INFO: Start training\n2024-01-11 04:55:27,378 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[4] - Fold[0] - Iteration[0/52] Loss: 0.114\n2024-01-11 04:55:34,068 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[4] - Fold[0] - Iteration[10/52] Loss: 0.066\n2024-01-11 04:55:40,731 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[4] - Fold[0] - Iteration[20/52] Loss: 0.082\n2024-01-11 04:55:47,427 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[4] - Fold[0] - Iteration[30/52] Loss: 0.080\n2024-01-11 04:55:54,163 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[4] - Fold[0] - Iteration[40/52] Loss: 0.106\n2024-01-11 04:56:00,841 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[4] - Fold[0] - Iteration[50/52] Loss: 0.117\n2024-01-11 04:56:01,424 segmentation_efficientnet-b4_deeplabv3plus INFO: Training Result: Epoch 4/10 - Fold 0/3, Loss: 0.075, Time epoch: 34.714s\n2024-01-11 04:56:08,961 segmentation_efficientnet-b4_deeplabv3plus INFO: Validation Result: Dice Loss: 0.083, IoU: 0.535, Dice Score: 0.478, F1-Score: 0.555, Average Score: 0.523\n2024-01-11 04:56:08,974 segmentation_efficientnet-b4_deeplabv3plus INFO: Start training\n2024-01-11 04:56:09,640 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[5] - Fold[0] - Iteration[0/52] Loss: 0.055\n2024-01-11 04:56:16,346 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[5] - Fold[0] - Iteration[10/52] Loss: 0.054\n2024-01-11 04:56:23,002 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[5] - Fold[0] - Iteration[20/52] Loss: 0.112\n2024-01-11 04:56:29,740 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[5] - Fold[0] - Iteration[30/52] Loss: 0.084\n2024-01-11 04:56:36,451 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[5] - Fold[0] - Iteration[40/52] Loss: 0.137\n2024-01-11 04:56:43,128 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[5] - Fold[0] - Iteration[50/52] Loss: 0.058\n2024-01-11 04:56:43,736 segmentation_efficientnet-b4_deeplabv3plus INFO: Training Result: Epoch 5/10 - Fold 0/3, Loss: 0.075, Time epoch: 34.766s\n2024-01-11 04:56:51,308 segmentation_efficientnet-b4_deeplabv3plus INFO: Validation Result: Dice Loss: 0.083, IoU: 0.535, Dice Score: 0.478, F1-Score: 0.555, Average Score: 0.523\n2024-01-11 04:56:51,319 segmentation_efficientnet-b4_deeplabv3plus INFO: Start training\n2024-01-11 04:56:51,989 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[6] - Fold[0] - Iteration[0/52] Loss: 0.063\n2024-01-11 04:56:58,732 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[6] - Fold[0] - Iteration[10/52] Loss: 0.040\n2024-01-11 04:57:05,376 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[6] - Fold[0] - Iteration[20/52] Loss: 0.092\n2024-01-11 04:57:12,076 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[6] - Fold[0] - Iteration[30/52] Loss: 0.079\n2024-01-11 04:57:18,751 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[6] - Fold[0] - Iteration[40/52] Loss: 0.119\n2024-01-11 04:57:25,427 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[6] - Fold[0] - Iteration[50/52] Loss: 0.041\n2024-01-11 04:57:26,030 segmentation_efficientnet-b4_deeplabv3plus INFO: Training Result: Epoch 6/10 - Fold 0/3, Loss: 0.075, Time epoch: 34.714s\n2024-01-11 04:57:33,741 segmentation_efficientnet-b4_deeplabv3plus INFO: Validation Result: Dice Loss: 0.083, IoU: 0.535, Dice Score: 0.478, F1-Score: 0.555, Average Score: 0.523\n2024-01-11 04:57:33,752 segmentation_efficientnet-b4_deeplabv3plus INFO: Start training\n2024-01-11 04:57:34,408 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[7] - Fold[0] - Iteration[0/52] Loss: 0.046\n2024-01-11 04:57:41,080 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[7] - Fold[0] - Iteration[10/52] Loss: 0.103\n2024-01-11 04:57:47,762 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[7] - Fold[0] - Iteration[20/52] Loss: 0.083\n2024-01-11 04:57:54,452 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[7] - Fold[0] - Iteration[30/52] Loss: 0.119\n2024-01-11 04:58:01,223 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[7] - Fold[0] - Iteration[40/52] Loss: 0.027\n2024-01-11 04:58:07,867 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[7] - Fold[0] - Iteration[50/52] Loss: 0.071\n2024-01-11 04:58:08,446 segmentation_efficientnet-b4_deeplabv3plus INFO: Training Result: Epoch 7/10 - Fold 0/3, Loss: 0.075, Time epoch: 34.697s\n2024-01-11 04:58:16,011 segmentation_efficientnet-b4_deeplabv3plus INFO: Validation Result: Dice Loss: 0.083, IoU: 0.535, Dice Score: 0.478, F1-Score: 0.555, Average Score: 0.523\n2024-01-11 04:58:16,022 segmentation_efficientnet-b4_deeplabv3plus INFO: Start training\n2024-01-11 04:58:16,693 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[8] - Fold[0] - Iteration[0/52] Loss: 0.075\n2024-01-11 04:58:23,390 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[8] - Fold[0] - Iteration[10/52] Loss: 0.108\n2024-01-11 04:58:30,079 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[8] - Fold[0] - Iteration[20/52] Loss: 0.066\n2024-01-11 04:58:36,808 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[8] - Fold[0] - Iteration[30/52] Loss: 0.084\n2024-01-11 04:58:43,601 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[8] - Fold[0] - Iteration[40/52] Loss: 0.027\n2024-01-11 04:58:50,207 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[8] - Fold[0] - Iteration[50/52] Loss: 0.122\n2024-01-11 04:58:50,786 segmentation_efficientnet-b4_deeplabv3plus INFO: Training Result: Epoch 8/10 - Fold 0/3, Loss: 0.075, Time epoch: 34.767s\n2024-01-11 04:58:58,855 segmentation_efficientnet-b4_deeplabv3plus INFO: Validation Result: Dice Loss: 0.083, IoU: 0.535, Dice Score: 0.478, F1-Score: 0.555, Average Score: 0.523\n2024-01-11 04:58:58,867 segmentation_efficientnet-b4_deeplabv3plus INFO: Start training\n2024-01-11 04:58:59,531 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[9] - Fold[0] - Iteration[0/52] Loss: 0.112\n2024-01-11 04:59:06,376 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[9] - Fold[0] - Iteration[10/52] Loss: 0.069\n2024-01-11 04:59:13,104 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[9] - Fold[0] - Iteration[20/52] Loss: 0.059\n2024-01-11 04:59:19,912 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[9] - Fold[0] - Iteration[30/52] Loss: 0.102\n2024-01-11 04:59:26,624 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[9] - Fold[0] - Iteration[40/52] Loss: 0.042\n2024-01-11 04:59:33,328 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[9] - Fold[0] - Iteration[50/52] Loss: 0.030\n2024-01-11 04:59:33,924 segmentation_efficientnet-b4_deeplabv3plus INFO: Training Result: Epoch 9/10 - Fold 0/3, Loss: 0.075, Time epoch: 35.060s\n2024-01-11 04:59:41,460 segmentation_efficientnet-b4_deeplabv3plus INFO: Validation Result: Dice Loss: 0.083, IoU: 0.535, Dice Score: 0.478, F1-Score: 0.555, Average Score: 0.523\n2024-01-11 04:59:41,471 segmentation_efficientnet-b4_deeplabv3plus INFO: Start training\n2024-01-11 04:59:42,160 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[10] - Fold[0] - Iteration[0/52] Loss: 0.078\n2024-01-11 04:59:48,864 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[10] - Fold[0] - Iteration[10/52] Loss: 0.044\n2024-01-11 04:59:55,542 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[10] - Fold[0] - Iteration[20/52] Loss: 0.052\n2024-01-11 05:00:02,257 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[10] - Fold[0] - Iteration[30/52] Loss: 0.038\n2024-01-11 05:00:09,021 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[10] - Fold[0] - Iteration[40/52] Loss: 0.129\n2024-01-11 05:00:15,706 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[10] - Fold[0] - Iteration[50/52] Loss: 0.099\n2024-01-11 05:00:16,293 segmentation_efficientnet-b4_deeplabv3plus INFO: Training Result: Epoch 10/10 - Fold 0/3, Loss: 0.075, Time epoch: 34.825s\n2024-01-11 05:00:23,836 segmentation_efficientnet-b4_deeplabv3plus INFO: Validation Result: Dice Loss: 0.083, IoU: 0.535, Dice Score: 0.478, F1-Score: 0.555, Average Score: 0.523\n2024-01-11 05:00:23,844 segmentation_efficientnet-b4_deeplabv3plus INFO: Save model at epoch 10, saving model\n2024-01-11 05:00:24,308 segmentation_efficientnet-b4_deeplabv3plus INFO: ==========Hyperparameters==========\n2024-01-11 05:00:24,308 segmentation_efficientnet-b4_deeplabv3plus INFO: ==========Hyperparameters==========\n2024-01-11 05:00:24,310 segmentation_efficientnet-b4_deeplabv3plus INFO: Device: cuda\n2024-01-11 05:00:24,310 segmentation_efficientnet-b4_deeplabv3plus INFO: Device: cuda\n2024-01-11 05:00:24,312 segmentation_efficientnet-b4_deeplabv3plus INFO: Architecture: deeplabv3plus\n2024-01-11 05:00:24,312 segmentation_efficientnet-b4_deeplabv3plus INFO: Architecture: deeplabv3plus\n2024-01-11 05:00:24,315 segmentation_efficientnet-b4_deeplabv3plus INFO: Encoder: efficientnet-b4\n2024-01-11 05:00:24,315 segmentation_efficientnet-b4_deeplabv3plus INFO: Encoder: efficientnet-b4\n2024-01-11 05:00:24,316 segmentation_efficientnet-b4_deeplabv3plus INFO: Encoder weight: imagenet\n2024-01-11 05:00:24,316 segmentation_efficientnet-b4_deeplabv3plus INFO: Encoder weight: imagenet\n2024-01-11 05:00:24,318 segmentation_efficientnet-b4_deeplabv3plus INFO: Input size: (448, 448)\n2024-01-11 05:00:24,318 segmentation_efficientnet-b4_deeplabv3plus INFO: Input size: (448, 448)\n2024-01-11 05:00:24,320 segmentation_efficientnet-b4_deeplabv3plus INFO: Batch size: 8\n2024-01-11 05:00:24,320 segmentation_efficientnet-b4_deeplabv3plus INFO: Batch size: 8\n2024-01-11 05:00:24,322 segmentation_efficientnet-b4_deeplabv3plus INFO: Base learning rate: 0.01\n2024-01-11 05:00:24,322 segmentation_efficientnet-b4_deeplabv3plus INFO: Base learning rate: 0.01\n2024-01-11 05:00:24,324 segmentation_efficientnet-b4_deeplabv3plus INFO: Max epochs: 10\n2024-01-11 05:00:24,324 segmentation_efficientnet-b4_deeplabv3plus INFO: Max epochs: 10\n2024-01-11 05:00:24,326 segmentation_efficientnet-b4_deeplabv3plus INFO: Weight decay: 1e-05\n2024-01-11 05:00:24,326 segmentation_efficientnet-b4_deeplabv3plus INFO: Weight decay: 1e-05\n2024-01-11 05:00:24,328 segmentation_efficientnet-b4_deeplabv3plus INFO: ===================================\n2024-01-11 05:00:24,328 segmentation_efficientnet-b4_deeplabv3plus INFO: ===================================\n2024-01-11 05:00:24,332 segmentation_efficientnet-b4_deeplabv3plus INFO: Start training\n2024-01-11 05:00:24,332 segmentation_efficientnet-b4_deeplabv3plus INFO: Start training\n2024-01-11 05:00:25,011 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[1] - Fold[1] - Iteration[0/52] Loss: 0.055\n2024-01-11 05:00:25,011 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[1] - Fold[1] - Iteration[0/52] Loss: 0.055\n2024-01-11 05:00:31,737 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[1] - Fold[1] - Iteration[10/52] Loss: 0.039\n2024-01-11 05:00:31,737 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[1] - Fold[1] - Iteration[10/52] Loss: 0.039\n2024-01-11 05:00:38,473 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[1] - Fold[1] - Iteration[20/52] Loss: 0.091\n2024-01-11 05:00:38,473 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[1] - Fold[1] - Iteration[20/52] Loss: 0.091\n2024-01-11 05:00:45,172 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[1] - Fold[1] - Iteration[30/52] Loss: 0.069\n2024-01-11 05:00:45,172 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[1] - Fold[1] - Iteration[30/52] Loss: 0.069\n2024-01-11 05:00:51,830 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[1] - Fold[1] - Iteration[40/52] Loss: 0.094\n2024-01-11 05:00:51,830 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[1] - Fold[1] - Iteration[40/52] Loss: 0.094\n2024-01-11 05:00:58,457 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[1] - Fold[1] - Iteration[50/52] Loss: 0.147\n2024-01-11 05:00:58,457 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[1] - Fold[1] - Iteration[50/52] Loss: 0.147\n2024-01-11 05:00:59,053 segmentation_efficientnet-b4_deeplabv3plus INFO: Training Result: Epoch 1/10 - Fold 1/3, Loss: 0.082, Time epoch: 34.724s\n2024-01-11 05:00:59,053 segmentation_efficientnet-b4_deeplabv3plus INFO: Training Result: Epoch 1/10 - Fold 1/3, Loss: 0.082, Time epoch: 34.724s\n2024-01-11 05:01:06,703 segmentation_efficientnet-b4_deeplabv3plus INFO: Validation Result: Dice Loss: 0.069, IoU: 0.542, Dice Score: 0.482, F1-Score: 0.559, Average Score: 0.528\n2024-01-11 05:01:06,703 segmentation_efficientnet-b4_deeplabv3plus INFO: Validation Result: Dice Loss: 0.069, IoU: 0.542, Dice Score: 0.482, F1-Score: 0.559, Average Score: 0.528\n2024-01-11 05:01:06,711 segmentation_efficientnet-b4_deeplabv3plus INFO: Best model found at epoch 1, saving model\n2024-01-11 05:01:06,711 segmentation_efficientnet-b4_deeplabv3plus INFO: Best model found at epoch 1, saving model\n2024-01-11 05:01:07,374 segmentation_efficientnet-b4_deeplabv3plus INFO: Start training\n2024-01-11 05:01:07,374 segmentation_efficientnet-b4_deeplabv3plus INFO: Start training\n2024-01-11 05:01:08,028 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[2] - Fold[1] - Iteration[0/52] Loss: 0.204\n2024-01-11 05:01:08,028 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[2] - Fold[1] - Iteration[0/52] Loss: 0.204\n2024-01-11 05:01:14,746 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[2] - Fold[1] - Iteration[10/52] Loss: 0.133\n2024-01-11 05:01:14,746 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[2] - Fold[1] - Iteration[10/52] Loss: 0.133\n2024-01-11 05:01:21,460 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[2] - Fold[1] - Iteration[20/52] Loss: 0.106\n2024-01-11 05:01:21,460 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[2] - Fold[1] - Iteration[20/52] Loss: 0.106\n2024-01-11 05:01:28,184 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[2] - Fold[1] - Iteration[30/52] Loss: 0.105\n2024-01-11 05:01:28,184 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[2] - Fold[1] - Iteration[30/52] Loss: 0.105\n2024-01-11 05:01:34,938 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[2] - Fold[1] - Iteration[40/52] Loss: 0.111\n2024-01-11 05:01:34,938 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[2] - Fold[1] - Iteration[40/52] Loss: 0.111\n2024-01-11 05:01:41,797 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[2] - Fold[1] - Iteration[50/52] Loss: 0.084\n2024-01-11 05:01:41,797 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[2] - Fold[1] - Iteration[50/52] Loss: 0.084\n2024-01-11 05:01:42,382 segmentation_efficientnet-b4_deeplabv3plus INFO: Training Result: Epoch 2/10 - Fold 1/3, Loss: 0.082, Time epoch: 35.012s\n2024-01-11 05:01:42,382 segmentation_efficientnet-b4_deeplabv3plus INFO: Training Result: Epoch 2/10 - Fold 1/3, Loss: 0.082, Time epoch: 35.012s\n2024-01-11 05:01:50,048 segmentation_efficientnet-b4_deeplabv3plus INFO: Validation Result: Dice Loss: 0.069, IoU: 0.542, Dice Score: 0.482, F1-Score: 0.559, Average Score: 0.528\n2024-01-11 05:01:50,048 segmentation_efficientnet-b4_deeplabv3plus INFO: Validation Result: Dice Loss: 0.069, IoU: 0.542, Dice Score: 0.482, F1-Score: 0.559, Average Score: 0.528\n2024-01-11 05:01:50,061 segmentation_efficientnet-b4_deeplabv3plus INFO: Start training\n2024-01-11 05:01:50,061 segmentation_efficientnet-b4_deeplabv3plus INFO: Start training\n2024-01-11 05:01:50,745 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[3] - Fold[1] - Iteration[0/52] Loss: 0.046\n2024-01-11 05:01:50,745 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[3] - Fold[1] - Iteration[0/52] Loss: 0.046\n2024-01-11 05:01:57,371 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[3] - Fold[1] - Iteration[10/52] Loss: 0.068\n2024-01-11 05:01:57,371 segmentation_efficientnet-b4_deeplabv3plus INFO: Epoch[3] - Fold[1] - Iteration[10/52] Loss: 0.068\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"id":"C6Kb62cAIwow"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Multitask Model","metadata":{"id":"QAD1fxEdxB6X"}},{"cell_type":"code","source":"RESNET50_ENCODER_WEIGHTS_URL = \"https://download.pytorch.org/models/resnet50-19c8e357.pth\"\n\ndef multitask_model():\n    aux_param=dict(\n                    pooling='avg',             # one of 'avg', 'max'\n                    dropout=0.5,               # dropout ratio, default is None\n                    # activation='sigmoid',      # activation function, default is None\n                    classes=CLA_NUM_CLASSES,      # define number of output labels\n                )\n    model = segmentation_model(aux_param=aux_param)\n    return model","metadata":{"id":"ZRut7ATHEaT2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(folds):\n    for fold in range(N_FOLDS):\n        #Task\n        TASK = \"multitask\"\n\n        #Path\n        weight_dir, log_dir, logger_name = init_path(TASK)\n\n\n        #Model\n        model = multitask_model().to(DEVICE)\n\n        #Loss & Optimizer\n        dice_loss = smp.losses.DiceLoss(mode='binary', from_logits=True)\n        # CE_loss = torch.nn.CrossEntropyLoss()\n\n        optimizer = optim.Adam(model.parameters(), lr=BASE_LR, weight_decay=1e-5)\n\n\n\n        #Common meter\n        overall_meter = AverageMeter()\n        train_loss_meter = AverageMeter()\n        val_loss_meter = AverageMeter()\n\n        #Meters segmentation\n        seg_train_loss_meter = AverageMeter()\n        seg_val_loss_meter = AverageMeter()\n        seg_iou_meter = AverageMeter()\n        seg_dice_meter = AverageMeter()\n        seg_precision_meter = AverageMeter()\n        seg_recall_meter = AverageMeter()\n        seg_f1_score_meter = AverageMeter()\n\n        #Meters classification\n        cla_train_loss_meter = AverageMeter()\n        cla_val_loss_meter = AverageMeter()\n        cla_acc_meter = AverageMeter()\n        cla_precision_meter = AverageMeter()\n        cla_recall_meter = AverageMeter()\n        cla_f1_score_meter = AverageMeter()\n\n        train_images, val_images = folds[fold]\n        train_set = BUSI(DATASET_DIR, train_images, val_images, input_size=INPUT_SIZE,transform=transform, target_transform=None, is_train=True)\n        val_set = BUSI(DATASET_DIR, train_images, val_images, input_size=INPUT_SIZE,transform=transform, target_transform=None, is_train=False)\n\n        train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n        val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n\n                #Setup logging\n        logger = setup_logger(logger_name, log_dir)\n\n        start_epoch=1\n        best_overall = 0\n        stale = 0\n\n        if CHECKPOINT is not None:\n            if os.path.exists(CHECKPOINT):\n                checkpoint = torch.load(CHECKPOINT)\n                model.load_state_dict(checkpoint['model_state_dict'])\n                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n                start_epoch = checkpoint['epoch']\n                best_overall = checkpoint['best_overall']\n                print(f\"Resume training from epoch {start_epoch}\")\n            else:\n                print(f\"Checkpoint not found, start training from epoch 1\")\n\n        #Logging hyperparameters\n        logging_hyperparameters(logger)\n\n        for epoch in range(start_epoch, 1+MAX_EPOCHS):\n            start_time = time.time()\n            #Train\n            model.train()\n\n            #Reset meters\n            #Common meter\n            train_loss_meter.reset()\n            val_loss_meter.reset()\n            overall_meter.reset()\n\n            #Meters segmentation\n            seg_iou_meter.reset()\n            seg_dice_meter.reset()\n            seg_precision_meter.reset()\n            seg_recall_meter.reset()\n            seg_f1_score_meter.reset()\n\n            #Meters classification\n            cla_acc_meter.reset()\n            cla_precision_meter.reset()\n            cla_recall_meter.reset()\n            cla_f1_score_meter.reset()\n\n            logger.info(\"Start training\")\n            for batch_idx, (image, mask, label) in enumerate(train_loader):\n                n = image.shape[0]\n                optimizer.zero_grad()\n                image = image.to(DEVICE)\n                mask = mask.to(DEVICE)\n                label = label.to(DEVICE)\n\n                #Forward\n                output_mask, output_classification = model(image)\n\n                #Cal loss\n                loss_segmentation = dice_loss(output_mask, mask)\n                loss_classification = focal_loss(output_classification, label, alpha=0.25, gamma=2,reduction='mean')\n                train_loss = ALPHA*loss_segmentation + (1 - ALPHA)*loss_classification\n\n                train_loss.backward()\n                optimizer.step()\n                train_loss_meter.update(train_loss.item(), n)\n                seg_train_loss_meter.update(loss_segmentation.item(), n)\n                cla_train_loss_meter.update(loss_classification.item(), n)\n                if batch_idx % 10 == 0:\n                    logger.info(f\"Epoch[{epoch}] Iteration[{batch_idx}/{len(train_loader)}] Loss: {train_loss:.3f}\")\n\n            end_time = time.time()\n            logger.info(f\"Training Result: Epoch {epoch}/{MAX_EPOCHS}, Loss: {train_loss_meter.avg:.3f}  Segmentation loss: {seg_train_loss_meter.avg:.3f} Classification loss: {cla_train_loss_meter.avg:.3f} Time epoch: {end_time-start_time:.3f}s\")\n\n            #Valid\n            model.eval()\n            with torch.no_grad():\n                for batch_idx, (image, mask, label) in enumerate(val_loader):\n                    n = image.shape[0]\n                    image = image.to(DEVICE)\n                    mask = mask.to(DEVICE)\n                    label = label.to(DEVICE)\n\n                    #Forward\n                    output_mask, output_classification = model(image)\n\n                    #Cal loss\n                    loss_segmentation = dice_loss(output_mask, mask)\n                    loss_classification = focal_loss(output_classification, label, alpha=0.25, gamma=2,reduction='mean')\n                    val_loss = ALPHA*loss_segmentation + (1 - ALPHA)*loss_classification\n\n\n                    #Calculate metrics\n                    #Segmentation: iou, dice, p, r, f1\n\n                    mask = F.sigmoid(mask).round().long()\n                    tp, fp, fn, tn = smp.metrics.get_stats(output_mask, mask, mode='binary', threshold=0.5)\n\n\n                    seg_iou_score = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"macro\")\n\n                    seg_dice_score = torch.mean((2*tp.sum(0)/(2*tp.sum(0) + fp.sum(0) + fn.sum(0) + 1e-5)))\n                    seg_precision_score = smp.metrics.precision(tp, fp, fn, tn, reduction=\"macro\")\n                    seg_recall_score = smp.metrics.recall(tp, fp, fn, tn, reduction=\"macro\")\n                    seg_f1_score = smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"macro\")\n\n\n                    #Classification: acc, p, r, f1\n                    label = label.detach().cpu().numpy()\n                    output_classification = output_classification.argmax(1).detach().cpu().numpy()\n\n                    cla_acc = accuracy_score(label, output_classification)\n                    cla_precision_score = precision_score(label, output_classification, average='macro', zero_division=0)\n                    cla_recall_score = recall_score(label, output_classification, average='macro', zero_division=0)\n                    cla_f1_score = f1_score(label, output_classification, average='macro')\n\n\n                    #Update meters\n                    val_loss_meter.update(val_loss.item(), n)\n\n                    #Segmentation\n                    seg_val_loss_meter.update(loss_segmentation.item(), n)\n                    seg_iou_meter.update(seg_iou_score.item(), n)\n                    seg_dice_meter.update(seg_dice_score.item(), n)\n                    seg_precision_meter.update(seg_precision_score.item(), n)\n                    seg_recall_meter.update(seg_recall_score.item(), n)\n                    seg_f1_score_meter.update(seg_f1_score.item(), n)\n\n                    #Classification\n                    cla_val_loss_meter.update(loss_classification.item(), n)\n                    cla_acc_meter.update(cla_acc.item(),n)\n                    cla_precision_meter.update(cla_precision_score.item(), n)\n                    cla_recall_meter.update(cla_recall_score.item(), n)\n                    cla_f1_score_meter.update(cla_f1_score.item(), n)\n\n                    #Common\n                    overall_score = ((seg_iou_score + seg_dice_score + seg_f1_score)/3 + cla_f1_score)/2\n                    overall_meter.update(overall_score.item(), n)\n\n            logger.info(f\"Validation Result: Loss: {val_loss_meter.avg:.3f}, Segmentation loss: {seg_val_loss_meter.avg:.3f} Classification loss: {cla_val_loss_meter.avg:.3f} Overal Score: {overall_meter.avg:.3f}\")\n            logger.info(f\"Classification: Accuracy: {cla_acc_meter.avg:.3f}, F1-Score: {cla_f1_score_meter.avg:.3f}, Precision: {cla_precision_meter.avg:.3f}, Recall: {cla_recall_meter.avg:.3f}\")\n            logger.info(f\"Segmentation: IoU: {seg_iou_meter.avg:.3f} Dice: {seg_dice_meter.avg:.3f}, F1-score: {seg_f1_score_meter.avg:.3f}, Precision: {seg_precision_meter.avg:.3f}, Recall: {seg_recall_meter.avg:.3f}\")\n            #Save best model\n            to_save = {\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'best_overall': best_overall\n            }\n            if overall_meter.avg > best_overall: # best base on IoU score\n                logger.info(f\"Best model found at epoch {epoch}, saving model\")\n                torch.save(to_save, os.path.join(weight_dir,f\"best_{epoch}_{INPUT_SIZE[0]}_BS={BATCH_SIZE}_overal={overall_meter.avg:.3f}.pth\"))\n                best_overall = overall_meter.avg\n                stale = 0\n            else:\n                stale += 1\n                if stale > 300:\n                    logger.info(f\"No improvement {300} consecutive epochs, early stopping\")\n                    break\n            if epoch % SAVE_INTERVAL == 0 or epoch == MAX_EPOCHS:\n                logger.info(f\"Save model at epoch {epoch}, saving model\")\n\n                torch.save(to_save, os.path.join(weight_dir,f\"epoch_{epoch}.pth\"))\n","metadata":{"id":"C-WsmKv8FVQD","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qv7b0JwnFYRl","outputId":"0fd82a38-3ed5-4708-8f78-11c8d65bd26a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"YSQO3Nr8ObDH"},"execution_count":null,"outputs":[]}]}